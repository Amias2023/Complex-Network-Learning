# 图神经网络


## 1. 图数据

### 1. 图数据实例

* 在电子商务中， 基于图的学习系统可以更好的表示用户和产品之间的联系， 从而做更加精确的推荐

* 在引用网络中， 论文通过引用关系相互链接， 从而归类成不同的组。‘



### 2. 图数据特点

**图数据是不规则的**

* 图中大小不定的的无序节点

* 每个节点又有数量不定的邻居


### 3. ML 处理

**优势：**

* 机器学习能自动化人类可以轻易实现但难以向计算机描述的任务。
> 例如： 描述图中的人物， 这个人物对人类来说很简单， 但是很难抽象为离散的算法， 这时候就需要使用机器学习。
这个在图上也是类似的， 但是不同于图像或者向量， 图有复杂的关系结构，需要找到一个适用图的 模式方法。

* 机器学习可以处理相当大的规模的数据

**难点 ：**

* 不规则， 这对于传统的机器学习模型很难处理这些非结构化的数据。

* 同时机器学习算法一个很重要的前提假设是事件是相互独立的，但是图网络中的节点（事件） 通过链接相互关联

* 图数据是无序（节点之间）， 但是要是直接输入到网络（CNN）， 就会因为输入堆叠顺序变得有序，影响预测

* 图数据中边的信息很重要，ML 很难处理， 通常只是表征为一个特征， 但是我们需要边的结构




### 4. 图的种类

**处理的图的基本分类:**

* 基本图

* 有向图

	有向的边能够表征更多的信息
	> 在传播过程就要区分从 父类还是从子类之间的区别


* 动态图（属性图）
	网络拓扑结构固定，节点的属性随着时间动态的变化


* 异质图
	节点的种类不同:

	1. 最简单： 所有节点 one-hot

	2. GraphInception ： 根据节点邻居的类型和距离，对节点邻居进行分组， 对于每个邻居组， 看作一个均匀子图， 在子图上进行传播， 之后将不同组（子图） 传播信息聚合

	3. Heterogeneous graph network ： 对 2 加入 attention 机制

-----


## 2. 图神经网络(GNN)

### 1. GNN vs 网络嵌入 Network embedding

网络嵌入的目标就是将节点表示为低维的向量（包含网络的拓扑结构和节点的内容信息）
> 随机游走， 矩阵分解等方法

**二者区别 :**

* 网络嵌入是使用不同的方法解决同一个问题， 手动进行特征工程

* GNN 是对于不同的任务有着不同的网络结构模型
	> GNN 可以通过 graph autoencoder framework 实行网络嵌入。


### 2. GNN vs 图的核方法

核方法 ： 使用一个核函数计算两个图之间的相似性， 用来做图分类

**二者区别 :** 核方法的函数要自己去定义， 就像 SVM 中的核函数


----


## 3. 图神经网络的分类



### 1. Recurrent Graph Neural Network(RecGNNs)

利用循环神经结构学习节点的表征


**假定节点不断的和周围的节点交互信息直到稳定(Banach's fixed point theorem)**

<div align="center"> <img src="https://i.loli.net/2019/09/09/l7QtwnjRzgMe8iL.png" width="300"/> </div><br>



RNN ---> Graph Echo Network ---> Gated Graph Neural Network ---> Stochastic Steady-state embedding

* RNN
	> 更新的时候使用相同的参数， 难于提取分层的特征

	> 边的信息没有很好的提取

	> 更新寻找不动点效率很低

* Graph Echo Network ： 使用回声网络， 回声状态方程

* Gated Graph Neural Network ： GRU 单元作为上面的循环方程

* Stochastic Steady-state embedding ： 解决 large graph 的问题 ：采样的方法，一组进行状态更新，一组计算梯度下降

-----

### 2. Convolutional Graph Neural Network(ConvGNNs)

<div align="center"> <img src="https://i.loli.net/2019/09/09/nGCpSfO49hMK76T.png" width="500"/> </div><br><div align="center"> <img src="" width=""/> </div><br>




**将 image（欧氏空间） 上的操作处理方法（CNNs）泛化到 graph （非欧氏空间）上面。**
> image 可以看作一个特别规整的 graph

CNNs 的关键是：

1. 局部连接
	> 图是典型的局部连接的结构

2. 共享权重
	> 共享权重可以减少计算，相比于传统的 基于光谱的图理论

3. 多层网络
	> 多层可以处理分层次的 pattern



卷积操作： 聚合自身特征和近邻节点的特征

<div align="center"> <img src="https://i.loli.net/2019/09/09/SIQgPHNu2LeVOWY.png" width="500"/> </div><br>


* 区别于 RecGNNs, ConvGNNs 每次使用不同的权重提取深层次的的特征 or states


-----


### 3. 图自动编码

一个非监督的框架， 将节点或者图编码进一个隐因子向量空间，之后再根据编码信息重建图数据。


**方法基本想法：**

* Deep Neural Network for Graph Pepresenting(DNGR)

	使用多层堆叠的 Autoencoder 对 pointwise mutual information matrix(PPMI) 进行编码解码。PPMI 通过随机游走进行采样获取。

* Sructural Deep Neural Network Embedding(SDNE)

	使用 Autoencoder 保留一阶和二阶近邻信息。 对编码和解码过程使用不同的损失函数，一个为了保留一阶近邻信息，一个为了保留二阶近邻信息。

* Graph Autoencoder (GAE)

	上述的方法都只考虑了图的拓扑结构信息， 忽略了节点本身包含的特征信息。 GAE 使用 GCN 去编码节点结构信息和节点的特征信息。




**应用**

* network embedding ： 学习隐因子节点表征

* 图生成

<div align="center"> <img src="https://i.loli.net/2019/09/09/NLrGw8cRSnE3C1H.png" width="500"/> </div><br>

![]()



----


### 4. 时空图神经网络(STGNNS)


🌟🌟🌟 **郑宇 KDD paper 使用图神经网络进行分类**

现实世界中的网络很多都是动态的，包括 图的结构动态变化( 🌟 Dropout 的想法)， 图的节点的输入动态变化。

假定连接节点之间是相互依赖的，对动态的节点输入进行建模。

从时空网络中学习隐式 pattern， 即考虑网络的空间依赖性，也考虑时间依赖性。


**方法基本想法：**


* 基于 RNN 的方法

* 基于 CNN 的方法


**应用**

* 交通方面 ： 流量预测

* 人的动作预测



----

## 4. 任务分类


* Node-level ：节点回归，分类任务
	> 半监督学习， 给定一个网络，部分节点打了标签， 其他的没有打标签。 使用 ConvGNN 进行节点分类


* Edge-level ： 边分类， 链路预测等（连接的标签或者强度(数值)）

* Garph-level : 图分类任务， graph embedding
	> 监督学习进行图分类。对整个图打标签。

	> 非监督学习进行图嵌入， 1. Autoencoder， 2. 负采样方法


----



## 5. 应用

<div align="center"> <img src="https://i.loli.net/2019/09/09/l5BbwgvWnuT8tSA.png" width="400"/> </div><br>




### 1. Traffic

**通常将交通看作一个时空图， 节点是路上的探测器， 边是两个探测器之间的距离**


#### 1. 出租车需求预测 ：

:page_facing_up: **2018 AAAI :Deep Multi-View Spatial-Temporal Network for Taxi Demand Prediction.** [paper](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16069/15978)

* Motivation :

	给定历史需求数据，位置信息等特征， 结合 LSTM， CNN， LINE， 和 Network Embedding 形成一个联合表征，


#### 2.


:page_facing_up: **2018 IJCAI : Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting.** [paper](https://arxiv.org/pdf/1709.04875.pdf)


#### 3.

:page_facing_up: **2018 ICLR : DIFFUSION CONVOLUTIONAL RECURRENT NEURAL NETWORK: DATA-DRIVEN TRAFFIC FORECASTING.** [paper](https://arxiv.org/pdf/1707.01926.pdf)




----


### 2. Social


#### 1. 社会影响预测 :

:page_facing_up: **2018 KDD : Deepinf:Social influence prediction with deep learning.** [paper](https://arxiv.org/pdf/1807.05560.pdf)

* Motivation :

	给定节点周围邻居的状态（投票，购买，转发等）， 和节点局部结构化信息，去预测节点的状态

	<div align="center"> <img src="https://i.loli.net/2019/09/10/2FlQigXdhVjBKTy.png" width="300"/> </div><br>




* Methed :

	1. 传统方法通常是手动设定规则

	2. 本文中使用 **Network embedding， graph convolution，graph attention mechanism** 构建一个 end-to-end 的framework

	3. Model Framework ：

	    <div align="center"> <img src="https://i.loli.net/2019/09/10/Maco7Xn6QeDBPwN.png" width="500"/> </div><br>


		* 通过随机游走的方法进行采样， 获得 节点的 neighbors 和 ego 网络（包含关注的节点和他的邻居节点） ，


		* 使用 embedding layer， 将网络的结构信息映射到一个低维的隐因子空间（将每个节点用户都映射到起相应的 D 维空间， ）
			> embedding 矩阵大小 ： D x n, 每一列是一个用户在网络中的表征

		* 之后使用 ** instance NOrmalization** ： 更关注节点在 embedding 空间中相互之间的相对位置，而不是绝对位置

		* 聚合 上述网络的 embedding 信息和 自身的 action states信息等形成一个向量 ，作为 input layer

		* 之后使用 GCN + Attention  进行 encode 得到输出

		* 对 ego user 的输出和 ground truth 进行比较， 计算loss
			> log-likehood

* Discussion :

	1. 文章使用 4 个 数据集  学术引用， twitter， weibo， digg 投票，相较于Baseline 表现较好

	2. 文章处理的网络是无向的，也没有考虑边的信息 🌟

	3. DeepInf 主要还是对局部区域做一个表征， 可以用在其他的任务上： 链路预测， 相似度搜索等

	4. 采样过程使用的 random walk， 和模型训练关系不是很大， 可以使用 强化学习，将采样和学习进行结合 🌟





-----



#### 2. 节点编码时将节点分为多方面

:page_facing_up: **2019 KDD : Is a Single Embedding Enough? Learning Node Representations that Capture Multiple Social Contexts** [paper](https://arxiv.org/pdf/1905.02138.pdf)


* Motivation：
 	对网络中的节点建立多方面的向量，从而更好的描述节点。
 	> 例如下面一个人的购物网络，如果使用一个向量表示这个人节点， 那么在 embedding 空间，这些类别要相互邻近。 然而其他人的兴趣不同，就会混乱编码向量的分布

 	<div align="center"> <img src="https://i.loli.net/2019/09/10/XQkHUgvSmW1JqB4.png" width="300"/> </div><br>


* Method

	存在 4 个要解决的问题 ：

	   1. 如何划分节点的方面

	   2. 如何建立不同节点方面之间的联系

	   3. 如何将多方面的节点模型嵌入到已有的模型中

	   4. 加入多方面之后，增加的模型复杂度


* Discussion

	文章提出的模型解决了 问题 1，3，4 ， 但是没有很好的解决不同方面之间的联系

----



#### 3. 社会网络中用户在多

:page_facing_up: **2019 KDD: MCNE: An End-to-End Framework for Learning Multiple Conditional Network Representations of Social Network** [paper](https://arxiv.org/abs/1905.11013)


* Motivation:

	如下图所示，网络中的节点(用户)， 可能在某些方面和其他的用户相似。 然而目前的网络表征方法只是对于每个节点使用一个单一的向量表征其独特的方面。

	<div align="center"> <img src="https://i.loli.net/2019/09/11/qVG3BfjTrbQsPCn.png" width="300"/> </div><br>



* Method :

	存在的挑战：

	   	1. 如何在单个向量空间表征多个方面的相似性

	    2. 多个方面之间的复杂关系


	<div align="center"> <img src="https://i.loli.net/2019/09/18/rMSkm1RUXwjNyWz.png" width="500"/> </div><br>

	1. 通过 embedding 层和 Binary Mask 层生成多条件网络表征

	2. 学习用户单方面的相似性
		> attention机制

	3. 学习用户多方面的相似性
		> multi-task framwork



* Discussion

	文中使用了 **binary mask layer** 将单个节点的额嵌入拆分成多方面的

	本文的节点没有加入属性信息， 未来考虑结合属性信息，增加模型的可解释性


-----


#### 4. 政治倾向检测

:page_facing_up: ** ACL 2019 Encoding Social Information with Graph Convolutional Networks for Political Perspective Detection in News Media**


* Motivation:

	新闻事件报道中通常隐藏着政治倾向， 不像那些政治文章那么明显，易于检测。 因此本文希望不仅使用文本本身的信息， 还利用 文本在社交网络中传播的信息(例如转发)


* Method :

	<div align="center"> <img src="https://i.loli.net/2019/09/18/a48jReLWI1suri6.png" width="500"/> </div><br>


	如上图所示， 将文章和社交网络 embedding 进一个空间， 构建一个信息传播的图，包括

		1. 哪些人进行了分享

		2. 人之间的 follow 关系

		3. 一些人的政治属性（各种党派的账号）


	结合文章本身的内容信息和在社交网络中的传播信息，进行预测（分类） 文章的政治倾向

	主要基于 GCN



* Discussion:

	本文将  新闻政治倾向分为 三类 ，最后转化一个分类问题。

	本文认为 分享相同文章的用户更可能有着相同的政治倾向
	> 但是有时候 转发新闻只是为了批判 ， 反对 🌟


------






## 6. 未来研究方向


### 1. 模型深度

区别于深度学习， ConvGNN 模型随着卷积层的增加，模型的表现能力显著的下降。 因为图卷积就是利用临近节点进行表征， 如果卷积层无限多， 所有节点的表征都会趋于一个单节点，

图数据是否能够 “deep learning”

### 2. 模型可扩展性和完整性

使用 sampling 或者 clustering 的时候，模型都会丢失部分的图信息。 sampling，节点会失去近邻节点信息。 clustering 会失去 远距离的结构信息。

1. 如何平衡 稳定性和完整性

2. GNN 计算在大数据的时候，算力消耗巨大
	> 非欧数据， 每个节点都有不定的邻居


### 3. 异质性

目前的都是假设图是均匀的。但是

1. 图可能包含不同种类的节点或者连边，

2. 或者不同形式的节点输入（图片，文本）



### 4. 动态

1. 边或者节点消失，导致图的拓扑结构发生改变

2. 边或者节点的输入发生改变。--- STGNNs











