{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 代码大部分内容类似于 kipf_GCN_keras 实现， 可以进行参考"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T09:51:52.454696Z",
     "start_time": "2020-06-01T09:51:46.992457Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn # 构建网络模块\n",
    "import torch.nn.functional as F  # 网络中函数 例如 F.relu\n",
    "from torch.nn.parameter import Parameter # 构建的网络的参数\n",
    "from torch.nn.modules.module import Module # 自己构建的网络需要继承的模块\n",
    "import torch.optim as optim # 优化器模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T09:51:58.637470Z",
     "start_time": "2020-06-01T09:51:58.613175Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)  # 注意因为 set ，所以每次生成的 y 的 onehot 值是不一样的\n",
    "    classes_dict = {\n",
    "        c: np.identity(len(classes))[i, :]\n",
    "        for i, c in enumerate(classes)\n",
    "    }  # np.identity() 创建一个单位对角阵， 单位矩阵的每一行对应一个one-hot向量\n",
    "    labels_onehot = np.array(\n",
    "        list(map(classes_dict.get, labels)),\n",
    "        dtype=np.int32)  # map(function, iterable)： 对每个 label，应用 class_dict()\n",
    "    return labels_onehot\n",
    "\n",
    "\n",
    "def load_data(path=\"data/cora/\", dataset=\"cora\"):\n",
    "    \"\"\"Load citation network dataset \"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    ### 读取样本id，特征和标签\n",
    "    idx_features_labels = np.genfromtxt(\n",
    "        \"{}{}.content\".format(path, dataset), dtype=np.dtype(\n",
    "            str))  # np.genfromtxt()生成 array： 文件数据的格式为id features labels\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1],\n",
    "                             dtype=np.float32)  # 提取样本的特征，并将其转换为csr矩阵\n",
    "    labels = encode_onehot(\n",
    "        idx_features_labels[:, -1])  # 提取样本的标签，并将其转换为one-hot编码形式\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)  # 样本的id数组\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}  # 创建一个字典储存数据id\n",
    "\n",
    "    ### 读取样本之间关系 ： 连边\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(\n",
    "                         edges_unordered.shape)  # 无序边  map 成为有序\n",
    "\n",
    "    # 构建邻接矩阵\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)  # 构建图的邻接矩阵\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(\n",
    "        adj.T >\n",
    "        adj)  # 矩阵进行对称化： 对于无向图，邻接矩阵是对称的。上一步得到的adj是按有向图构建的，转换成无向图的邻接矩阵需要扩充成对称矩阵。\n",
    "\n",
    "    features = normalize(features)  # 对特征进行归一化处理\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))  #对邻接矩阵进行归一化处理\n",
    "\n",
    "    idx_train = range(140)  # 训练集样本\n",
    "    idx_val = range(200, 500)  # 验证集样本\n",
    "    idx_test = range(500, 1500)  # 测试集样本\n",
    "\n",
    "    # 从 numpy 转换为 torch\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"按行对矩阵进行归一化\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))  # 每行的值进行加和 ：x_sum =  (x_11 + x_12 + x_13 ...)\n",
    "    r_inv = np.power(rowsum, -1).flatten()  # 加和的值取倒数  1/x_sum\n",
    "    r_inv[np.isinf(r_inv)] = 0.  # 将结果中的无穷值转换为 0 ( x_sum可能为0 ，产生无穷值)\n",
    "    r_mat_inv = sp.diags(r_inv)  # 将  1/x_sum 进行对角化\n",
    "    mx = r_mat_inv.dot(mx)  # 初始矩阵和 1/x_sum 对角化矩阵进行乘积运算\n",
    "    return mx\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    \"\"\"计算准确率\"\"\"\n",
    "    preds = output.max(1)[1].type_as(labels)  # 类型转换\n",
    "    correct = preds.eq(labels).double()  # 是否相同， true， false\n",
    "    correct = correct.sum()  # true false 加和\n",
    "    return correct / len(labels)\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(\n",
    "            np.int64))  # # 获得稀疏矩阵坐标 (2708, 1433)  --> (49216, 2)\n",
    "    values = torch.from_numpy(sparse_mx.data)  # 相应位置的值 (49216, ) 即矩阵中的所有非零值\n",
    "    shape = torch.Size(sparse_mx.shape)  # 稀疏矩阵的大小\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T10:22:07.820260Z",
     "start_time": "2020-06-01T10:22:07.813907Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://camo.githubusercontent.com/05cc2a7f9417d663c85fa6816cd87b7dcb18cd8d/68747470733a2f2f747661312e73696e61696d672e636e2f6c617267652f30303753385a496c6c793167653834366e66666b686a333077743067796a76642e6a7067\", width=320, heigth=240>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"https://camo.githubusercontent.com/05cc2a7f9417d663c85fa6816cd87b7dcb18cd8d/68747470733a2f2f747661312e73696e61696d672e636e2f6c617267652f30303753385a496c6c793167653834366e66666b686a333077743067796a76642e6a7067\", width=320, heigth=240>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T09:52:02.581654Z",
     "start_time": "2020-06-01T09:52:02.572016Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()  # 确保父类被正确的初始化了\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(\n",
    "            torch.FloatTensor(in_features, out_features)\n",
    "        )  # 当Paramenters赋值给Module的属性的时候，他会自动的被加到 Module的 参数列表中\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters() # 进行参数初始化\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"参数初始化方式\"\"\"\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv) # 权重满足 \n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight) # 将输入特征矩阵与权重参数矩阵相\n",
    "        output = torch.spmm(adj, support) # 左乘标准化的邻接矩阵，邻接矩阵的存储时用的是稀疏矩阵\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"输出类内部变量的名称\"\"\"\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T09:52:09.430984Z",
     "start_time": "2020-06-01T09:52:09.424232Z"
    }
   },
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        # 定义所需要的操作 ： 图卷积层+ dropout\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \"\"\"每次运行时都会执行的步骤，所有自定义的module都要重写这个函数\"\"\"\n",
    "        x = F.relu(self.gc1(x, adj)) # 第一层图卷积 + relu\n",
    "        x = F.dropout(x, self.dropout, training=self.training) \n",
    "        x = self.gc2(x, adj) # 第二层图卷积\n",
    "        return F.log_softmax(x, dim=1) # 计算 sotmax + log 输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T10:02:07.113821Z",
     "start_time": "2020-06-01T10:02:07.109559Z"
    }
   },
   "outputs": [],
   "source": [
    "# 超参数\n",
    "\n",
    "seed = 42\n",
    "epochs =200 # Number of epochs to train.\n",
    "lr = 0.01 # Initial learning rate.\n",
    "weight_decay = 5e-4 # Weight decay (L2 loss on parameters)\n",
    "hidden = 16 # Number of hidden units.'\n",
    "dropout = 0.5 # Dropout rate (1 - keep probability)\n",
    "fastmode = False # val 时候是否和训练区分（dropout， BN）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T09:56:36.004680Z",
     "start_time": "2020-06-01T09:56:30.890559Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "# 导入数据\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()\n",
    "\n",
    "# 构建模型\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=dropout)\n",
    "# 创建优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay) # 包含了权重正则化部分的 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T10:02:43.386426Z",
     "start_time": "2020-06-01T10:02:43.377738Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    \"\"\"标准 pytorch 神经网络流程\"\"\"\n",
    "    t = time.time()\n",
    "    model.train() # 先将model置为训练状态\n",
    "    optimizer.zero_grad() # 梯度置0\n",
    "    output = model(features, adj) # 将输入送到模型得到输出结果\n",
    "    loss_train = F.nll_loss(\n",
    "        output[idx_train], labels[idx_train]\n",
    "    )  # ；计算损失与准确率；交叉熵loss， 因为模型计算包含 log， 这里使用 nll_loss（CrossEntropyLoss =Softmax+Log+NLLLoss）\n",
    "\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward() # 反向传播求梯度\n",
    "    optimizer.step() # 更新参数\n",
    "\n",
    "    if not fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval(\n",
    "        )  #pytorch会自动把BN和DropOut固定住, dropout和batch normalization的操作在训练和测试的时候是不一样的\n",
    "        output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch + 1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval() # 置为 evaluation 状态 \n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\", \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T10:03:44.886265Z",
     "start_time": "2020-06-01T10:03:42.446973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 2.0122 acc_train: 0.1500 loss_val: 2.0183 acc_val: 0.1200 time: 0.0317s\n",
      "Epoch: 0002 loss_train: 1.9934 acc_train: 0.1000 loss_val: 1.9994 acc_val: 0.1600 time: 0.0175s\n",
      "Epoch: 0003 loss_train: 1.9656 acc_train: 0.2357 loss_val: 1.9822 acc_val: 0.1567 time: 0.0144s\n",
      "Epoch: 0004 loss_train: 1.9460 acc_train: 0.2000 loss_val: 1.9661 acc_val: 0.1567 time: 0.0127s\n",
      "Epoch: 0005 loss_train: 1.9270 acc_train: 0.2143 loss_val: 1.9505 acc_val: 0.1567 time: 0.0119s\n",
      "Epoch: 0006 loss_train: 1.9155 acc_train: 0.2000 loss_val: 1.9352 acc_val: 0.1567 time: 0.0126s\n",
      "Epoch: 0007 loss_train: 1.8913 acc_train: 0.2000 loss_val: 1.9200 acc_val: 0.1567 time: 0.0127s\n",
      "Epoch: 0008 loss_train: 1.8845 acc_train: 0.2000 loss_val: 1.9052 acc_val: 0.1567 time: 0.0117s\n",
      "Epoch: 0009 loss_train: 1.8729 acc_train: 0.2000 loss_val: 1.8902 acc_val: 0.1567 time: 0.0115s\n",
      "Epoch: 0010 loss_train: 1.8567 acc_train: 0.2000 loss_val: 1.8755 acc_val: 0.1567 time: 0.0117s\n",
      "Epoch: 0011 loss_train: 1.8375 acc_train: 0.2000 loss_val: 1.8610 acc_val: 0.1567 time: 0.0114s\n",
      "Epoch: 0012 loss_train: 1.8296 acc_train: 0.2000 loss_val: 1.8465 acc_val: 0.1567 time: 0.0107s\n",
      "Epoch: 0013 loss_train: 1.8100 acc_train: 0.2000 loss_val: 1.8322 acc_val: 0.1567 time: 0.0112s\n",
      "Epoch: 0014 loss_train: 1.8140 acc_train: 0.2000 loss_val: 1.8178 acc_val: 0.1567 time: 0.0114s\n",
      "Epoch: 0015 loss_train: 1.7869 acc_train: 0.2071 loss_val: 1.8036 acc_val: 0.1567 time: 0.0108s\n",
      "Epoch: 0016 loss_train: 1.7701 acc_train: 0.2786 loss_val: 1.7896 acc_val: 0.1733 time: 0.0109s\n",
      "Epoch: 0017 loss_train: 1.7629 acc_train: 0.3571 loss_val: 1.7758 acc_val: 0.4267 time: 0.0107s\n",
      "Epoch: 0018 loss_train: 1.7482 acc_train: 0.3500 loss_val: 1.7624 acc_val: 0.4300 time: 0.0115s\n",
      "Epoch: 0019 loss_train: 1.7391 acc_train: 0.3786 loss_val: 1.7496 acc_val: 0.3667 time: 0.0144s\n",
      "Epoch: 0020 loss_train: 1.7169 acc_train: 0.3429 loss_val: 1.7376 acc_val: 0.3633 time: 0.0265s\n",
      "Epoch: 0021 loss_train: 1.7046 acc_train: 0.3857 loss_val: 1.7262 acc_val: 0.3500 time: 0.0096s\n",
      "Epoch: 0022 loss_train: 1.7054 acc_train: 0.2929 loss_val: 1.7155 acc_val: 0.3500 time: 0.0091s\n",
      "Epoch: 0023 loss_train: 1.7084 acc_train: 0.3071 loss_val: 1.7055 acc_val: 0.3500 time: 0.0098s\n",
      "Epoch: 0024 loss_train: 1.6798 acc_train: 0.3214 loss_val: 1.6961 acc_val: 0.3500 time: 0.0110s\n",
      "Epoch: 0025 loss_train: 1.6651 acc_train: 0.3000 loss_val: 1.6871 acc_val: 0.3500 time: 0.0112s\n",
      "Epoch: 0026 loss_train: 1.6900 acc_train: 0.3000 loss_val: 1.6782 acc_val: 0.3500 time: 0.0104s\n",
      "Epoch: 0027 loss_train: 1.6600 acc_train: 0.3143 loss_val: 1.6693 acc_val: 0.3500 time: 0.0097s\n",
      "Epoch: 0028 loss_train: 1.6202 acc_train: 0.3000 loss_val: 1.6604 acc_val: 0.3500 time: 0.0112s\n",
      "Epoch: 0029 loss_train: 1.6329 acc_train: 0.3071 loss_val: 1.6514 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0030 loss_train: 1.6085 acc_train: 0.3214 loss_val: 1.6424 acc_val: 0.3500 time: 0.0110s\n",
      "Epoch: 0031 loss_train: 1.5828 acc_train: 0.3286 loss_val: 1.6332 acc_val: 0.3500 time: 0.0098s\n",
      "Epoch: 0032 loss_train: 1.5947 acc_train: 0.3357 loss_val: 1.6239 acc_val: 0.3500 time: 0.0111s\n",
      "Epoch: 0033 loss_train: 1.5537 acc_train: 0.3214 loss_val: 1.6144 acc_val: 0.3600 time: 0.0111s\n",
      "Epoch: 0034 loss_train: 1.5523 acc_train: 0.3857 loss_val: 1.6049 acc_val: 0.3600 time: 0.0108s\n",
      "Epoch: 0035 loss_train: 1.5406 acc_train: 0.4000 loss_val: 1.5952 acc_val: 0.3633 time: 0.0115s\n",
      "Epoch: 0036 loss_train: 1.5180 acc_train: 0.4143 loss_val: 1.5852 acc_val: 0.3667 time: 0.0131s\n",
      "Epoch: 0037 loss_train: 1.5165 acc_train: 0.4143 loss_val: 1.5749 acc_val: 0.3833 time: 0.0140s\n",
      "Epoch: 0038 loss_train: 1.5020 acc_train: 0.4214 loss_val: 1.5645 acc_val: 0.3900 time: 0.0109s\n",
      "Epoch: 0039 loss_train: 1.4574 acc_train: 0.4500 loss_val: 1.5538 acc_val: 0.4100 time: 0.0105s\n",
      "Epoch: 0040 loss_train: 1.4204 acc_train: 0.5214 loss_val: 1.5429 acc_val: 0.4333 time: 0.0100s\n",
      "Epoch: 0041 loss_train: 1.4408 acc_train: 0.4929 loss_val: 1.5319 acc_val: 0.4633 time: 0.0107s\n",
      "Epoch: 0042 loss_train: 1.4092 acc_train: 0.5643 loss_val: 1.5203 acc_val: 0.4800 time: 0.0097s\n",
      "Epoch: 0043 loss_train: 1.4441 acc_train: 0.5429 loss_val: 1.5087 acc_val: 0.4900 time: 0.0094s\n",
      "Epoch: 0044 loss_train: 1.4267 acc_train: 0.5429 loss_val: 1.4968 acc_val: 0.4967 time: 0.0096s\n",
      "Epoch: 0045 loss_train: 1.3703 acc_train: 0.5857 loss_val: 1.4845 acc_val: 0.5033 time: 0.0110s\n",
      "Epoch: 0046 loss_train: 1.3606 acc_train: 0.6071 loss_val: 1.4720 acc_val: 0.5033 time: 0.0101s\n",
      "Epoch: 0047 loss_train: 1.3488 acc_train: 0.6357 loss_val: 1.4593 acc_val: 0.5200 time: 0.0098s\n",
      "Epoch: 0048 loss_train: 1.3010 acc_train: 0.6143 loss_val: 1.4463 acc_val: 0.5233 time: 0.0094s\n",
      "Epoch: 0049 loss_train: 1.2932 acc_train: 0.6357 loss_val: 1.4331 acc_val: 0.5433 time: 0.0097s\n",
      "Epoch: 0050 loss_train: 1.2992 acc_train: 0.6643 loss_val: 1.4197 acc_val: 0.5700 time: 0.0111s\n",
      "Epoch: 0051 loss_train: 1.2894 acc_train: 0.6071 loss_val: 1.4062 acc_val: 0.5800 time: 0.0098s\n",
      "Epoch: 0052 loss_train: 1.2572 acc_train: 0.6500 loss_val: 1.3927 acc_val: 0.5867 time: 0.0111s\n",
      "Epoch: 0053 loss_train: 1.2659 acc_train: 0.6500 loss_val: 1.3794 acc_val: 0.6000 time: 0.0108s\n",
      "Epoch: 0054 loss_train: 1.2068 acc_train: 0.6857 loss_val: 1.3664 acc_val: 0.6133 time: 0.0119s\n",
      "Epoch: 0055 loss_train: 1.2041 acc_train: 0.6857 loss_val: 1.3535 acc_val: 0.6333 time: 0.0137s\n",
      "Epoch: 0056 loss_train: 1.1808 acc_train: 0.7357 loss_val: 1.3408 acc_val: 0.6433 time: 0.0099s\n",
      "Epoch: 0057 loss_train: 1.2017 acc_train: 0.6786 loss_val: 1.3281 acc_val: 0.6500 time: 0.0110s\n",
      "Epoch: 0058 loss_train: 1.1682 acc_train: 0.7214 loss_val: 1.3155 acc_val: 0.6533 time: 0.0111s\n",
      "Epoch: 0059 loss_train: 1.1355 acc_train: 0.7071 loss_val: 1.3029 acc_val: 0.6567 time: 0.0097s\n",
      "Epoch: 0060 loss_train: 1.1437 acc_train: 0.7214 loss_val: 1.2906 acc_val: 0.6600 time: 0.0098s\n",
      "Epoch: 0061 loss_train: 1.0960 acc_train: 0.7214 loss_val: 1.2786 acc_val: 0.6667 time: 0.0098s\n",
      "Epoch: 0062 loss_train: 1.0767 acc_train: 0.7357 loss_val: 1.2670 acc_val: 0.6633 time: 0.0111s\n",
      "Epoch: 0063 loss_train: 1.0974 acc_train: 0.7429 loss_val: 1.2555 acc_val: 0.6667 time: 0.0093s\n",
      "Epoch: 0064 loss_train: 1.1106 acc_train: 0.7000 loss_val: 1.2437 acc_val: 0.6667 time: 0.0097s\n",
      "Epoch: 0065 loss_train: 1.0864 acc_train: 0.7143 loss_val: 1.2319 acc_val: 0.6700 time: 0.0098s\n",
      "Epoch: 0066 loss_train: 1.0863 acc_train: 0.6929 loss_val: 1.2202 acc_val: 0.6767 time: 0.0110s\n",
      "Epoch: 0067 loss_train: 1.0310 acc_train: 0.7214 loss_val: 1.2091 acc_val: 0.6800 time: 0.0099s\n",
      "Epoch: 0068 loss_train: 1.0628 acc_train: 0.7571 loss_val: 1.1982 acc_val: 0.6833 time: 0.0102s\n",
      "Epoch: 0069 loss_train: 0.9982 acc_train: 0.7643 loss_val: 1.1878 acc_val: 0.6967 time: 0.0106s\n",
      "Epoch: 0070 loss_train: 1.0463 acc_train: 0.8143 loss_val: 1.1777 acc_val: 0.7033 time: 0.0106s\n",
      "Epoch: 0071 loss_train: 0.9636 acc_train: 0.8143 loss_val: 1.1681 acc_val: 0.7033 time: 0.0110s\n",
      "Epoch: 0072 loss_train: 0.9712 acc_train: 0.7714 loss_val: 1.1585 acc_val: 0.7133 time: 0.0109s\n",
      "Epoch: 0073 loss_train: 0.9816 acc_train: 0.7429 loss_val: 1.1487 acc_val: 0.7133 time: 0.0109s\n",
      "Epoch: 0074 loss_train: 0.9705 acc_train: 0.7643 loss_val: 1.1390 acc_val: 0.7233 time: 0.0130s\n",
      "Epoch: 0075 loss_train: 0.9557 acc_train: 0.7714 loss_val: 1.1297 acc_val: 0.7200 time: 0.0123s\n",
      "Epoch: 0076 loss_train: 0.9280 acc_train: 0.7643 loss_val: 1.1209 acc_val: 0.7300 time: 0.0120s\n",
      "Epoch: 0077 loss_train: 0.9481 acc_train: 0.7857 loss_val: 1.1122 acc_val: 0.7300 time: 0.0108s\n",
      "Epoch: 0078 loss_train: 0.9335 acc_train: 0.8214 loss_val: 1.1034 acc_val: 0.7333 time: 0.0104s\n",
      "Epoch: 0079 loss_train: 0.8585 acc_train: 0.8286 loss_val: 1.0946 acc_val: 0.7333 time: 0.0121s\n",
      "Epoch: 0080 loss_train: 0.9062 acc_train: 0.8143 loss_val: 1.0858 acc_val: 0.7500 time: 0.0104s\n",
      "Epoch: 0081 loss_train: 0.8713 acc_train: 0.8357 loss_val: 1.0767 acc_val: 0.7567 time: 0.0133s\n",
      "Epoch: 0082 loss_train: 0.8950 acc_train: 0.8286 loss_val: 1.0673 acc_val: 0.7600 time: 0.0143s\n",
      "Epoch: 0083 loss_train: 0.8615 acc_train: 0.8071 loss_val: 1.0583 acc_val: 0.7600 time: 0.0136s\n",
      "Epoch: 0084 loss_train: 0.8737 acc_train: 0.8357 loss_val: 1.0493 acc_val: 0.7600 time: 0.0116s\n",
      "Epoch: 0085 loss_train: 0.8521 acc_train: 0.8357 loss_val: 1.0410 acc_val: 0.7633 time: 0.0131s\n",
      "Epoch: 0086 loss_train: 0.8770 acc_train: 0.8286 loss_val: 1.0328 acc_val: 0.7667 time: 0.0120s\n",
      "Epoch: 0087 loss_train: 0.8027 acc_train: 0.8571 loss_val: 1.0250 acc_val: 0.7700 time: 0.0145s\n",
      "Epoch: 0088 loss_train: 0.7656 acc_train: 0.8857 loss_val: 1.0174 acc_val: 0.7700 time: 0.0109s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0089 loss_train: 0.7589 acc_train: 0.8786 loss_val: 1.0097 acc_val: 0.7733 time: 0.0138s\n",
      "Epoch: 0090 loss_train: 0.7995 acc_train: 0.8357 loss_val: 1.0026 acc_val: 0.7833 time: 0.0165s\n",
      "Epoch: 0091 loss_train: 0.8170 acc_train: 0.8429 loss_val: 0.9955 acc_val: 0.7833 time: 0.0168s\n",
      "Epoch: 0092 loss_train: 0.7629 acc_train: 0.8643 loss_val: 0.9887 acc_val: 0.7900 time: 0.0190s\n",
      "Epoch: 0093 loss_train: 0.7805 acc_train: 0.8571 loss_val: 0.9817 acc_val: 0.7900 time: 0.0206s\n",
      "Epoch: 0094 loss_train: 0.7111 acc_train: 0.8929 loss_val: 0.9737 acc_val: 0.7900 time: 0.0153s\n",
      "Epoch: 0095 loss_train: 0.7874 acc_train: 0.8357 loss_val: 0.9660 acc_val: 0.7900 time: 0.0122s\n",
      "Epoch: 0096 loss_train: 0.7656 acc_train: 0.8857 loss_val: 0.9583 acc_val: 0.7933 time: 0.0124s\n",
      "Epoch: 0097 loss_train: 0.7598 acc_train: 0.8429 loss_val: 0.9510 acc_val: 0.7967 time: 0.0138s\n",
      "Epoch: 0098 loss_train: 0.7509 acc_train: 0.8500 loss_val: 0.9440 acc_val: 0.7967 time: 0.0139s\n",
      "Epoch: 0099 loss_train: 0.7228 acc_train: 0.8429 loss_val: 0.9373 acc_val: 0.7967 time: 0.0110s\n",
      "Epoch: 0100 loss_train: 0.6906 acc_train: 0.8643 loss_val: 0.9312 acc_val: 0.7933 time: 0.0121s\n",
      "Epoch: 0101 loss_train: 0.7485 acc_train: 0.8571 loss_val: 0.9258 acc_val: 0.7967 time: 0.0122s\n",
      "Epoch: 0102 loss_train: 0.6995 acc_train: 0.8500 loss_val: 0.9214 acc_val: 0.7867 time: 0.0117s\n",
      "Epoch: 0103 loss_train: 0.6805 acc_train: 0.8643 loss_val: 0.9174 acc_val: 0.7867 time: 0.0115s\n",
      "Epoch: 0104 loss_train: 0.6948 acc_train: 0.8643 loss_val: 0.9124 acc_val: 0.7867 time: 0.0137s\n",
      "Epoch: 0105 loss_train: 0.6456 acc_train: 0.9000 loss_val: 0.9074 acc_val: 0.7867 time: 0.0095s\n",
      "Epoch: 0106 loss_train: 0.6523 acc_train: 0.8643 loss_val: 0.9019 acc_val: 0.7900 time: 0.0109s\n",
      "Epoch: 0107 loss_train: 0.6465 acc_train: 0.9000 loss_val: 0.8968 acc_val: 0.7900 time: 0.0111s\n",
      "Epoch: 0108 loss_train: 0.6777 acc_train: 0.8643 loss_val: 0.8917 acc_val: 0.7933 time: 0.0097s\n",
      "Epoch: 0109 loss_train: 0.6679 acc_train: 0.8643 loss_val: 0.8868 acc_val: 0.7933 time: 0.0100s\n",
      "Epoch: 0110 loss_train: 0.6402 acc_train: 0.8857 loss_val: 0.8815 acc_val: 0.7933 time: 0.0098s\n",
      "Epoch: 0111 loss_train: 0.6162 acc_train: 0.8857 loss_val: 0.8766 acc_val: 0.7967 time: 0.0102s\n",
      "Epoch: 0112 loss_train: 0.6451 acc_train: 0.8714 loss_val: 0.8730 acc_val: 0.7967 time: 0.0094s\n",
      "Epoch: 0113 loss_train: 0.6292 acc_train: 0.9000 loss_val: 0.8688 acc_val: 0.7967 time: 0.0101s\n",
      "Epoch: 0114 loss_train: 0.6102 acc_train: 0.8929 loss_val: 0.8650 acc_val: 0.7967 time: 0.0103s\n",
      "Epoch: 0115 loss_train: 0.6389 acc_train: 0.8929 loss_val: 0.8604 acc_val: 0.7967 time: 0.0097s\n",
      "Epoch: 0116 loss_train: 0.6722 acc_train: 0.8786 loss_val: 0.8555 acc_val: 0.7967 time: 0.0110s\n",
      "Epoch: 0117 loss_train: 0.6215 acc_train: 0.8857 loss_val: 0.8514 acc_val: 0.8067 time: 0.0093s\n",
      "Epoch: 0118 loss_train: 0.5745 acc_train: 0.9071 loss_val: 0.8472 acc_val: 0.8100 time: 0.0110s\n",
      "Epoch: 0119 loss_train: 0.6194 acc_train: 0.8786 loss_val: 0.8434 acc_val: 0.8100 time: 0.0098s\n",
      "Epoch: 0120 loss_train: 0.5611 acc_train: 0.9214 loss_val: 0.8403 acc_val: 0.8100 time: 0.0109s\n",
      "Epoch: 0121 loss_train: 0.5447 acc_train: 0.8929 loss_val: 0.8375 acc_val: 0.8133 time: 0.0110s\n",
      "Epoch: 0122 loss_train: 0.5668 acc_train: 0.9143 loss_val: 0.8346 acc_val: 0.8100 time: 0.0093s\n",
      "Epoch: 0123 loss_train: 0.5871 acc_train: 0.9000 loss_val: 0.8321 acc_val: 0.8000 time: 0.0139s\n",
      "Epoch: 0124 loss_train: 0.6288 acc_train: 0.8714 loss_val: 0.8298 acc_val: 0.7967 time: 0.0119s\n",
      "Epoch: 0125 loss_train: 0.5738 acc_train: 0.9071 loss_val: 0.8268 acc_val: 0.7967 time: 0.0154s\n",
      "Epoch: 0126 loss_train: 0.5623 acc_train: 0.9143 loss_val: 0.8243 acc_val: 0.7967 time: 0.0150s\n",
      "Epoch: 0127 loss_train: 0.5544 acc_train: 0.9071 loss_val: 0.8218 acc_val: 0.7967 time: 0.0110s\n",
      "Epoch: 0128 loss_train: 0.5404 acc_train: 0.9000 loss_val: 0.8189 acc_val: 0.7967 time: 0.0123s\n",
      "Epoch: 0129 loss_train: 0.5458 acc_train: 0.9286 loss_val: 0.8159 acc_val: 0.8000 time: 0.0117s\n",
      "Epoch: 0130 loss_train: 0.5728 acc_train: 0.8929 loss_val: 0.8122 acc_val: 0.8000 time: 0.0105s\n",
      "Epoch: 0131 loss_train: 0.5605 acc_train: 0.9071 loss_val: 0.8084 acc_val: 0.7967 time: 0.0109s\n",
      "Epoch: 0132 loss_train: 0.5793 acc_train: 0.9071 loss_val: 0.8046 acc_val: 0.7933 time: 0.0129s\n",
      "Epoch: 0133 loss_train: 0.5644 acc_train: 0.9071 loss_val: 0.8013 acc_val: 0.8000 time: 0.0154s\n",
      "Epoch: 0134 loss_train: 0.5305 acc_train: 0.9071 loss_val: 0.7984 acc_val: 0.7967 time: 0.0108s\n",
      "Epoch: 0135 loss_train: 0.5124 acc_train: 0.9214 loss_val: 0.7952 acc_val: 0.7967 time: 0.0144s\n",
      "Epoch: 0136 loss_train: 0.5530 acc_train: 0.8786 loss_val: 0.7925 acc_val: 0.8000 time: 0.0129s\n",
      "Epoch: 0137 loss_train: 0.5106 acc_train: 0.9000 loss_val: 0.7902 acc_val: 0.8033 time: 0.0116s\n",
      "Epoch: 0138 loss_train: 0.4972 acc_train: 0.9357 loss_val: 0.7883 acc_val: 0.8033 time: 0.0141s\n",
      "Epoch: 0139 loss_train: 0.5025 acc_train: 0.9071 loss_val: 0.7863 acc_val: 0.8033 time: 0.0139s\n",
      "Epoch: 0140 loss_train: 0.5360 acc_train: 0.9214 loss_val: 0.7848 acc_val: 0.8067 time: 0.0126s\n",
      "Epoch: 0141 loss_train: 0.5149 acc_train: 0.9071 loss_val: 0.7834 acc_val: 0.8033 time: 0.0131s\n",
      "Epoch: 0142 loss_train: 0.4916 acc_train: 0.9143 loss_val: 0.7824 acc_val: 0.8033 time: 0.0101s\n",
      "Epoch: 0143 loss_train: 0.4847 acc_train: 0.9143 loss_val: 0.7815 acc_val: 0.8033 time: 0.0122s\n",
      "Epoch: 0144 loss_train: 0.5083 acc_train: 0.9000 loss_val: 0.7800 acc_val: 0.8033 time: 0.0130s\n",
      "Epoch: 0145 loss_train: 0.5155 acc_train: 0.9143 loss_val: 0.7773 acc_val: 0.8033 time: 0.0119s\n",
      "Epoch: 0146 loss_train: 0.4613 acc_train: 0.9143 loss_val: 0.7751 acc_val: 0.8033 time: 0.0107s\n",
      "Epoch: 0147 loss_train: 0.4791 acc_train: 0.9214 loss_val: 0.7734 acc_val: 0.8067 time: 0.0125s\n",
      "Epoch: 0148 loss_train: 0.5718 acc_train: 0.8857 loss_val: 0.7726 acc_val: 0.8067 time: 0.0130s\n",
      "Epoch: 0149 loss_train: 0.5002 acc_train: 0.9286 loss_val: 0.7708 acc_val: 0.8067 time: 0.0114s\n",
      "Epoch: 0150 loss_train: 0.5036 acc_train: 0.9286 loss_val: 0.7684 acc_val: 0.8067 time: 0.0118s\n",
      "Epoch: 0151 loss_train: 0.4792 acc_train: 0.9286 loss_val: 0.7659 acc_val: 0.8067 time: 0.0122s\n",
      "Epoch: 0152 loss_train: 0.4539 acc_train: 0.9571 loss_val: 0.7644 acc_val: 0.8033 time: 0.0122s\n",
      "Epoch: 0153 loss_train: 0.5157 acc_train: 0.9214 loss_val: 0.7640 acc_val: 0.8100 time: 0.0111s\n",
      "Epoch: 0154 loss_train: 0.4488 acc_train: 0.9643 loss_val: 0.7627 acc_val: 0.8100 time: 0.0106s\n",
      "Epoch: 0155 loss_train: 0.4969 acc_train: 0.9071 loss_val: 0.7608 acc_val: 0.8033 time: 0.0138s\n",
      "Epoch: 0156 loss_train: 0.4562 acc_train: 0.9214 loss_val: 0.7589 acc_val: 0.8033 time: 0.0119s\n",
      "Epoch: 0157 loss_train: 0.4763 acc_train: 0.9214 loss_val: 0.7566 acc_val: 0.8033 time: 0.0140s\n",
      "Epoch: 0158 loss_train: 0.5017 acc_train: 0.9214 loss_val: 0.7540 acc_val: 0.8033 time: 0.0128s\n",
      "Epoch: 0159 loss_train: 0.4453 acc_train: 0.8929 loss_val: 0.7520 acc_val: 0.8033 time: 0.0105s\n",
      "Epoch: 0160 loss_train: 0.4738 acc_train: 0.9286 loss_val: 0.7503 acc_val: 0.8033 time: 0.0127s\n",
      "Epoch: 0161 loss_train: 0.4556 acc_train: 0.9357 loss_val: 0.7486 acc_val: 0.8033 time: 0.0121s\n",
      "Epoch: 0162 loss_train: 0.4712 acc_train: 0.9071 loss_val: 0.7486 acc_val: 0.8033 time: 0.0104s\n",
      "Epoch: 0163 loss_train: 0.4657 acc_train: 0.9429 loss_val: 0.7494 acc_val: 0.8033 time: 0.0115s\n",
      "Epoch: 0164 loss_train: 0.4532 acc_train: 0.9286 loss_val: 0.7501 acc_val: 0.8033 time: 0.0134s\n",
      "Epoch: 0165 loss_train: 0.4912 acc_train: 0.9143 loss_val: 0.7514 acc_val: 0.8067 time: 0.0103s\n",
      "Epoch: 0166 loss_train: 0.4427 acc_train: 0.9357 loss_val: 0.7507 acc_val: 0.8100 time: 0.0124s\n",
      "Epoch: 0167 loss_train: 0.4206 acc_train: 0.9357 loss_val: 0.7488 acc_val: 0.8100 time: 0.0118s\n",
      "Epoch: 0168 loss_train: 0.4195 acc_train: 0.9357 loss_val: 0.7450 acc_val: 0.8100 time: 0.0114s\n",
      "Epoch: 0169 loss_train: 0.4541 acc_train: 0.9429 loss_val: 0.7408 acc_val: 0.8133 time: 0.0102s\n",
      "Epoch: 0170 loss_train: 0.4518 acc_train: 0.9286 loss_val: 0.7372 acc_val: 0.8133 time: 0.0115s\n",
      "Epoch: 0171 loss_train: 0.4387 acc_train: 0.9643 loss_val: 0.7344 acc_val: 0.8133 time: 0.0100s\n",
      "Epoch: 0172 loss_train: 0.4677 acc_train: 0.9357 loss_val: 0.7315 acc_val: 0.8133 time: 0.0105s\n",
      "Epoch: 0173 loss_train: 0.4314 acc_train: 0.9643 loss_val: 0.7296 acc_val: 0.8167 time: 0.0101s\n",
      "Epoch: 0174 loss_train: 0.4224 acc_train: 0.9286 loss_val: 0.7279 acc_val: 0.8167 time: 0.0110s\n",
      "Epoch: 0175 loss_train: 0.4702 acc_train: 0.9000 loss_val: 0.7264 acc_val: 0.8133 time: 0.0128s\n",
      "Epoch: 0176 loss_train: 0.4018 acc_train: 0.9214 loss_val: 0.7262 acc_val: 0.8133 time: 0.0126s\n",
      "Epoch: 0177 loss_train: 0.4167 acc_train: 0.9357 loss_val: 0.7266 acc_val: 0.8100 time: 0.0130s\n",
      "Epoch: 0178 loss_train: 0.4229 acc_train: 0.9571 loss_val: 0.7269 acc_val: 0.8067 time: 0.0108s\n",
      "Epoch: 0179 loss_train: 0.4168 acc_train: 0.9214 loss_val: 0.7258 acc_val: 0.8100 time: 0.0098s\n",
      "Epoch: 0180 loss_train: 0.4189 acc_train: 0.9286 loss_val: 0.7243 acc_val: 0.8133 time: 0.0113s\n",
      "Epoch: 0181 loss_train: 0.4533 acc_train: 0.9071 loss_val: 0.7215 acc_val: 0.8167 time: 0.0099s\n",
      "Epoch: 0182 loss_train: 0.4513 acc_train: 0.9429 loss_val: 0.7182 acc_val: 0.8167 time: 0.0094s\n",
      "Epoch: 0183 loss_train: 0.4457 acc_train: 0.9286 loss_val: 0.7154 acc_val: 0.8167 time: 0.0096s\n",
      "Epoch: 0184 loss_train: 0.4409 acc_train: 0.9214 loss_val: 0.7142 acc_val: 0.8233 time: 0.0107s\n",
      "Epoch: 0185 loss_train: 0.3959 acc_train: 0.9571 loss_val: 0.7146 acc_val: 0.8167 time: 0.0103s\n",
      "Epoch: 0186 loss_train: 0.4043 acc_train: 0.9500 loss_val: 0.7146 acc_val: 0.8200 time: 0.0112s\n",
      "Epoch: 0187 loss_train: 0.4027 acc_train: 0.9429 loss_val: 0.7139 acc_val: 0.8200 time: 0.0103s\n",
      "Epoch: 0188 loss_train: 0.4025 acc_train: 0.9357 loss_val: 0.7126 acc_val: 0.8167 time: 0.0117s\n",
      "Epoch: 0189 loss_train: 0.4157 acc_train: 0.9500 loss_val: 0.7104 acc_val: 0.8167 time: 0.0137s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0190 loss_train: 0.4102 acc_train: 0.9357 loss_val: 0.7092 acc_val: 0.8167 time: 0.0165s\n",
      "Epoch: 0191 loss_train: 0.3966 acc_train: 0.9643 loss_val: 0.7081 acc_val: 0.8167 time: 0.0122s\n",
      "Epoch: 0192 loss_train: 0.4595 acc_train: 0.9214 loss_val: 0.7066 acc_val: 0.8167 time: 0.0148s\n",
      "Epoch: 0193 loss_train: 0.4161 acc_train: 0.9143 loss_val: 0.7061 acc_val: 0.8133 time: 0.0127s\n",
      "Epoch: 0194 loss_train: 0.4422 acc_train: 0.9214 loss_val: 0.7065 acc_val: 0.8133 time: 0.0115s\n",
      "Epoch: 0195 loss_train: 0.3905 acc_train: 0.9643 loss_val: 0.7069 acc_val: 0.8133 time: 0.0119s\n",
      "Epoch: 0196 loss_train: 0.3947 acc_train: 0.9571 loss_val: 0.7075 acc_val: 0.8133 time: 0.0113s\n",
      "Epoch: 0197 loss_train: 0.3923 acc_train: 0.9214 loss_val: 0.7090 acc_val: 0.8133 time: 0.0115s\n",
      "Epoch: 0198 loss_train: 0.4342 acc_train: 0.9500 loss_val: 0.7116 acc_val: 0.8067 time: 0.0125s\n",
      "Epoch: 0199 loss_train: 0.3986 acc_train: 0.9286 loss_val: 0.7125 acc_val: 0.8067 time: 0.0112s\n",
      "Epoch: 0200 loss_train: 0.4183 acc_train: 0.9429 loss_val: 0.7110 acc_val: 0.8100 time: 0.0107s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 2.4359s\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "t_total = time.time()\n",
    "for epoch in range(epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T10:03:49.225052Z",
     "start_time": "2020-06-01T10:03:49.216047Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.7367 accuracy= 0.8150\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
