{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 代码基于 GraphSAGE论文第一作者 William L. Hamilton 开源的 pytorch 实现  ： https://github.com/williamleif/graphsage-simple， 代码相对于源代码加了注释和少量改动\n",
    "\n",
    "* 按照 William L. Hamilton 所说， 这个实现更适用于小图\n",
    "    > the code is easier to read and it performs better (in terms of speed) on small-graph benchmarks.\n",
    "    \n",
    "* 目前只包含论文中 GraphSAGE-mean 和 GraphSAGE-GCN 两种方法\n",
    "\n",
    "* 数据集目前只支持 cora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T01:06:54.719810Z",
     "start_time": "2020-06-02T01:06:54.714770Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T01:06:59.055673Z",
     "start_time": "2020-06-02T01:06:59.037052Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class MeanAggregator(nn.Module):\n",
    "    \"\"\"\n",
    "    Aggregates a node's embeddings using mean of neighbors' embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, features, cuda=False, gcn=False):\n",
    "        \"\"\"\n",
    "        Initializes the aggregator for a specific graph.\n",
    "\n",
    "        features -- function mapping LongTensor of node ids to FloatTensor of feature values.\n",
    "        cuda -- whether to use GPU\n",
    "        gcn --- whether to perform concatenation GraphSAGE-style, or add self-loops GCN-style\n",
    "        \"\"\"\n",
    "\n",
    "        super(MeanAggregator, self).__init__()  # 确保父类被正确的初始化了\n",
    "\n",
    "        self.features = features\n",
    "        self.cuda = cuda\n",
    "        self.gcn = gcn\n",
    "\n",
    "    def forward(self, nodes, to_neighs, num_sample=10):\n",
    "        \"\"\"\n",
    "        nodes --- list of nodes in a batch\n",
    "        to_neighs --- list of sets, each set is the set of neighbors for node in batch\n",
    "        num_sample --- number of neighbors to sample. No sampling if None.\n",
    "        \n",
    "        每次运行时都会执行的步骤，所有自定义的module都要重写这个函数\n",
    "        \"\"\"\n",
    "\n",
    "        _set = set  # Local pointers to functions (speed hack) 使用内嵌 set 函数\n",
    "        if not num_sample is None:  # 如果要进行采样： 有采样数\n",
    "            _sample = random.sample  # 使用 random中sample 函数 ：不放回的从一个list 或者 set 中筛选指定书目的数据\n",
    "            samp_neighs = [\n",
    "                _set(_sample(\n",
    "                    to_neigh,\n",
    "                    num_sample,\n",
    "                )) if len(to_neigh) >= num_sample else to_neigh\n",
    "                for to_neigh in to_neighs\n",
    "            ]  # 对于所有节点， 如果邻居数大于节点数，就进行采样并转换为set， 如果不大于，就直接使用 邻居\n",
    "        else:  # 如果不采样，就直接使用邻居\n",
    "            samp_neighs = to_neighs\n",
    "\n",
    "        if self.gcn:  # 采样的邻居节点中是否加入自身的节点\n",
    "            samp_neighs = [\n",
    "                samp_neigh.union(set([nodes[i]]))\n",
    "                for i, samp_neigh in enumerate(samp_neighs)\n",
    "            ]\n",
    "\n",
    "        unique_nodes_list = list(set.union(*samp_neighs)) # 所有节点采样到的邻居节点 ： 训练使用到的节点 m\n",
    "        unique_nodes = {n: i for i, n in enumerate(unique_nodes_list)} # 为用到的节点创建字典\n",
    "        mask = Variable(torch.zeros(len(samp_neighs), len(unique_nodes))) # 创建一个矩阵 ： 所有节点 * 用到的节点： (n,m)\n",
    "        # mask 矩阵 每一个节点对应的一行，  一行包含所有可能用到的节点， 是该节点用到的邻居节点位置值为1 ， 其余位置为0\n",
    "        column_indices = [\n",
    "            unique_nodes[n] for samp_neigh in samp_neighs for n in samp_neigh\n",
    "        ]  # 将邻居节点 按照创建的字典生成相应的 index\n",
    "        \"\"\"\n",
    "        1号节点邻居对应的新编号 5,6,7,8; 2 号节点邻居对应新编号：3,4,5 --> [5,6,7,8,3,4,5] \n",
    "        for samp_neigh in samp_neighs:\n",
    "            for n in samp_neigh:\n",
    "                unique_nodes[n] \n",
    "        \"\"\"\n",
    "        row_indices = [\n",
    "            i for i in range(len(samp_neighs))\n",
    "            for j in range(len(samp_neighs[i]))\n",
    "        ]\n",
    "        \n",
    "        \"\"\"\n",
    "        1 号节点有 4个邻居; 2号节点3个邻居， 就表示为 [1,1,1,1,2,2,2]\n",
    "        for i in range(len(samp_neighs)):\n",
    "            for j in range(len(samp_neighs[i])):\n",
    "                i \n",
    "        \"\"\"\n",
    "        mask[row_indices, column_indices] = 1\n",
    "        \"\"\"      \n",
    "        [5,6,7,8,3,4,5] \n",
    "        [1,1,1,1,2,2,2] \n",
    "        \n",
    "        所以 mask 矩阵 [1,5], [1,6]...  处值为1。 表示节点 1 有新编号为 5， 6 的邻居\n",
    "        \"\"\"\n",
    "\n",
    "        if self.cuda:\n",
    "            mask = mask.cuda()\n",
    "            \n",
    "        num_neigh = mask.sum(1, keepdim=True) # torch.sum()  (n,m) --> (n, 1)\n",
    "        #若keepdim值为True，则在输出张量中，除了被操作的dim维度值降为1，其它维度与输入张量input相同。\n",
    "        #否则，dim维度相当于被执行torch.squeeze()维度压缩操作，导致此维度消失，最终输出张量会比输入张量少一个维度。\n",
    "        mask = mask.div(num_neigh) # 按行进行归一化 [1,0,1,1,0,0] -->[1/6,0,1/6,1/6,0,0]\n",
    "\n",
    "        if self.cuda:\n",
    "            embed_matrix = self.features(\n",
    "                torch.LongTensor(unique_nodes_list).cuda())\n",
    "        else:\n",
    "            embed_matrix = self.features(torch.LongTensor(unique_nodes_list)) # (m,f) -- m: 候选节点数 ， 节点的特征数 f\n",
    "        to_feats = mask.mm(embed_matrix) # 矩阵相乘 (n,m) * (m,k) --> (n,k)， 相当于进行邻居聚合： 每一行对应邻居位置的特征加和\n",
    "        return to_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T01:07:00.722245Z",
     "start_time": "2020-06-02T01:07:00.709856Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes a node's using 'convolutional' GraphSage approach\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 features,\n",
    "                 feature_dim,\n",
    "                 embed_dim,\n",
    "                 adj_lists,\n",
    "                 aggregator,\n",
    "                 num_sample=10,\n",
    "                 base_model=None,\n",
    "                 gcn=False,\n",
    "                 cuda=False,\n",
    "                 feature_transform=False):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.features = features\n",
    "        self.feat_dim = feature_dim\n",
    "        self.adj_lists = adj_lists\n",
    "        self.aggregator = aggregator\n",
    "        self.num_sample = num_sample\n",
    "        if base_model != None:\n",
    "            self.base_model = base_model\n",
    "\n",
    "        self.gcn = gcn  # 是否拼接自身节点特征  False 是拼接\n",
    "        self.embed_dim = embed_dim\n",
    "        self.cuda = cuda\n",
    "        self.aggregator.cuda = cuda\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.FloatTensor(\n",
    "                embed_dim, self.feat_dim if self.gcn else 2 * self.feat_dim\n",
    "            )  # 不拼接 [embed_dim,feat_dim ] ; 拼接：[embed_dim, 2*feat_dim ] \n",
    "        )  # 当Paramenters赋值给Module的属性的时候，他会自动的被加到 Module的 参数列表中\n",
    "        init.xavier_uniform_(self.weight)  # 进行参数初始化 xavier 初始化\n",
    "\n",
    "    def forward(self, nodes):\n",
    "        \"\"\"\n",
    "        Generates embeddings for a batch of nodes.\n",
    "\n",
    "        nodes     -- list of nodes\n",
    "        \"\"\"\n",
    "        neigh_feats = self.aggregator.forward(\n",
    "            nodes, [self.adj_lists[int(node)] for node in nodes],\n",
    "            self.num_sample)  # (节点， 节点邻居， 采样数)\n",
    "        if not self.gcn:\n",
    "            if self.cuda:\n",
    "                self_feats = self.features(torch.LongTensor(nodes).cuda())\n",
    "            else:\n",
    "                self_feats = self.features(\n",
    "                    torch.LongTensor(nodes))  # (n , f) n : 节点数\n",
    "            combined = torch.cat([self_feats, neigh_feats], dim=1)  # (n , 2f)\n",
    "        else:\n",
    "            combined = neigh_feats\n",
    "        combined = F.relu(self.weight.mm(combined.t(\n",
    "        )))  # relu W* F^T : (e, 2f) * (2f, n)  --> (e,n)  e: 嵌入向量维度\n",
    "        return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T01:07:01.860094Z",
     "start_time": "2020-06-02T01:07:01.850781Z"
    }
   },
   "outputs": [],
   "source": [
    "class SupervisedGraphSage(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, enc):\n",
    "        super(SupervisedGraphSage, self).__init__()\n",
    "        self.enc = enc # 嵌入函数\n",
    "        self.xent = nn.CrossEntropyLoss() # 交叉熵损失函数\n",
    "\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(num_classes, enc.embed_dim)) # (c,e)  自动的被加到 Module的 参数列表中\n",
    "        init.xavier_uniform_(self.weight) # 进行参数初始化 xavier 初始化\n",
    "\n",
    "    def forward(self, nodes):\n",
    "        # GraphSAGE 模型\n",
    "        embeds = self.enc(nodes) # (e,n)  e: 嵌入向量维度\n",
    "        scores = self.weight.mm(embeds) #  W* E : 嵌入结果进行一个全连接映射。 (c,e) * (e,n)  --> (c,n) c : 类别数\n",
    "        return scores.t()  # (c,n) -->(n,c) \n",
    "\n",
    "    def loss(self, nodes, labels):\n",
    "        scores = self.forward(nodes) # 分类结果\n",
    "        return self.xent(scores, labels.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T01:08:43.978351Z",
     "start_time": "2020-06-02T01:08:43.963189Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.68156159e+154],\n",
       "       [1.29074453e-231],\n",
       "       [3.50977942e+064],\n",
       "       [2.78134232e-309]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.empty((4,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T01:45:25.355930Z",
     "start_time": "2020-06-02T01:45:25.328941Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_cora():\n",
    "    num_nodes = 2708\n",
    "    num_feats = 1433\n",
    "    feat_data = np.zeros((num_nodes, num_feats))  # （n,f)\n",
    "    labels = np.empty((num_nodes, 1), dtype=np.int64)  # (n,1)\n",
    "    node_map = {}\n",
    "    label_map = {}\n",
    "    with open(\"data/cora/cora.content\") as fp:\n",
    "        for i, line in enumerate(fp):\n",
    "            info = line.strip().split()\n",
    "            feat_data[i, :] = list(map(float, info[1:-1]))  # 获取节点特征\n",
    "            node_map[info[0]] = i  # 节点字典\n",
    "            if not info[-1] in label_map:\n",
    "                label_map[info[-1]] = len(label_map)\n",
    "            labels[i] = label_map[info[-1]]  # 节点标签\n",
    "\n",
    "    adj_lists = defaultdict(set)  # 当字典里的key不存在但被查找时 , 不反回 keyerror\n",
    "    with open(\"data/cora/cora.cites\") as fp:\n",
    "        for i, line in enumerate(fp):\n",
    "            info = line.strip().split()\n",
    "            paper1 = node_map[info[0]]\n",
    "            paper2 = node_map[info[1]]\n",
    "            adj_lists[paper1].add(paper2)  # 互相添加邻居节点\n",
    "            adj_lists[paper2].add(paper1)\n",
    "    return feat_data, labels, adj_lists\n",
    "\n",
    "\n",
    "def run_cora():\n",
    "    np.random.seed(1)\n",
    "    random.seed(1)\n",
    "    num_nodes = 2708\n",
    "    feat_data, labels, adj_lists = load_cora()\n",
    "    features = nn.Embedding(2708, 1433)\n",
    "    features.weight = nn.Parameter(torch.FloatTensor(feat_data),\n",
    "                                   requires_grad=False)\n",
    "\n",
    "    agg1 = MeanAggregator(features, gcn=False, cuda=False)  # 采样的邻居节点中不加入自身的节点\n",
    "    enc1 = Encoder(features, 1433, 128, adj_lists, agg1, gcn=True,\n",
    "                   cuda=False)  # 不拼接自身特征\n",
    "    agg2 = MeanAggregator(lambda nodes: enc1(nodes).t(),\n",
    "                          cuda=False)  # 聚合第一次嵌入的特征结果， 采样的邻居节点中不加入自身的节点\n",
    "    enc2 = Encoder(lambda nodes: enc1(nodes).t(),\n",
    "                   enc1.embed_dim,\n",
    "                   128,\n",
    "                   adj_lists,\n",
    "                   agg2,\n",
    "                   base_model=enc1,\n",
    "                   gcn=True,\n",
    "                   cuda=False)  # 不拼接自身特征\n",
    "    enc1.num_samples = 5  # 第一次采样邻居数\n",
    "    enc2.num_samples = 5  # 第二次采样邻居数\n",
    "\n",
    "    graphsage = SupervisedGraphSage(7, enc2)  # cora 数据集有7 个类别\n",
    "    rand_indices = np.random.permutation(num_nodes)  # 随机打乱节点\n",
    "    test = rand_indices[:1000]\n",
    "    val = rand_indices[1000:1500]\n",
    "    train = list(rand_indices[1500:])\n",
    "\n",
    "    optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad,\n",
    "                                       graphsage.parameters()),\n",
    "                                lr=0.7)\n",
    "    times = []\n",
    "    for batch in range(100):\n",
    "        \"\"\"\n",
    "        每个 epoch 只有 256 个数据， 没有用到所有数据？？？\n",
    "        \"\"\"\n",
    "        batch_nodes = train[:256]  # 每个batch 256个\n",
    "        random.shuffle(train)  # 打乱 train\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()  # 梯度清零\n",
    "        train_output = graphsage.forward(batch_nodes)\n",
    "        acc_train = f1_score(labels[batch_nodes],\n",
    "                             train_output.data.numpy().argmax(axis=1),\n",
    "                             average=\"micro\") # 训练集准确率\n",
    "        loss = graphsage.loss(\n",
    "            batch_nodes,\n",
    "            Variable(torch.LongTensor(labels[np.array(batch_nodes)])))  # 计算训练损失\n",
    "        loss.backward()  # 损失梯度计算，回传\n",
    "        optimizer.step()  #  参数更新\n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "\n",
    "        val_output = graphsage.forward(val)  # 没有使用 eval() 因为没有 BN 和dropout\n",
    "        loss_val = graphsage.loss(\n",
    "            val, Variable(torch.LongTensor(labels[np.array(val)])))  # 计算损失\n",
    "        acc_val = f1_score(labels[val],\n",
    "                           val_output.data.numpy().argmax(axis=1),\n",
    "                           average=\"micro\")\n",
    "\n",
    "        print(\n",
    "            'Epoch: {:04d}'.format(batch + 1),\n",
    "            'loss_train: {:.4f}'.format(loss.item()),\n",
    "            'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "            'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "            'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "            'time: {:.4f}s'.format(end_time - start_time)\n",
    "        )\n",
    "\n",
    "    print(\"Average batch time:\", np.mean(times))\n",
    "    \n",
    "    \n",
    "    # 测试部分\n",
    "    test_output = graphsage.forward(test)  # 没有使用 eval() 没有 BN 和dropout\n",
    "    loss_test = graphsage.loss(\n",
    "        test, Variable(torch.LongTensor(labels[np.array(test)])))  # 计算损失\n",
    "    acc_test = f1_score(labels[test],\n",
    "                        test_output.data.numpy().argmax(axis=1),\n",
    "                        average=\"micro\")\n",
    "    print(\"Test set results:\", \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T01:45:52.413685Z",
     "start_time": "2020-06-02T01:45:26.768777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.9394 acc_train: 0.1758 loss_val: 1.9299 acc_val: 0.1940 time: 0.0741s\n",
      "Epoch: 0002 loss_train: 1.9258 acc_train: 0.2578 loss_val: 1.9065 acc_val: 0.3760 time: 0.0650s\n",
      "Epoch: 0003 loss_train: 1.9037 acc_train: 0.3594 loss_val: 1.8806 acc_val: 0.3940 time: 0.0710s\n",
      "Epoch: 0004 loss_train: 1.8642 acc_train: 0.4102 loss_val: 1.8514 acc_val: 0.4060 time: 0.0792s\n",
      "Epoch: 0005 loss_train: 1.8409 acc_train: 0.4258 loss_val: 1.8118 acc_val: 0.3740 time: 0.0702s\n",
      "Epoch: 0006 loss_train: 1.7966 acc_train: 0.3828 loss_val: 1.7648 acc_val: 0.3500 time: 0.0721s\n",
      "Epoch: 0007 loss_train: 1.7636 acc_train: 0.3203 loss_val: 1.7157 acc_val: 0.3540 time: 0.0689s\n",
      "Epoch: 0008 loss_train: 1.6945 acc_train: 0.3984 loss_val: 1.6628 acc_val: 0.3700 time: 0.0724s\n",
      "Epoch: 0009 loss_train: 1.6266 acc_train: 0.4180 loss_val: 1.6142 acc_val: 0.3560 time: 0.0653s\n",
      "Epoch: 0010 loss_train: 1.5626 acc_train: 0.3516 loss_val: 1.5650 acc_val: 0.4180 time: 0.0639s\n",
      "Epoch: 0011 loss_train: 1.5606 acc_train: 0.4023 loss_val: 1.5135 acc_val: 0.4300 time: 0.0757s\n",
      "Epoch: 0012 loss_train: 1.5414 acc_train: 0.3906 loss_val: 1.4517 acc_val: 0.4360 time: 0.0703s\n",
      "Epoch: 0013 loss_train: 1.3990 acc_train: 0.4688 loss_val: 1.3995 acc_val: 0.4340 time: 0.0642s\n",
      "Epoch: 0014 loss_train: 1.3175 acc_train: 0.4844 loss_val: 1.3365 acc_val: 0.4500 time: 0.0705s\n",
      "Epoch: 0015 loss_train: 1.2743 acc_train: 0.4805 loss_val: 1.2825 acc_val: 0.5300 time: 0.0663s\n",
      "Epoch: 0016 loss_train: 1.2023 acc_train: 0.6211 loss_val: 1.2286 acc_val: 0.5360 time: 0.0729s\n",
      "Epoch: 0017 loss_train: 1.2655 acc_train: 0.5156 loss_val: 1.1752 acc_val: 0.7340 time: 0.0683s\n",
      "Epoch: 0018 loss_train: 1.1223 acc_train: 0.7344 loss_val: 1.1086 acc_val: 0.6900 time: 0.0763s\n",
      "Epoch: 0019 loss_train: 1.0330 acc_train: 0.7109 loss_val: 1.0638 acc_val: 0.7200 time: 0.0678s\n",
      "Epoch: 0020 loss_train: 1.0018 acc_train: 0.7344 loss_val: 1.0097 acc_val: 0.7960 time: 0.0998s\n",
      "Epoch: 0021 loss_train: 0.9595 acc_train: 0.7617 loss_val: 0.9631 acc_val: 0.7900 time: 0.0688s\n",
      "Epoch: 0022 loss_train: 0.8643 acc_train: 0.8008 loss_val: 0.9293 acc_val: 0.7700 time: 0.0692s\n",
      "Epoch: 0023 loss_train: 0.8680 acc_train: 0.7969 loss_val: 0.8728 acc_val: 0.7900 time: 0.0787s\n",
      "Epoch: 0024 loss_train: 0.8592 acc_train: 0.7773 loss_val: 0.8963 acc_val: 0.7840 time: 0.0703s\n",
      "Epoch: 0025 loss_train: 0.7989 acc_train: 0.8359 loss_val: 0.9059 acc_val: 0.6680 time: 0.1949s\n",
      "Epoch: 0026 loss_train: 0.9598 acc_train: 0.6250 loss_val: 1.2103 acc_val: 0.6200 time: 0.0614s\n",
      "Epoch: 0027 loss_train: 1.0898 acc_train: 0.6797 loss_val: 0.9195 acc_val: 0.6740 time: 0.0733s\n",
      "Epoch: 0028 loss_train: 0.9307 acc_train: 0.6328 loss_val: 1.0064 acc_val: 0.7320 time: 0.0745s\n",
      "Epoch: 0029 loss_train: 0.9031 acc_train: 0.7266 loss_val: 0.7221 acc_val: 0.8180 time: 0.0773s\n",
      "Epoch: 0030 loss_train: 0.7135 acc_train: 0.7734 loss_val: 0.6888 acc_val: 0.8180 time: 0.0811s\n",
      "Epoch: 0031 loss_train: 0.6674 acc_train: 0.8281 loss_val: 0.6501 acc_val: 0.8020 time: 0.0882s\n",
      "Epoch: 0032 loss_train: 0.5889 acc_train: 0.8281 loss_val: 0.6155 acc_val: 0.8320 time: 0.0727s\n",
      "Epoch: 0033 loss_train: 0.5133 acc_train: 0.8242 loss_val: 0.6120 acc_val: 0.8520 time: 0.0641s\n",
      "Epoch: 0034 loss_train: 0.4921 acc_train: 0.8633 loss_val: 0.6015 acc_val: 0.8420 time: 0.0782s\n",
      "Epoch: 0035 loss_train: 0.5307 acc_train: 0.8203 loss_val: 0.5877 acc_val: 0.8460 time: 0.0844s\n",
      "Epoch: 0036 loss_train: 0.4939 acc_train: 0.8906 loss_val: 0.5912 acc_val: 0.8300 time: 0.0711s\n",
      "Epoch: 0037 loss_train: 0.4711 acc_train: 0.8984 loss_val: 0.5628 acc_val: 0.8520 time: 0.0989s\n",
      "Epoch: 0038 loss_train: 0.4888 acc_train: 0.8594 loss_val: 0.5625 acc_val: 0.8600 time: 0.0624s\n",
      "Epoch: 0039 loss_train: 0.4917 acc_train: 0.8633 loss_val: 0.5380 acc_val: 0.8700 time: 0.0760s\n",
      "Epoch: 0040 loss_train: 0.4361 acc_train: 0.8555 loss_val: 0.5235 acc_val: 0.8760 time: 0.0764s\n",
      "Epoch: 0041 loss_train: 0.4108 acc_train: 0.8984 loss_val: 0.5377 acc_val: 0.8560 time: 0.0613s\n",
      "Epoch: 0042 loss_train: 0.4090 acc_train: 0.8750 loss_val: 0.5119 acc_val: 0.8620 time: 0.0851s\n",
      "Epoch: 0043 loss_train: 0.4031 acc_train: 0.8984 loss_val: 0.5079 acc_val: 0.8720 time: 0.0590s\n",
      "Epoch: 0044 loss_train: 0.3894 acc_train: 0.9102 loss_val: 0.5191 acc_val: 0.8720 time: 0.0699s\n",
      "Epoch: 0045 loss_train: 0.3995 acc_train: 0.8711 loss_val: 0.5093 acc_val: 0.8520 time: 0.0668s\n",
      "Epoch: 0046 loss_train: 0.4000 acc_train: 0.8906 loss_val: 0.5230 acc_val: 0.8420 time: 0.0694s\n",
      "Epoch: 0047 loss_train: 0.3499 acc_train: 0.9062 loss_val: 0.4904 acc_val: 0.8580 time: 0.0884s\n",
      "Epoch: 0048 loss_train: 0.3298 acc_train: 0.8945 loss_val: 0.4960 acc_val: 0.8660 time: 0.0770s\n",
      "Epoch: 0049 loss_train: 0.3489 acc_train: 0.9062 loss_val: 0.4829 acc_val: 0.8600 time: 0.1037s\n",
      "Epoch: 0050 loss_train: 0.3120 acc_train: 0.9297 loss_val: 0.5386 acc_val: 0.8500 time: 0.0860s\n",
      "Epoch: 0051 loss_train: 0.3692 acc_train: 0.8828 loss_val: 0.4762 acc_val: 0.8760 time: 0.1025s\n",
      "Epoch: 0052 loss_train: 0.3784 acc_train: 0.8906 loss_val: 0.5266 acc_val: 0.8260 time: 0.0720s\n",
      "Epoch: 0053 loss_train: 0.3842 acc_train: 0.8945 loss_val: 0.5093 acc_val: 0.8520 time: 0.0850s\n",
      "Epoch: 0054 loss_train: 0.3315 acc_train: 0.8828 loss_val: 0.5840 acc_val: 0.8300 time: 0.0850s\n",
      "Epoch: 0055 loss_train: 0.3601 acc_train: 0.8945 loss_val: 0.5066 acc_val: 0.8580 time: 0.1261s\n",
      "Epoch: 0056 loss_train: 0.3163 acc_train: 0.8906 loss_val: 0.5740 acc_val: 0.8260 time: 0.0782s\n",
      "Epoch: 0057 loss_train: 0.3463 acc_train: 0.9023 loss_val: 0.4710 acc_val: 0.8560 time: 0.0717s\n",
      "Epoch: 0058 loss_train: 0.2764 acc_train: 0.9102 loss_val: 0.4746 acc_val: 0.8660 time: 0.0973s\n",
      "Epoch: 0059 loss_train: 0.3244 acc_train: 0.9141 loss_val: 0.4654 acc_val: 0.8620 time: 0.0641s\n",
      "Epoch: 0060 loss_train: 0.2605 acc_train: 0.9258 loss_val: 0.5194 acc_val: 0.8360 time: 0.0790s\n",
      "Epoch: 0061 loss_train: 0.3082 acc_train: 0.8984 loss_val: 0.5060 acc_val: 0.8560 time: 0.0705s\n",
      "Epoch: 0062 loss_train: 0.2870 acc_train: 0.8984 loss_val: 0.4536 acc_val: 0.8760 time: 0.0685s\n",
      "Epoch: 0063 loss_train: 0.2748 acc_train: 0.9062 loss_val: 0.4813 acc_val: 0.8740 time: 0.1021s\n",
      "Epoch: 0064 loss_train: 0.2679 acc_train: 0.9258 loss_val: 0.4582 acc_val: 0.8660 time: 0.0728s\n",
      "Epoch: 0065 loss_train: 0.2270 acc_train: 0.9453 loss_val: 0.4456 acc_val: 0.8760 time: 0.0605s\n",
      "Epoch: 0066 loss_train: 0.2487 acc_train: 0.9219 loss_val: 0.4560 acc_val: 0.8480 time: 0.0759s\n",
      "Epoch: 0067 loss_train: 0.2696 acc_train: 0.9258 loss_val: 0.5094 acc_val: 0.8460 time: 0.0787s\n",
      "Epoch: 0068 loss_train: 0.2824 acc_train: 0.8945 loss_val: 0.4534 acc_val: 0.8680 time: 0.0755s\n",
      "Epoch: 0069 loss_train: 0.2384 acc_train: 0.9570 loss_val: 0.4477 acc_val: 0.8660 time: 0.0644s\n",
      "Epoch: 0070 loss_train: 0.2437 acc_train: 0.9297 loss_val: 0.4903 acc_val: 0.8520 time: 0.0855s\n",
      "Epoch: 0071 loss_train: 0.2266 acc_train: 0.9297 loss_val: 0.4637 acc_val: 0.8640 time: 0.0634s\n",
      "Epoch: 0072 loss_train: 0.2403 acc_train: 0.9375 loss_val: 0.4395 acc_val: 0.8680 time: 0.0754s\n",
      "Epoch: 0073 loss_train: 0.2703 acc_train: 0.9023 loss_val: 0.6756 acc_val: 0.8000 time: 0.0683s\n",
      "Epoch: 0074 loss_train: 0.3690 acc_train: 0.8789 loss_val: 0.6083 acc_val: 0.8100 time: 0.0650s\n",
      "Epoch: 0075 loss_train: 0.4555 acc_train: 0.8438 loss_val: 0.9519 acc_val: 0.7460 time: 0.0769s\n",
      "Epoch: 0076 loss_train: 0.5330 acc_train: 0.8633 loss_val: 0.6824 acc_val: 0.7740 time: 0.0747s\n",
      "Epoch: 0077 loss_train: 0.4496 acc_train: 0.8320 loss_val: 0.8568 acc_val: 0.7720 time: 0.0744s\n",
      "Epoch: 0078 loss_train: 0.4316 acc_train: 0.8867 loss_val: 0.4489 acc_val: 0.8740 time: 0.0830s\n",
      "Epoch: 0079 loss_train: 0.2702 acc_train: 0.9258 loss_val: 0.4566 acc_val: 0.8660 time: 0.0703s\n",
      "Epoch: 0080 loss_train: 0.2068 acc_train: 0.9375 loss_val: 0.4491 acc_val: 0.8800 time: 0.0724s\n",
      "Epoch: 0081 loss_train: 0.2528 acc_train: 0.9375 loss_val: 0.4447 acc_val: 0.8660 time: 0.0676s\n",
      "Epoch: 0082 loss_train: 0.2649 acc_train: 0.9219 loss_val: 0.4421 acc_val: 0.8660 time: 0.0669s\n",
      "Epoch: 0083 loss_train: 0.2411 acc_train: 0.9375 loss_val: 0.4639 acc_val: 0.8580 time: 0.0624s\n",
      "Epoch: 0084 loss_train: 0.2463 acc_train: 0.9258 loss_val: 0.4338 acc_val: 0.8760 time: 0.0724s\n",
      "Epoch: 0085 loss_train: 0.2237 acc_train: 0.9297 loss_val: 0.4898 acc_val: 0.8500 time: 0.0995s\n",
      "Epoch: 0086 loss_train: 0.1775 acc_train: 0.9531 loss_val: 0.4399 acc_val: 0.8780 time: 0.0701s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0087 loss_train: 0.1906 acc_train: 0.9414 loss_val: 0.4618 acc_val: 0.8660 time: 0.0711s\n",
      "Epoch: 0088 loss_train: 0.1689 acc_train: 0.9336 loss_val: 0.4753 acc_val: 0.8460 time: 0.0869s\n",
      "Epoch: 0089 loss_train: 0.1877 acc_train: 0.9297 loss_val: 0.4414 acc_val: 0.8720 time: 0.0638s\n",
      "Epoch: 0090 loss_train: 0.2151 acc_train: 0.9258 loss_val: 0.4465 acc_val: 0.8660 time: 0.0676s\n",
      "Epoch: 0091 loss_train: 0.2066 acc_train: 0.9453 loss_val: 0.4788 acc_val: 0.8520 time: 0.0720s\n",
      "Epoch: 0092 loss_train: 0.2375 acc_train: 0.9492 loss_val: 0.4482 acc_val: 0.8600 time: 0.0706s\n",
      "Epoch: 0093 loss_train: 0.1756 acc_train: 0.9375 loss_val: 0.4505 acc_val: 0.8680 time: 0.1150s\n",
      "Epoch: 0094 loss_train: 0.1968 acc_train: 0.9453 loss_val: 0.5029 acc_val: 0.8420 time: 0.0776s\n",
      "Epoch: 0095 loss_train: 0.2740 acc_train: 0.9141 loss_val: 0.4688 acc_val: 0.8620 time: 0.0701s\n",
      "Epoch: 0096 loss_train: 0.1604 acc_train: 0.9375 loss_val: 0.4615 acc_val: 0.8520 time: 0.0801s\n",
      "Epoch: 0097 loss_train: 0.1679 acc_train: 0.9492 loss_val: 0.4407 acc_val: 0.8800 time: 0.0639s\n",
      "Epoch: 0098 loss_train: 0.2153 acc_train: 0.9180 loss_val: 0.5159 acc_val: 0.8460 time: 0.0714s\n",
      "Epoch: 0099 loss_train: 0.1656 acc_train: 0.9531 loss_val: 0.4382 acc_val: 0.8580 time: 0.0809s\n",
      "Epoch: 0100 loss_train: 0.1842 acc_train: 0.9375 loss_val: 0.4971 acc_val: 0.8440 time: 0.0702s\n",
      "Average batch time: 0.07661833763122558\n",
      "Test set results: loss= 0.4764 accuracy= 0.8520\n"
     ]
    }
   ],
   "source": [
    "run_cora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "120px",
    "left": "1078px",
    "right": "20px",
    "top": "33px",
    "width": "340px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
