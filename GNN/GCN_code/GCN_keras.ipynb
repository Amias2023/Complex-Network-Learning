{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T02:44:04.425504Z",
     "start_time": "2020-04-27T02:44:04.419405Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function  \n",
    "\n",
    "import scipy.sparse as sp  \n",
    "import numpy as np  \n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh, ArpackNoConvergence  # 稀疏矩阵中查找特征值/特征向量的函数\n",
    "\n",
    "from keras import activations, initializers, constraints\n",
    "from keras import regularizers\n",
    "from keras.engine import Layer\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T02:44:07.460429Z",
     "start_time": "2020-04-27T02:44:07.411377Z"
    },
    "code_folding": [
     0,
     17,
     55,
     75,
     84,
     104,
     128,
     142,
     174,
     185,
     210,
     240
    ]
   },
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    \"\"\"\n",
    "    将标签转换为one-hot编码形式\n",
    "    \n",
    "    \"\"\"\n",
    "    classes = set(labels)\n",
    "    classes_dict = {\n",
    "        c: np.identity(len(classes))[i, :]\n",
    "        for i, c in enumerate(classes)\n",
    "    }  # np.identity() 创建一个单位对角阵\n",
    "\n",
    "    labels_onehot = np.array(\n",
    "        list(map(classes_dict.get, labels)),\n",
    "        dtype=np.int32)  # map(function, iterable)： 对每个 label，应用 class_dict()\n",
    "    return labels_onehot\n",
    "\n",
    "\n",
    "def load_data(path=\"data/cora/\", dataset=\"cora\"):\n",
    "    \"\"\"\n",
    "    Load citation network dataset \n",
    "    \n",
    "    (cora only for now)\n",
    "    \"\"\"\n",
    "\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    ### 读取样本id，特征和标签\n",
    "    idx_features_labels = np.genfromtxt(\n",
    "        \"{}{}.content\".format(path, dataset),\n",
    "        dtype=np.dtype(str))  # np.genfromtxt()生成 array\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1],\n",
    "                             dtype=np.float32)  # 提取样本的特征，并将其转换为csr矩阵\n",
    "    labels = encode_onehot(\n",
    "        idx_features_labels[:, -1])  # 提取样本的标签，并将其转换为one-hot编码形式\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)  # 样本的id数组\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}  # 创建一个字典储存数据id\n",
    "\n",
    "    ### 读取样本之间关系 ： 连边\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(\n",
    "                         edges_unordered.shape)  # 无序边  map 成为有序\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)  # 构建图的邻接矩阵\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(\n",
    "        adj.T > adj)  # 将非对称邻接矩阵转变为对称邻接矩阵\n",
    "\n",
    "    print('Dataset has {} nodes, {} edges, {} features.'.format(\n",
    "        adj.shape[0], edges.shape[0], features.shape[1]))\n",
    "\n",
    "    return features.todense(), adj, labels  # 返回特征的密集矩阵表示、邻接矩阵和标签的one-hot编码\n",
    "\n",
    "\n",
    "def normalize_adj(adj, symmetric=True):\n",
    "    \"\"\"\n",
    "    对邻接矩阵进行归一化处理\n",
    "    对称归一化 ： D^(-1/2) * A * D^(-1/2)\n",
    "    非对称归一化： D^(-1) * A\n",
    "    \"\"\"\n",
    "\n",
    "    if symmetric:\n",
    "        d = sp.diags(np.power(np.array(adj.sum(1)), -0.5).flatten(),\n",
    "                     0)  # D^(-1/2)\n",
    "        # tocsr()函数将矩阵转化为压缩稀疏行矩阵\n",
    "        a_norm = adj.dot(d).transpose().dot(d).tocsr(\n",
    "        )  #  d 和 A 是对称矩阵 ： (A*  d)^T * d = d^T * A^T  * d= d * A * d\n",
    "\n",
    "    else:\n",
    "        d = sp.diags(np.power(np.array(adj.sum(1)), -1).flatten(), 0)\n",
    "        a_norm = d.dot(adj).tocsr()  # D^(-1) * A\n",
    "    return a_norm\n",
    "\n",
    "\n",
    "def preprocess_adj(adj, symmetric=True):\n",
    "    \"\"\"\n",
    "    在邻接矩阵中加入自连接\n",
    "    \"\"\"\n",
    "    adj = adj + sp.eye(adj.shape[0])  # \\tilde{A}=A+I_{N}\n",
    "    adj = normalize_adj(adj, symmetric)  # renormalization trick\n",
    "    return adj\n",
    "\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"\n",
    "    :param idx: 有标签样本的索引列表\n",
    "    :param l: 所有样本数量\n",
    "    :return: 布尔类型数组，其中有标签样本所对应的位置为True，无标签样本所对应的位置为False\n",
    "    \n",
    "    example : \n",
    "    \n",
    "    >>> mask = np.zeros(5)\n",
    "    >>> list_ = [1,3]\n",
    "    >>> mask[list_]  = 1 \n",
    "    >>> np.array(mask,dtype = np.bool)\n",
    "    \n",
    "    return : array([False,  True, False,  True, False])\n",
    "    \"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "\n",
    "def get_splits(y):\n",
    "    \"\"\"\n",
    "    数据集划分：\n",
    "    \n",
    "    y : 标签数据 即 load_data() 函数 返回的 labels 数据 \n",
    "    数据格式  (2708 , 7 ) 2708 个数据，一共 7 个 class ， one-hot 维度 7 维 \n",
    "    \"\"\"\n",
    "\n",
    "    idx_train = range(140)  # ??? 少一部分数据啊\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    y_train = np.zeros(y.shape, dtype=np.int32) # 初始化 (2708,7) size 的全 0 矩阵。\n",
    "    y_val = np.zeros(y.shape, dtype=np.int32)\n",
    "    y_test = np.zeros(y.shape, dtype=np.int32)\n",
    "    y_train[idx_train] = y[idx_train] #  取 y 矩阵前 140 行， 其余行赋值为 0 \n",
    "    y_val[idx_val] = y[idx_val]\n",
    "    y_test[idx_test] = y[idx_test]\n",
    "\n",
    "    # 训练数据的样本掩码\n",
    "    train_mask = sample_mask(idx_train, y.shape[0]) # 前 140行为 true， 后面为 false 的向量\n",
    "    return y_train, y_val, y_test, idx_train, idx_val, idx_test, train_mask\n",
    "\n",
    "\n",
    "def categorical_crossentropy(preds, labels):\n",
    "    \"\"\"\n",
    "    损失函数计算： 所有有标签的数据，主要用于训练时进行梯度下降 ： \n",
    "    \n",
    "    L=Y*ln(Z)\n",
    "    \n",
    "    param preds: 模型对样本的输出数组\n",
    "    param labels: 样本的one-hot标签数组\n",
    "    return: 样本的平均交叉熵损失\n",
    "    \"\"\"\n",
    "    return np.mean(-np.log(np.extract(\n",
    "        labels, preds)))  # np.extract(condition, x)函数，根据某个条件从数组中抽取元素\n",
    "\n",
    "\n",
    "def accuracy(preds, labels):\n",
    "    \"\"\"\n",
    "    计算准确率\n",
    "    \"\"\"\n",
    "    # np.argmax(x,1) 每一行元素最大值索引  : 相当于将 one-hot 转成一个数值 [0,1,0,0,0] = 1\n",
    "    # np.equal(x1, x2) 比较 array 对应元素是否相等 返回 np.array([ True,  True, False])\n",
    "    # np.mean(np.array([ True,  True, False])) = 0.666\n",
    "    return np.mean(np.equal(np.argmax(labels, 1), np.argmax(preds, 1)))\n",
    "\n",
    "\n",
    "def evaluate_preds(preds, labels, indices):\n",
    "    \"\"\"\n",
    "     评估样本划分的损失函数和准确率\n",
    "     \n",
    "    :param preds:对于样本的预测值\n",
    "    :param labels:样本的标签one-hot向量\n",
    "    :param indices:样本的索引集合\n",
    "    :return:交叉熵损失函数列表、准确率列表\n",
    "    \"\"\"\n",
    "    split_loss = list()\n",
    "    split_acc = list()\n",
    "\n",
    "    for y_split, idx_split in zip(labels, indices):\n",
    "        # 计算每一个样本划分的交叉熵损失函数\n",
    "        split_loss.append(\n",
    "            categorical_crossentropy(preds[idx_split], y_split[idx_split]))\n",
    "        # 计算每一个样本划分的准确率\n",
    "        split_acc.append(accuracy(preds[idx_split], y_split[idx_split]))\n",
    "\n",
    "    return split_loss, split_acc\n",
    "\n",
    "\n",
    "def normalized_laplacian(adj, symmetric=True):\n",
    "    \"\"\"\n",
    "    对拉普拉斯矩阵进行归一化处理\n",
    "    \"\"\"\n",
    "    # 对称归一化的邻接矩阵，D ^ (-1/2) * A * D ^ (-1/2)\n",
    "    adj_normalized = normalize_adj(adj, symmetric)\n",
    "    # 得到对称规范化的图拉普拉斯矩阵，L = I - D ^ (-1/2) * A * D ^ (-1/2)\n",
    "    laplacian = sp.eye(adj.shape[0]) - adj_normalized\n",
    "    return laplacian\n",
    "\n",
    "\n",
    "def rescale_laplacian(laplacian):\n",
    "    \"\"\"\n",
    "    特征值调整到 [-1,1]\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\n",
    "            'Calculating largest eigenvalue of normalized graph Laplacian...')\n",
    "        # 计算对称归一化图拉普拉斯矩阵的最大特征值\n",
    "        largest_eigval = eigsh(laplacian,\n",
    "                               1,\n",
    "                               which='LM',\n",
    "                               return_eigenvectors=False)[0]\n",
    "    # 如果计算过程不收敛 : 直接设置为 2\n",
    "    except ArpackNoConvergence:\n",
    "        print(\n",
    "            'Eigenvalue calculation did not converge! Using largest_eigval=2 instead.'\n",
    "        )\n",
    "        largest_eigval = 2\n",
    "\n",
    "    # 调整后的对称归一化图拉普拉斯矩阵，L~ = 2 / Lambda * L - I\n",
    "    scaled_laplacian = (2. / largest_eigval) * laplacian - sp.eye(\n",
    "        laplacian.shape[0])\n",
    "    return scaled_laplacian\n",
    "\n",
    "\n",
    "def chebyshev_polynomial(X, k):\n",
    "    \"\"\"\n",
    "    Calculate Chebyshev polynomials up to order k. \n",
    "    Return a list of sparse matrices.\n",
    "    \"\"\"\n",
    "    print(\"Calculating Chebyshev polynomials up to order {}...\".format(k))\n",
    "\n",
    "    T_k = list()\n",
    "    T_k.append(sp.eye(X.shape[0]).tocsr())  # T0(X) = I\n",
    "    T_k.append(X)  # T1(X) = L~\n",
    "\n",
    "    def chebyshev_recurrence(T_k_minus_one, T_k_minus_two, X):\n",
    "        \"\"\"\n",
    "        定义切比雪夫递归公式\n",
    "        :param T_k_minus_one: T(k-1)(L~)\n",
    "        :param T_k_minus_two: T(k-2)(L~)\n",
    "        :param X: L~\n",
    "        :return: Tk(L~)\n",
    "        \"\"\"\n",
    "        X_ = sp.csr_matrix(X, copy=True)  # 将输入转化为csr矩阵（压缩稀疏行矩阵）\n",
    "        # 递归公式：Tk(L~) = 2L~ * T(k-1)(L~) - T(k-2)(L~)\n",
    "        return 2 * X_.dot(T_k_minus_one) - T_k_minus_two\n",
    "\n",
    "    for i in range(2, k + 1):\n",
    "        T_k.append(chebyshev_recurrence(T_k[-1], T_k[-2], X))\n",
    "\n",
    "    # 返回切比雪夫多项式列表\n",
    "    return T_k\n",
    "\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\"\n",
    "     将稀疏矩阵转化为元组表示\n",
    "    \"\"\"\n",
    "    if not sp.isspmatrix_coo(sparse_mx):\n",
    "        # 将稀疏矩阵转化为coo矩阵形式\n",
    "        # coo矩阵采用三个数组分别存储行、列和非零元素值的信息\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack(\n",
    "        (sparse_mx.row, sparse_mx.col)).transpose()  # 获取非零元素的位置索引\n",
    "    values = sparse_mx.data  # 获取矩阵的非零元素\n",
    "    shape = sparse_mx.shape  # 获取矩阵的形状\n",
    "\n",
    "    return coords, values, shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **GCN 层**\n",
    " \n",
    " \n",
    "\n",
    "<img style=\"float:;\" src=\"https://tva1.sinaimg.cn/large/007S8ZIlly1ge846nffkhj30wt0gyjvd.jpg\" width=\"40%\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T02:44:09.891881Z",
     "start_time": "2020-04-27T02:44:09.872774Z"
    },
    "code_folding": [
     0,
     17,
     45,
     72,
     90
    ]
   },
   "outputs": [],
   "source": [
    "class GraphConvolution(Layer):\n",
    "    \"\"\"\n",
    "    Basic graph convolution layer as in \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 units,\n",
    "                 support=1,\n",
    "                 activation=None,\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            #不同 keras 版本\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'), )\n",
    "        super(GraphConvolution, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "        self.supports_masking = True\n",
    "        self.support = support  # 多个图的情况\n",
    "        assert support >= 1\n",
    "\n",
    "    ### 参考 https://keras.io/zh/layers/writing-your-own-keras-layers/\n",
    "    def compute_output_shape(self, input_shapes):\n",
    "        \"\"\"\n",
    "        层更改了输入张量的形状\n",
    "        让Keras能够自动推断各层的形状\n",
    "        \"\"\"\n",
    "        features_shape = input_shapes[0] #  n * c ， 输入列表的第一部分\n",
    "        output_shape = (features_shape[0], self.units) # 输出  n * f (输出特征维度)\n",
    "        return output_shape  # (batch_size, output_dim)\n",
    "\n",
    "    def build(self, input_shapes):\n",
    "        \"\"\"\n",
    "        定义层中的权重\n",
    "        为该层创建一个可训练的权重\n",
    "        \"\"\"\n",
    "        features_shape = input_shapes[0]  #  n * c ， 输入列表的第一部分\n",
    "        assert len(features_shape) == 2\n",
    "        input_dim = features_shape[1] #  权重 维度 ： c * f（unit）\n",
    "\n",
    "        self.kernel = self.add_weight(shape=(input_dim * self.support,\n",
    "                                             self.units),\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      name='kernel',\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "        # 如果存在偏置\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.units, ),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        name='bias',\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        self.built = True  # 必须设定self.bulit = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        \"\"\"\n",
    "        编写层的功能逻辑\n",
    "        \"\"\"\n",
    "        features = inputs[0]  # 特征\n",
    "        basis = inputs[1:]  # 对称归一化的邻接矩阵， 输入列表的第二部分\n",
    "\n",
    "        supports = list()  # 多个图的情况\n",
    "        for i in range(self.support):\n",
    "            supports.append(K.dot(basis[i], features))  # A * X\n",
    "        supports = K.concatenate(supports, axis=1)  # 将多个图的结果按行拼接\n",
    "        output = K.dot(supports, self.kernel)  # A * X * W\n",
    "\n",
    "        if self.bias:\n",
    "            # A * X * W + b\n",
    "            output += self.bias\n",
    "        return self.activation(output)\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        当前层的配置信息\n",
    "        dictionary containing the configuration of the model\n",
    "        \"\"\"\n",
    "        config = {\n",
    "            'units':\n",
    "            self.units,\n",
    "            'support':\n",
    "            self.support,\n",
    "            'activation':\n",
    "            activations.serialize(self.activation),\n",
    "            'use_bias':\n",
    "            self.use_bias,\n",
    "            'kernel_initializer':\n",
    "            initializers.serialize(self.kernel_initializer),\n",
    "            'bias_initializer':\n",
    "            initializers.serialize(self.bias_initializer),\n",
    "            'kernel_regularizer':\n",
    "            regularizers.serialize(self.kernel_regularizer),\n",
    "            'bias_regularizer':\n",
    "            regularizers.serialize(self.bias_regularizer),\n",
    "            'activity_regularizer':\n",
    "            regularizers.serialize(self.activity_regularizer),\n",
    "            'kernel_constraint':\n",
    "            constraints.serialize(self.kernel_constraint),\n",
    "            'bias_constraint':\n",
    "            constraints.serialize(self.bias_constraint)\n",
    "        }\n",
    "\n",
    "        base_config = super(GraphConvolution, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T02:44:45.197002Z",
     "start_time": "2020-04-27T02:44:10.995323Z"
    },
    "code_folding": [
     22,
     35
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n",
      "Dataset has 2708 nodes, 5429 edges, 1433 features.\n",
      "Using local pooling filters...\n",
      "Epoch: 0001 train_loss= 1.9363 train_acc= 0.2214 val_loss= 1.9395 val_acc= 0.1600 time= 1.0304\n",
      "Epoch: 0002 train_loss= 1.9225 train_acc= 0.2000 val_loss= 1.9290 val_acc= 0.1600 time= 0.0714\n",
      "Epoch: 0003 train_loss= 1.9081 train_acc= 0.2000 val_loss= 1.9182 val_acc= 0.1567 time= 0.0608\n",
      "Epoch: 0004 train_loss= 1.8933 train_acc= 0.2000 val_loss= 1.9069 val_acc= 0.1567 time= 0.0636\n",
      "Epoch: 0005 train_loss= 1.8786 train_acc= 0.2000 val_loss= 1.8960 val_acc= 0.1567 time= 0.0659\n",
      "Epoch: 0006 train_loss= 1.8631 train_acc= 0.2000 val_loss= 1.8847 val_acc= 0.1567 time= 0.0626\n",
      "Epoch: 0007 train_loss= 1.8474 train_acc= 0.2000 val_loss= 1.8735 val_acc= 0.1567 time= 0.0598\n",
      "Epoch: 0008 train_loss= 1.8327 train_acc= 0.2000 val_loss= 1.8631 val_acc= 0.1567 time= 0.0596\n",
      "Epoch: 0009 train_loss= 1.8182 train_acc= 0.2000 val_loss= 1.8532 val_acc= 0.1567 time= 0.0664\n",
      "Epoch: 0010 train_loss= 1.8039 train_acc= 0.2000 val_loss= 1.8432 val_acc= 0.1567 time= 0.0616\n",
      "Epoch: 0011 train_loss= 1.7900 train_acc= 0.2000 val_loss= 1.8335 val_acc= 0.1567 time= 0.0601\n",
      "Epoch: 0012 train_loss= 1.7766 train_acc= 0.2000 val_loss= 1.8240 val_acc= 0.1567 time= 0.0637\n",
      "Epoch: 0013 train_loss= 1.7638 train_acc= 0.2071 val_loss= 1.8147 val_acc= 0.1567 time= 0.0645\n",
      "Epoch: 0014 train_loss= 1.7514 train_acc= 0.2500 val_loss= 1.8055 val_acc= 0.1700 time= 0.0646\n",
      "Epoch: 0015 train_loss= 1.7395 train_acc= 0.2571 val_loss= 1.7964 val_acc= 0.2067 time= 0.0620\n",
      "Epoch: 0016 train_loss= 1.7279 train_acc= 0.3143 val_loss= 1.7873 val_acc= 0.2533 time= 0.0595\n",
      "Epoch: 0017 train_loss= 1.7165 train_acc= 0.3929 val_loss= 1.7783 val_acc= 0.3200 time= 0.0654\n",
      "Epoch: 0018 train_loss= 1.7053 train_acc= 0.4214 val_loss= 1.7693 val_acc= 0.3800 time= 0.0637\n",
      "Epoch: 0019 train_loss= 1.6945 train_acc= 0.4571 val_loss= 1.7603 val_acc= 0.4133 time= 0.0597\n",
      "Epoch: 0020 train_loss= 1.6838 train_acc= 0.4643 val_loss= 1.7515 val_acc= 0.4367 time= 0.0589\n",
      "Epoch: 0021 train_loss= 1.6733 train_acc= 0.4714 val_loss= 1.7430 val_acc= 0.4433 time= 0.0753\n",
      "Epoch: 0022 train_loss= 1.6628 train_acc= 0.4929 val_loss= 1.7345 val_acc= 0.4667 time= 0.0701\n",
      "Epoch: 0023 train_loss= 1.6523 train_acc= 0.4929 val_loss= 1.7263 val_acc= 0.4733 time= 0.0628\n",
      "Epoch: 0024 train_loss= 1.6418 train_acc= 0.4929 val_loss= 1.7183 val_acc= 0.4800 time= 0.0608\n",
      "Epoch: 0025 train_loss= 1.6312 train_acc= 0.4857 val_loss= 1.7104 val_acc= 0.4733 time= 0.0718\n",
      "Epoch: 0026 train_loss= 1.6206 train_acc= 0.4786 val_loss= 1.7028 val_acc= 0.4700 time= 0.0641\n",
      "Epoch: 0027 train_loss= 1.6102 train_acc= 0.4786 val_loss= 1.6954 val_acc= 0.4667 time= 0.0603\n",
      "Epoch: 0028 train_loss= 1.5997 train_acc= 0.4714 val_loss= 1.6878 val_acc= 0.4567 time= 0.0620\n",
      "Epoch: 0029 train_loss= 1.5895 train_acc= 0.4714 val_loss= 1.6804 val_acc= 0.4567 time= 0.0666\n",
      "Epoch: 0030 train_loss= 1.5793 train_acc= 0.4714 val_loss= 1.6731 val_acc= 0.4533 time= 0.0611\n",
      "Epoch: 0031 train_loss= 1.5692 train_acc= 0.4786 val_loss= 1.6659 val_acc= 0.4533 time= 0.0625\n",
      "Epoch: 0032 train_loss= 1.5591 train_acc= 0.4857 val_loss= 1.6590 val_acc= 0.4500 time= 0.0613\n",
      "Epoch: 0033 train_loss= 1.5490 train_acc= 0.4857 val_loss= 1.6524 val_acc= 0.4533 time= 0.0777\n",
      "Epoch: 0034 train_loss= 1.5386 train_acc= 0.4929 val_loss= 1.6456 val_acc= 0.4567 time= 0.0684\n",
      "Epoch: 0035 train_loss= 1.5282 train_acc= 0.5000 val_loss= 1.6390 val_acc= 0.4767 time= 0.0728\n",
      "Epoch: 0036 train_loss= 1.5175 train_acc= 0.5143 val_loss= 1.6325 val_acc= 0.4900 time= 0.0673\n",
      "Epoch: 0037 train_loss= 1.5068 train_acc= 0.5214 val_loss= 1.6261 val_acc= 0.5033 time= 0.0728\n",
      "Epoch: 0038 train_loss= 1.4962 train_acc= 0.5286 val_loss= 1.6195 val_acc= 0.5033 time= 0.0661\n",
      "Epoch: 0039 train_loss= 1.4854 train_acc= 0.5357 val_loss= 1.6127 val_acc= 0.5200 time= 0.0614\n",
      "Epoch: 0040 train_loss= 1.4746 train_acc= 0.5429 val_loss= 1.6056 val_acc= 0.5233 time= 0.0690\n",
      "Epoch: 0041 train_loss= 1.4636 train_acc= 0.5571 val_loss= 1.5979 val_acc= 0.5233 time= 0.0644\n",
      "Epoch: 0042 train_loss= 1.4525 train_acc= 0.5643 val_loss= 1.5897 val_acc= 0.5300 time= 0.0612\n",
      "Epoch: 0043 train_loss= 1.4414 train_acc= 0.5786 val_loss= 1.5810 val_acc= 0.5333 time= 0.0600\n",
      "Epoch: 0044 train_loss= 1.4305 train_acc= 0.5714 val_loss= 1.5721 val_acc= 0.5300 time= 0.0669\n",
      "Epoch: 0045 train_loss= 1.4199 train_acc= 0.5714 val_loss= 1.5632 val_acc= 0.5233 time= 0.0624\n",
      "Epoch: 0046 train_loss= 1.4097 train_acc= 0.5571 val_loss= 1.5545 val_acc= 0.5233 time= 0.0638\n",
      "Epoch: 0047 train_loss= 1.3992 train_acc= 0.5643 val_loss= 1.5461 val_acc= 0.5233 time= 0.0636\n",
      "Epoch: 0048 train_loss= 1.3886 train_acc= 0.5643 val_loss= 1.5378 val_acc= 0.5200 time= 0.0692\n",
      "Epoch: 0049 train_loss= 1.3778 train_acc= 0.5643 val_loss= 1.5298 val_acc= 0.5167 time= 0.0612\n",
      "Epoch: 0050 train_loss= 1.3670 train_acc= 0.5857 val_loss= 1.5221 val_acc= 0.5233 time= 0.0656\n",
      "Epoch: 0051 train_loss= 1.3562 train_acc= 0.5929 val_loss= 1.5147 val_acc= 0.5233 time= 0.0677\n",
      "Epoch: 0052 train_loss= 1.3453 train_acc= 0.6286 val_loss= 1.5075 val_acc= 0.5333 time= 0.0780\n",
      "Epoch: 0053 train_loss= 1.3348 train_acc= 0.6357 val_loss= 1.5007 val_acc= 0.5467 time= 0.0651\n",
      "Epoch: 0054 train_loss= 1.3246 train_acc= 0.6500 val_loss= 1.4939 val_acc= 0.5567 time= 0.0605\n",
      "Epoch: 0055 train_loss= 1.3144 train_acc= 0.6643 val_loss= 1.4869 val_acc= 0.5700 time= 0.0627\n",
      "Epoch: 0056 train_loss= 1.3041 train_acc= 0.6643 val_loss= 1.4795 val_acc= 0.5767 time= 0.0761\n",
      "Epoch: 0057 train_loss= 1.2937 train_acc= 0.6714 val_loss= 1.4715 val_acc= 0.5767 time= 0.0946\n",
      "Epoch: 0058 train_loss= 1.2834 train_acc= 0.6714 val_loss= 1.4635 val_acc= 0.5767 time= 0.0802\n",
      "Epoch: 0059 train_loss= 1.2731 train_acc= 0.6786 val_loss= 1.4554 val_acc= 0.5767 time= 0.0969\n",
      "Epoch: 0060 train_loss= 1.2628 train_acc= 0.7000 val_loss= 1.4470 val_acc= 0.5867 time= 0.0892\n",
      "Epoch: 0061 train_loss= 1.2524 train_acc= 0.7000 val_loss= 1.4385 val_acc= 0.5867 time= 0.0973\n",
      "Epoch: 0062 train_loss= 1.2420 train_acc= 0.7071 val_loss= 1.4301 val_acc= 0.5933 time= 0.0851\n",
      "Epoch: 0063 train_loss= 1.2317 train_acc= 0.7143 val_loss= 1.4220 val_acc= 0.5967 time= 0.1690\n",
      "Epoch: 0064 train_loss= 1.2215 train_acc= 0.7143 val_loss= 1.4140 val_acc= 0.6100 time= 0.0711\n",
      "Epoch: 0065 train_loss= 1.2111 train_acc= 0.7214 val_loss= 1.4061 val_acc= 0.6100 time= 0.0714\n",
      "Epoch: 0066 train_loss= 1.2008 train_acc= 0.7214 val_loss= 1.3981 val_acc= 0.6100 time= 0.0715\n",
      "Epoch: 0067 train_loss= 1.1907 train_acc= 0.7214 val_loss= 1.3902 val_acc= 0.6200 time= 0.0679\n",
      "Epoch: 0068 train_loss= 1.1807 train_acc= 0.7643 val_loss= 1.3825 val_acc= 0.6333 time= 0.0677\n",
      "Epoch: 0069 train_loss= 1.1709 train_acc= 0.7714 val_loss= 1.3748 val_acc= 0.6433 time= 0.0682\n",
      "Epoch: 0070 train_loss= 1.1614 train_acc= 0.7714 val_loss= 1.3672 val_acc= 0.6467 time= 0.0669\n",
      "Epoch: 0071 train_loss= 1.1519 train_acc= 0.7714 val_loss= 1.3590 val_acc= 0.6467 time= 0.0685\n",
      "Epoch: 0072 train_loss= 1.1422 train_acc= 0.7786 val_loss= 1.3508 val_acc= 0.6467 time= 0.0680\n",
      "Epoch: 0073 train_loss= 1.1326 train_acc= 0.7786 val_loss= 1.3429 val_acc= 0.6500 time= 0.0673\n",
      "Epoch: 0074 train_loss= 1.1230 train_acc= 0.7786 val_loss= 1.3355 val_acc= 0.6533 time= 0.0683\n",
      "Epoch: 0075 train_loss= 1.1135 train_acc= 0.7786 val_loss= 1.3282 val_acc= 0.6533 time= 0.0666\n",
      "Epoch: 0076 train_loss= 1.1042 train_acc= 0.7786 val_loss= 1.3211 val_acc= 0.6600 time= 0.0713\n",
      "Epoch: 0077 train_loss= 1.0950 train_acc= 0.7786 val_loss= 1.3138 val_acc= 0.6600 time= 0.0814\n",
      "Epoch: 0078 train_loss= 1.0857 train_acc= 0.7786 val_loss= 1.3066 val_acc= 0.6600 time= 0.0725\n",
      "Epoch: 0079 train_loss= 1.0763 train_acc= 0.7929 val_loss= 1.2994 val_acc= 0.6633 time= 0.0709\n",
      "Epoch: 0080 train_loss= 1.0665 train_acc= 0.7929 val_loss= 1.2920 val_acc= 0.6633 time= 0.0756\n",
      "Epoch: 0081 train_loss= 1.0569 train_acc= 0.8000 val_loss= 1.2847 val_acc= 0.6767 time= 0.0740\n",
      "Epoch: 0082 train_loss= 1.0474 train_acc= 0.8000 val_loss= 1.2776 val_acc= 0.6800 time= 0.0704\n",
      "Epoch: 0083 train_loss= 1.0381 train_acc= 0.8143 val_loss= 1.2707 val_acc= 0.6900 time= 0.0736\n",
      "Epoch: 0084 train_loss= 1.0291 train_acc= 0.8214 val_loss= 1.2638 val_acc= 0.7033 time= 0.0787\n",
      "Epoch: 0085 train_loss= 1.0201 train_acc= 0.8286 val_loss= 1.2572 val_acc= 0.7067 time= 0.0760\n",
      "Epoch: 0086 train_loss= 1.0115 train_acc= 0.8286 val_loss= 1.2508 val_acc= 0.7200 time= 0.0728\n",
      "Epoch: 0087 train_loss= 1.0028 train_acc= 0.8357 val_loss= 1.2444 val_acc= 0.7267 time= 0.0734\n",
      "Epoch: 0088 train_loss= 0.9945 train_acc= 0.8357 val_loss= 1.2375 val_acc= 0.7300 time= 0.0730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0089 train_loss= 0.9864 train_acc= 0.8357 val_loss= 1.2309 val_acc= 0.7300 time= 0.0718\n",
      "Epoch: 0090 train_loss= 0.9781 train_acc= 0.8357 val_loss= 1.2247 val_acc= 0.7300 time= 0.0709\n",
      "Epoch: 0091 train_loss= 0.9699 train_acc= 0.8429 val_loss= 1.2183 val_acc= 0.7300 time= 0.0796\n",
      "Epoch: 0092 train_loss= 0.9618 train_acc= 0.8429 val_loss= 1.2121 val_acc= 0.7300 time= 0.0708\n",
      "Epoch: 0093 train_loss= 0.9538 train_acc= 0.8357 val_loss= 1.2056 val_acc= 0.7300 time= 0.0704\n",
      "Epoch: 0094 train_loss= 0.9460 train_acc= 0.8357 val_loss= 1.1992 val_acc= 0.7267 time= 0.0715\n",
      "Epoch: 0095 train_loss= 0.9384 train_acc= 0.8429 val_loss= 1.1929 val_acc= 0.7267 time= 0.0818\n",
      "Epoch: 0096 train_loss= 0.9310 train_acc= 0.8429 val_loss= 1.1869 val_acc= 0.7300 time= 0.0785\n",
      "Epoch: 0097 train_loss= 0.9237 train_acc= 0.8429 val_loss= 1.1809 val_acc= 0.7333 time= 0.0739\n",
      "Epoch: 0098 train_loss= 0.9163 train_acc= 0.8429 val_loss= 1.1753 val_acc= 0.7433 time= 0.0730\n",
      "Epoch: 0099 train_loss= 0.9090 train_acc= 0.8429 val_loss= 1.1703 val_acc= 0.7533 time= 0.0751\n",
      "Epoch: 0100 train_loss= 0.9017 train_acc= 0.8500 val_loss= 1.1648 val_acc= 0.7533 time= 0.0877\n",
      "Epoch: 0101 train_loss= 0.8941 train_acc= 0.8500 val_loss= 1.1591 val_acc= 0.7500 time= 0.0848\n",
      "Epoch: 0102 train_loss= 0.8866 train_acc= 0.8500 val_loss= 1.1527 val_acc= 0.7533 time= 0.1101\n",
      "Epoch: 0103 train_loss= 0.8795 train_acc= 0.8500 val_loss= 1.1466 val_acc= 0.7500 time= 0.0643\n",
      "Epoch: 0104 train_loss= 0.8725 train_acc= 0.8571 val_loss= 1.1404 val_acc= 0.7433 time= 0.0805\n",
      "Epoch: 0105 train_loss= 0.8658 train_acc= 0.8571 val_loss= 1.1347 val_acc= 0.7400 time= 0.0654\n",
      "Epoch: 0106 train_loss= 0.8591 train_acc= 0.8643 val_loss= 1.1285 val_acc= 0.7433 time= 0.0755\n",
      "Epoch: 0107 train_loss= 0.8526 train_acc= 0.8643 val_loss= 1.1226 val_acc= 0.7367 time= 0.0658\n",
      "Epoch: 0108 train_loss= 0.8460 train_acc= 0.8643 val_loss= 1.1169 val_acc= 0.7400 time= 0.0687\n",
      "Epoch: 0109 train_loss= 0.8395 train_acc= 0.8643 val_loss= 1.1119 val_acc= 0.7500 time= 0.0652\n",
      "Epoch: 0110 train_loss= 0.8331 train_acc= 0.8714 val_loss= 1.1076 val_acc= 0.7533 time= 0.0655\n",
      "Epoch: 0111 train_loss= 0.8272 train_acc= 0.8857 val_loss= 1.1042 val_acc= 0.7533 time= 0.0648\n",
      "Epoch: 0112 train_loss= 0.8218 train_acc= 0.8857 val_loss= 1.1009 val_acc= 0.7533 time= 0.0659\n",
      "Epoch: 0113 train_loss= 0.8165 train_acc= 0.8857 val_loss= 1.0979 val_acc= 0.7533 time= 0.0656\n",
      "Epoch: 0114 train_loss= 0.8113 train_acc= 0.8857 val_loss= 1.0958 val_acc= 0.7600 time= 0.0654\n",
      "Epoch: 0115 train_loss= 0.8059 train_acc= 0.8857 val_loss= 1.0923 val_acc= 0.7600 time= 0.0678\n",
      "Epoch: 0116 train_loss= 0.8004 train_acc= 0.8857 val_loss= 1.0884 val_acc= 0.7600 time= 0.0645\n",
      "Epoch: 0117 train_loss= 0.7944 train_acc= 0.8857 val_loss= 1.0833 val_acc= 0.7567 time= 0.0629\n",
      "Epoch: 0118 train_loss= 0.7882 train_acc= 0.8857 val_loss= 1.0772 val_acc= 0.7567 time= 0.0622\n",
      "Epoch: 0119 train_loss= 0.7822 train_acc= 0.8857 val_loss= 1.0707 val_acc= 0.7600 time= 0.0632\n",
      "Epoch: 0120 train_loss= 0.7766 train_acc= 0.8857 val_loss= 1.0648 val_acc= 0.7600 time= 0.0614\n",
      "Epoch: 0121 train_loss= 0.7718 train_acc= 0.8786 val_loss= 1.0601 val_acc= 0.7500 time= 0.0627\n",
      "Epoch: 0122 train_loss= 0.7667 train_acc= 0.8786 val_loss= 1.0557 val_acc= 0.7467 time= 0.0671\n",
      "Epoch: 0123 train_loss= 0.7614 train_acc= 0.8857 val_loss= 1.0515 val_acc= 0.7567 time= 0.0644\n",
      "Epoch: 0124 train_loss= 0.7560 train_acc= 0.8929 val_loss= 1.0479 val_acc= 0.7567 time= 0.0630\n",
      "Epoch: 0125 train_loss= 0.7509 train_acc= 0.9071 val_loss= 1.0449 val_acc= 0.7633 time= 0.0622\n",
      "Epoch: 0126 train_loss= 0.7460 train_acc= 0.9071 val_loss= 1.0428 val_acc= 0.7667 time= 0.0625\n",
      "Epoch: 0127 train_loss= 0.7415 train_acc= 0.9071 val_loss= 1.0407 val_acc= 0.7700 time= 0.0636\n",
      "Epoch: 0128 train_loss= 0.7371 train_acc= 0.9143 val_loss= 1.0383 val_acc= 0.7733 time= 0.0630\n",
      "Epoch: 0129 train_loss= 0.7326 train_acc= 0.9071 val_loss= 1.0360 val_acc= 0.7767 time= 0.0638\n",
      "Epoch: 0130 train_loss= 0.7281 train_acc= 0.9071 val_loss= 1.0327 val_acc= 0.7733 time= 0.0630\n",
      "Epoch: 0131 train_loss= 0.7233 train_acc= 0.9000 val_loss= 1.0285 val_acc= 0.7733 time= 0.0643\n",
      "Epoch: 0132 train_loss= 0.7184 train_acc= 0.9000 val_loss= 1.0238 val_acc= 0.7733 time= 0.0628\n",
      "Epoch: 0133 train_loss= 0.7135 train_acc= 0.9000 val_loss= 1.0184 val_acc= 0.7733 time= 0.0633\n",
      "Epoch: 0134 train_loss= 0.7087 train_acc= 0.9000 val_loss= 1.0134 val_acc= 0.7700 time= 0.0636\n",
      "Epoch: 0135 train_loss= 0.7039 train_acc= 0.9143 val_loss= 1.0089 val_acc= 0.7733 time= 0.0633\n",
      "Epoch: 0136 train_loss= 0.6994 train_acc= 0.9143 val_loss= 1.0046 val_acc= 0.7733 time= 0.0653\n",
      "Epoch: 0137 train_loss= 0.6953 train_acc= 0.9143 val_loss= 1.0009 val_acc= 0.7733 time= 0.0690\n",
      "Epoch: 0138 train_loss= 0.6920 train_acc= 0.9143 val_loss= 0.9966 val_acc= 0.7733 time= 0.0619\n",
      "Epoch: 0139 train_loss= 0.6881 train_acc= 0.9143 val_loss= 0.9935 val_acc= 0.7767 time= 0.0625\n",
      "Epoch: 0140 train_loss= 0.6839 train_acc= 0.9143 val_loss= 0.9904 val_acc= 0.7767 time= 0.0636\n",
      "Epoch: 0141 train_loss= 0.6790 train_acc= 0.9143 val_loss= 0.9877 val_acc= 0.7833 time= 0.0655\n",
      "Epoch: 0142 train_loss= 0.6745 train_acc= 0.9143 val_loss= 0.9853 val_acc= 0.7900 time= 0.0694\n",
      "Epoch: 0143 train_loss= 0.6705 train_acc= 0.9143 val_loss= 0.9836 val_acc= 0.7867 time= 0.0692\n",
      "Epoch: 0144 train_loss= 0.6664 train_acc= 0.9143 val_loss= 0.9814 val_acc= 0.7833 time= 0.0678\n",
      "Epoch: 0145 train_loss= 0.6624 train_acc= 0.9214 val_loss= 0.9790 val_acc= 0.7833 time= 0.0669\n",
      "Epoch: 0146 train_loss= 0.6584 train_acc= 0.9214 val_loss= 0.9761 val_acc= 0.7900 time= 0.0998\n",
      "Epoch: 0147 train_loss= 0.6537 train_acc= 0.9214 val_loss= 0.9710 val_acc= 0.7900 time= 0.1183\n",
      "Epoch: 0148 train_loss= 0.6493 train_acc= 0.9143 val_loss= 0.9659 val_acc= 0.7800 time= 0.1131\n",
      "Epoch: 0149 train_loss= 0.6456 train_acc= 0.9143 val_loss= 0.9610 val_acc= 0.7833 time= 0.0693\n",
      "Epoch: 0150 train_loss= 0.6427 train_acc= 0.9143 val_loss= 0.9570 val_acc= 0.7833 time= 0.0647\n",
      "Epoch: 0151 train_loss= 0.6391 train_acc= 0.9143 val_loss= 0.9536 val_acc= 0.7833 time= 0.0671\n",
      "Epoch: 0152 train_loss= 0.6350 train_acc= 0.9143 val_loss= 0.9501 val_acc= 0.7833 time= 0.0731\n",
      "Epoch: 0153 train_loss= 0.6300 train_acc= 0.9214 val_loss= 0.9466 val_acc= 0.7833 time= 0.0799\n",
      "Epoch: 0154 train_loss= 0.6249 train_acc= 0.9286 val_loss= 0.9443 val_acc= 0.7900 time= 0.0743\n",
      "Epoch: 0155 train_loss= 0.6210 train_acc= 0.9357 val_loss= 0.9439 val_acc= 0.7833 time= 0.0703\n",
      "Epoch: 0156 train_loss= 0.6177 train_acc= 0.9357 val_loss= 0.9432 val_acc= 0.7867 time= 0.0701\n",
      "Epoch: 0157 train_loss= 0.6146 train_acc= 0.9286 val_loss= 0.9418 val_acc= 0.7867 time= 0.0784\n",
      "Epoch: 0158 train_loss= 0.6111 train_acc= 0.9286 val_loss= 0.9390 val_acc= 0.7867 time= 0.0704\n",
      "Epoch: 0159 train_loss= 0.6079 train_acc= 0.9286 val_loss= 0.9372 val_acc= 0.7933 time= 0.0722\n",
      "Epoch: 0160 train_loss= 0.6048 train_acc= 0.9214 val_loss= 0.9341 val_acc= 0.7933 time= 0.0662\n",
      "Epoch: 0161 train_loss= 0.6022 train_acc= 0.9214 val_loss= 0.9308 val_acc= 0.7933 time= 0.0670\n",
      "Epoch: 0162 train_loss= 0.6001 train_acc= 0.9214 val_loss= 0.9281 val_acc= 0.7933 time= 0.0667\n",
      "Epoch: 0163 train_loss= 0.5975 train_acc= 0.9214 val_loss= 0.9250 val_acc= 0.7933 time= 0.0650\n",
      "Epoch: 0164 train_loss= 0.5951 train_acc= 0.9214 val_loss= 0.9218 val_acc= 0.7900 time= 0.1033\n",
      "Epoch: 0165 train_loss= 0.5922 train_acc= 0.9214 val_loss= 0.9188 val_acc= 0.7867 time= 0.0675\n",
      "Epoch: 0166 train_loss= 0.5895 train_acc= 0.9214 val_loss= 0.9162 val_acc= 0.7867 time= 0.0779\n",
      "Epoch: 0167 train_loss= 0.5860 train_acc= 0.9214 val_loss= 0.9141 val_acc= 0.7867 time= 0.1046\n",
      "Epoch: 0168 train_loss= 0.5826 train_acc= 0.9214 val_loss= 0.9121 val_acc= 0.7867 time= 0.0848\n",
      "Epoch: 0169 train_loss= 0.5783 train_acc= 0.9214 val_loss= 0.9108 val_acc= 0.7933 time= 0.0635\n",
      "Epoch: 0170 train_loss= 0.5744 train_acc= 0.9214 val_loss= 0.9102 val_acc= 0.7933 time= 0.0632\n",
      "Epoch: 0171 train_loss= 0.5713 train_acc= 0.9214 val_loss= 0.9099 val_acc= 0.7967 time= 0.0620\n",
      "Epoch: 0172 train_loss= 0.5684 train_acc= 0.9214 val_loss= 0.9088 val_acc= 0.7967 time= 0.0644\n",
      "Epoch: 0173 train_loss= 0.5653 train_acc= 0.9214 val_loss= 0.9059 val_acc= 0.7967 time= 0.0659\n",
      "Epoch: 0174 train_loss= 0.5621 train_acc= 0.9286 val_loss= 0.9027 val_acc= 0.8000 time= 0.0640\n",
      "Epoch: 0175 train_loss= 0.5591 train_acc= 0.9286 val_loss= 0.8995 val_acc= 0.7967 time= 0.0653\n",
      "Epoch: 0176 train_loss= 0.5564 train_acc= 0.9286 val_loss= 0.8957 val_acc= 0.7967 time= 0.0677\n",
      "Epoch: 0177 train_loss= 0.5536 train_acc= 0.9357 val_loss= 0.8921 val_acc= 0.7967 time= 0.0619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0178 train_loss= 0.5509 train_acc= 0.9286 val_loss= 0.8899 val_acc= 0.7967 time= 0.0644\n",
      "Epoch: 0179 train_loss= 0.5483 train_acc= 0.9286 val_loss= 0.8889 val_acc= 0.8033 time= 0.0618\n",
      "Epoch: 0180 train_loss= 0.5459 train_acc= 0.9286 val_loss= 0.8883 val_acc= 0.8033 time= 0.0621\n",
      "Epoch: 0181 train_loss= 0.5438 train_acc= 0.9286 val_loss= 0.8871 val_acc= 0.8033 time= 0.0612\n",
      "Epoch: 0182 train_loss= 0.5414 train_acc= 0.9357 val_loss= 0.8843 val_acc= 0.8033 time= 0.0664\n",
      "Epoch: 0183 train_loss= 0.5390 train_acc= 0.9357 val_loss= 0.8818 val_acc= 0.8033 time= 0.0722\n",
      "Epoch: 0184 train_loss= 0.5366 train_acc= 0.9357 val_loss= 0.8789 val_acc= 0.8067 time= 0.0679\n",
      "Epoch: 0185 train_loss= 0.5345 train_acc= 0.9357 val_loss= 0.8766 val_acc= 0.8033 time= 0.0666\n",
      "Epoch: 0186 train_loss= 0.5325 train_acc= 0.9357 val_loss= 0.8742 val_acc= 0.8033 time= 0.0737\n",
      "Epoch: 0187 train_loss= 0.5300 train_acc= 0.9286 val_loss= 0.8730 val_acc= 0.8000 time= 0.0772\n",
      "Epoch: 0188 train_loss= 0.5273 train_acc= 0.9286 val_loss= 0.8718 val_acc= 0.8000 time= 0.0824\n",
      "Epoch: 0189 train_loss= 0.5244 train_acc= 0.9357 val_loss= 0.8696 val_acc= 0.7967 time= 0.0681\n",
      "Epoch: 0190 train_loss= 0.5216 train_acc= 0.9357 val_loss= 0.8672 val_acc= 0.7933 time= 0.0667\n",
      "Epoch: 0191 train_loss= 0.5190 train_acc= 0.9357 val_loss= 0.8654 val_acc= 0.7933 time= 0.0668\n",
      "Epoch: 0192 train_loss= 0.5167 train_acc= 0.9429 val_loss= 0.8640 val_acc= 0.7900 time= 0.1035\n",
      "Epoch: 0193 train_loss= 0.5142 train_acc= 0.9429 val_loss= 0.8630 val_acc= 0.7967 time= 0.0870\n",
      "Epoch: 0194 train_loss= 0.5116 train_acc= 0.9500 val_loss= 0.8599 val_acc= 0.7933 time= 0.0890\n",
      "Epoch: 0195 train_loss= 0.5096 train_acc= 0.9500 val_loss= 0.8574 val_acc= 0.7967 time= 0.0933\n",
      "Epoch: 0196 train_loss= 0.5080 train_acc= 0.9500 val_loss= 0.8558 val_acc= 0.7967 time= 0.0716\n",
      "Epoch: 0197 train_loss= 0.5064 train_acc= 0.9429 val_loss= 0.8554 val_acc= 0.7967 time= 0.0826\n",
      "Epoch: 0198 train_loss= 0.5049 train_acc= 0.9429 val_loss= 0.8561 val_acc= 0.8067 time= 0.0790\n",
      "Epoch: 0199 train_loss= 0.5031 train_acc= 0.9357 val_loss= 0.8558 val_acc= 0.8033 time= 0.0663\n",
      "Epoch: 0200 train_loss= 0.5015 train_acc= 0.9357 val_loss= 0.8568 val_acc= 0.8033 time= 0.0884\n",
      "Epoch: 0201 train_loss= 0.4991 train_acc= 0.9357 val_loss= 0.8542 val_acc= 0.8033 time= 0.1096\n",
      "Epoch: 0202 train_loss= 0.4972 train_acc= 0.9357 val_loss= 0.8513 val_acc= 0.8067 time= 0.0786\n",
      "Epoch: 0203 train_loss= 0.4954 train_acc= 0.9357 val_loss= 0.8489 val_acc= 0.8067 time= 0.0755\n",
      "Epoch: 0204 train_loss= 0.4939 train_acc= 0.9357 val_loss= 0.8470 val_acc= 0.8067 time= 0.0782\n",
      "Epoch: 0205 train_loss= 0.4926 train_acc= 0.9357 val_loss= 0.8454 val_acc= 0.8067 time= 0.0850\n",
      "Epoch: 0206 train_loss= 0.4911 train_acc= 0.9357 val_loss= 0.8447 val_acc= 0.8067 time= 0.0843\n",
      "Epoch: 0207 train_loss= 0.4891 train_acc= 0.9357 val_loss= 0.8441 val_acc= 0.8067 time= 0.0700\n",
      "Epoch: 0208 train_loss= 0.4877 train_acc= 0.9357 val_loss= 0.8437 val_acc= 0.8033 time= 0.0652\n",
      "Epoch: 0209 train_loss= 0.4866 train_acc= 0.9429 val_loss= 0.8441 val_acc= 0.8000 time= 0.0664\n",
      "Epoch: 0210 train_loss= 0.4845 train_acc= 0.9357 val_loss= 0.8442 val_acc= 0.8100 time= 0.0712\n",
      "Epoch: 0211 train_loss= 0.4821 train_acc= 0.9429 val_loss= 0.8427 val_acc= 0.8067 time= 0.0865\n",
      "Epoch: 0212 train_loss= 0.4792 train_acc= 0.9429 val_loss= 0.8414 val_acc= 0.8067 time= 0.0835\n",
      "Epoch: 0213 train_loss= 0.4765 train_acc= 0.9357 val_loss= 0.8412 val_acc= 0.8067 time= 0.0674\n",
      "Epoch: 0214 train_loss= 0.4742 train_acc= 0.9429 val_loss= 0.8402 val_acc= 0.8033 time= 0.0779\n",
      "Epoch: 0215 train_loss= 0.4718 train_acc= 0.9500 val_loss= 0.8395 val_acc= 0.8000 time= 0.0720\n",
      "Epoch: 0216 train_loss= 0.4692 train_acc= 0.9500 val_loss= 0.8373 val_acc= 0.8067 time= 0.0859\n",
      "Epoch: 0217 train_loss= 0.4669 train_acc= 0.9500 val_loss= 0.8348 val_acc= 0.8067 time= 0.0798\n",
      "Epoch: 0218 train_loss= 0.4649 train_acc= 0.9571 val_loss= 0.8317 val_acc= 0.8033 time= 0.0784\n",
      "Epoch: 0219 train_loss= 0.4631 train_acc= 0.9571 val_loss= 0.8280 val_acc= 0.8100 time= 0.0707\n",
      "Epoch: 0220 train_loss= 0.4618 train_acc= 0.9643 val_loss= 0.8252 val_acc= 0.8033 time= 0.0710\n",
      "Epoch: 0221 train_loss= 0.4603 train_acc= 0.9643 val_loss= 0.8245 val_acc= 0.8000 time= 0.0687\n",
      "Epoch: 0222 train_loss= 0.4592 train_acc= 0.9571 val_loss= 0.8250 val_acc= 0.7900 time= 0.0664\n",
      "Epoch: 0223 train_loss= 0.4587 train_acc= 0.9500 val_loss= 0.8268 val_acc= 0.7933 time= 0.0742\n",
      "Epoch: 0224 train_loss= 0.4582 train_acc= 0.9429 val_loss= 0.8288 val_acc= 0.7933 time= 0.0819\n",
      "Epoch: 0225 train_loss= 0.4577 train_acc= 0.9429 val_loss= 0.8304 val_acc= 0.7933 time= 0.0788\n",
      "Epoch: 0226 train_loss= 0.4567 train_acc= 0.9429 val_loss= 0.8296 val_acc= 0.8000 time= 0.0697\n",
      "Epoch: 0227 train_loss= 0.4559 train_acc= 0.9429 val_loss= 0.8284 val_acc= 0.8033 time= 0.0650\n",
      "Epoch: 0228 train_loss= 0.4559 train_acc= 0.9429 val_loss= 0.8267 val_acc= 0.8067 time= 0.0646\n",
      "Epoch: 0229 train_loss= 0.4568 train_acc= 0.9571 val_loss= 0.8271 val_acc= 0.8033 time= 0.0669\n",
      "Epoch: 0230 train_loss= 0.4570 train_acc= 0.9571 val_loss= 0.8261 val_acc= 0.8000 time= 0.0631\n",
      "Epoch: 0231 train_loss= 0.4564 train_acc= 0.9571 val_loss= 0.8237 val_acc= 0.8000 time= 0.0658\n",
      "Epoch: 0232 train_loss= 0.4544 train_acc= 0.9571 val_loss= 0.8208 val_acc= 0.8000 time= 0.0783\n",
      "Epoch: 0233 train_loss= 0.4512 train_acc= 0.9571 val_loss= 0.8176 val_acc= 0.7967 time= 0.0862\n",
      "Epoch: 0234 train_loss= 0.4470 train_acc= 0.9571 val_loss= 0.8147 val_acc= 0.8000 time= 0.0739\n",
      "Epoch: 0235 train_loss= 0.4431 train_acc= 0.9571 val_loss= 0.8136 val_acc= 0.8033 time= 0.0650\n",
      "Epoch: 0236 train_loss= 0.4401 train_acc= 0.9571 val_loss= 0.8124 val_acc= 0.7967 time= 0.0629\n",
      "Epoch: 0237 train_loss= 0.4377 train_acc= 0.9571 val_loss= 0.8121 val_acc= 0.7933 time= 0.0640\n",
      "Epoch: 0238 train_loss= 0.4353 train_acc= 0.9571 val_loss= 0.8106 val_acc= 0.7933 time= 0.0677\n",
      "Epoch: 0239 train_loss= 0.4334 train_acc= 0.9571 val_loss= 0.8083 val_acc= 0.7933 time= 0.0813\n",
      "Epoch: 0240 train_loss= 0.4315 train_acc= 0.9571 val_loss= 0.8064 val_acc= 0.7933 time= 0.0789\n",
      "Epoch: 0241 train_loss= 0.4298 train_acc= 0.9643 val_loss= 0.8045 val_acc= 0.8033 time= 0.0751\n",
      "Epoch: 0242 train_loss= 0.4287 train_acc= 0.9714 val_loss= 0.8033 val_acc= 0.8033 time= 0.0731\n",
      "Epoch: 0243 train_loss= 0.4272 train_acc= 0.9714 val_loss= 0.8033 val_acc= 0.8067 time= 0.0653\n",
      "Epoch: 0244 train_loss= 0.4259 train_acc= 0.9714 val_loss= 0.8046 val_acc= 0.8100 time= 0.0666\n",
      "Epoch: 0245 train_loss= 0.4247 train_acc= 0.9714 val_loss= 0.8060 val_acc= 0.8100 time= 0.0714\n",
      "Epoch: 0246 train_loss= 0.4236 train_acc= 0.9714 val_loss= 0.8067 val_acc= 0.8100 time= 0.0822\n",
      "Epoch: 0247 train_loss= 0.4222 train_acc= 0.9643 val_loss= 0.8066 val_acc= 0.8067 time= 0.0726\n",
      "Epoch: 0248 train_loss= 0.4200 train_acc= 0.9643 val_loss= 0.8039 val_acc= 0.8067 time= 0.0793\n",
      "Epoch: 0249 train_loss= 0.4179 train_acc= 0.9714 val_loss= 0.8000 val_acc= 0.8067 time= 0.0679\n",
      "Epoch: 0250 train_loss= 0.4158 train_acc= 0.9714 val_loss= 0.7968 val_acc= 0.8067 time= 0.0699\n",
      "Epoch: 0251 train_loss= 0.4139 train_acc= 0.9714 val_loss= 0.7930 val_acc= 0.8100 time= 0.0648\n",
      "Epoch: 0252 train_loss= 0.4124 train_acc= 0.9714 val_loss= 0.7898 val_acc= 0.8100 time= 0.0651\n",
      "Epoch: 0253 train_loss= 0.4111 train_acc= 0.9714 val_loss= 0.7864 val_acc= 0.8133 time= 0.0718\n",
      "Epoch: 0254 train_loss= 0.4098 train_acc= 0.9714 val_loss= 0.7843 val_acc= 0.8133 time= 0.0651\n",
      "Epoch: 0255 train_loss= 0.4083 train_acc= 0.9714 val_loss= 0.7825 val_acc= 0.8167 time= 0.0769\n",
      "Epoch: 0256 train_loss= 0.4068 train_acc= 0.9714 val_loss= 0.7811 val_acc= 0.8167 time= 0.0673\n",
      "Epoch: 0257 train_loss= 0.4047 train_acc= 0.9714 val_loss= 0.7805 val_acc= 0.8200 time= 0.0658\n",
      "Epoch: 0258 train_loss= 0.4028 train_acc= 0.9714 val_loss= 0.7809 val_acc= 0.8167 time= 0.0649\n",
      "Epoch: 0259 train_loss= 0.4019 train_acc= 0.9714 val_loss= 0.7842 val_acc= 0.8167 time= 0.0649\n",
      "Epoch: 0260 train_loss= 0.4021 train_acc= 0.9714 val_loss= 0.7884 val_acc= 0.8167 time= 0.0655\n",
      "Epoch: 0261 train_loss= 0.4012 train_acc= 0.9714 val_loss= 0.7892 val_acc= 0.8067 time= 0.0722\n",
      "Epoch: 0262 train_loss= 0.3996 train_acc= 0.9714 val_loss= 0.7879 val_acc= 0.8133 time= 0.0665\n",
      "Epoch: 0263 train_loss= 0.3973 train_acc= 0.9714 val_loss= 0.7834 val_acc= 0.8133 time= 0.0662\n",
      "Epoch: 0264 train_loss= 0.3962 train_acc= 0.9714 val_loss= 0.7790 val_acc= 0.8100 time= 0.0653\n",
      "Epoch: 0265 train_loss= 0.3964 train_acc= 0.9714 val_loss= 0.7764 val_acc= 0.8067 time= 0.0662\n",
      "Epoch: 0266 train_loss= 0.3971 train_acc= 0.9786 val_loss= 0.7753 val_acc= 0.8033 time= 0.0627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0267 train_loss= 0.3965 train_acc= 0.9786 val_loss= 0.7746 val_acc= 0.8033 time= 0.0646\n",
      "Epoch: 0268 train_loss= 0.3952 train_acc= 0.9714 val_loss= 0.7742 val_acc= 0.8033 time= 0.0634\n",
      "Epoch: 0269 train_loss= 0.3938 train_acc= 0.9714 val_loss= 0.7737 val_acc= 0.8033 time= 0.0640\n",
      "Epoch: 0270 train_loss= 0.3920 train_acc= 0.9643 val_loss= 0.7743 val_acc= 0.8033 time= 0.0627\n",
      "Epoch: 0271 train_loss= 0.3908 train_acc= 0.9643 val_loss= 0.7758 val_acc= 0.8067 time= 0.0670\n",
      "Epoch: 0272 train_loss= 0.3898 train_acc= 0.9643 val_loss= 0.7779 val_acc= 0.8000 time= 0.0639\n",
      "Epoch: 0273 train_loss= 0.3885 train_acc= 0.9571 val_loss= 0.7783 val_acc= 0.8000 time= 0.0622\n",
      "Epoch: 0274 train_loss= 0.3866 train_acc= 0.9643 val_loss= 0.7774 val_acc= 0.8000 time= 0.0635\n",
      "Epoch: 0275 train_loss= 0.3844 train_acc= 0.9714 val_loss= 0.7749 val_acc= 0.8033 time= 0.0643\n",
      "Epoch: 0276 train_loss= 0.3825 train_acc= 0.9714 val_loss= 0.7725 val_acc= 0.8067 time= 0.0623\n",
      "Epoch: 0277 train_loss= 0.3813 train_acc= 0.9786 val_loss= 0.7710 val_acc= 0.8100 time= 0.0628\n",
      "Epoch: 0278 train_loss= 0.3805 train_acc= 0.9786 val_loss= 0.7704 val_acc= 0.8133 time= 0.0642\n",
      "Epoch: 0279 train_loss= 0.3798 train_acc= 0.9786 val_loss= 0.7699 val_acc= 0.8133 time= 0.0633\n",
      "Epoch: 0280 train_loss= 0.3790 train_acc= 0.9786 val_loss= 0.7694 val_acc= 0.8100 time= 0.0617\n",
      "Epoch: 0281 train_loss= 0.3779 train_acc= 0.9786 val_loss= 0.7683 val_acc= 0.8067 time= 0.0620\n",
      "Epoch: 0282 train_loss= 0.3767 train_acc= 0.9786 val_loss= 0.7672 val_acc= 0.8100 time= 0.0624\n",
      "Epoch: 0283 train_loss= 0.3755 train_acc= 0.9786 val_loss= 0.7658 val_acc= 0.8133 time= 0.0629\n",
      "Epoch: 0284 train_loss= 0.3743 train_acc= 0.9714 val_loss= 0.7649 val_acc= 0.8133 time= 0.0669\n",
      "Epoch: 0285 train_loss= 0.3730 train_acc= 0.9714 val_loss= 0.7641 val_acc= 0.8133 time= 0.0623\n",
      "Epoch: 0286 train_loss= 0.3720 train_acc= 0.9714 val_loss= 0.7628 val_acc= 0.8100 time= 0.0622\n",
      "Epoch: 0287 train_loss= 0.3712 train_acc= 0.9714 val_loss= 0.7613 val_acc= 0.8100 time= 0.0670\n",
      "Epoch: 0288 train_loss= 0.3704 train_acc= 0.9714 val_loss= 0.7595 val_acc= 0.8133 time= 0.0649\n",
      "Epoch: 0289 train_loss= 0.3694 train_acc= 0.9714 val_loss= 0.7579 val_acc= 0.8167 time= 0.0623\n",
      "Epoch: 0290 train_loss= 0.3684 train_acc= 0.9714 val_loss= 0.7562 val_acc= 0.8167 time= 0.0625\n",
      "Epoch: 0291 train_loss= 0.3674 train_acc= 0.9714 val_loss= 0.7552 val_acc= 0.8167 time= 0.0685\n",
      "Epoch: 0292 train_loss= 0.3665 train_acc= 0.9714 val_loss= 0.7532 val_acc= 0.8200 time= 0.0622\n",
      "Epoch: 0293 train_loss= 0.3657 train_acc= 0.9714 val_loss= 0.7518 val_acc= 0.8200 time= 0.0652\n",
      "Epoch: 0294 train_loss= 0.3650 train_acc= 0.9714 val_loss= 0.7506 val_acc= 0.8167 time= 0.0636\n",
      "Epoch: 0295 train_loss= 0.3635 train_acc= 0.9714 val_loss= 0.7506 val_acc= 0.8167 time= 0.0629\n",
      "Epoch: 0296 train_loss= 0.3620 train_acc= 0.9714 val_loss= 0.7511 val_acc= 0.8133 time= 0.0652\n",
      "Epoch: 0297 train_loss= 0.3605 train_acc= 0.9714 val_loss= 0.7510 val_acc= 0.8200 time= 0.0611\n",
      "Epoch: 0298 train_loss= 0.3593 train_acc= 0.9714 val_loss= 0.7501 val_acc= 0.8167 time= 0.0624\n",
      "Epoch: 0299 train_loss= 0.3581 train_acc= 0.9714 val_loss= 0.7494 val_acc= 0.8200 time= 0.0655\n",
      "Epoch: 0300 train_loss= 0.3566 train_acc= 0.9714 val_loss= 0.7483 val_acc= 0.8167 time= 0.0620\n",
      "Epoch: 0301 train_loss= 0.3550 train_acc= 0.9714 val_loss= 0.7478 val_acc= 0.8167 time= 0.0640\n",
      "Epoch: 0302 train_loss= 0.3538 train_acc= 0.9714 val_loss= 0.7478 val_acc= 0.8133 time= 0.0646\n",
      "Epoch: 0303 train_loss= 0.3526 train_acc= 0.9714 val_loss= 0.7448 val_acc= 0.8200 time= 0.0659\n",
      "Epoch: 0304 train_loss= 0.3518 train_acc= 0.9714 val_loss= 0.7422 val_acc= 0.8200 time= 0.0639\n",
      "Epoch: 0305 train_loss= 0.3515 train_acc= 0.9714 val_loss= 0.7414 val_acc= 0.8200 time= 0.0689\n",
      "Epoch: 0306 train_loss= 0.3518 train_acc= 0.9714 val_loss= 0.7414 val_acc= 0.8200 time= 0.0770\n",
      "Epoch: 0307 train_loss= 0.3523 train_acc= 0.9714 val_loss= 0.7422 val_acc= 0.8200 time= 0.0747\n",
      "Epoch: 0308 train_loss= 0.3535 train_acc= 0.9714 val_loss= 0.7435 val_acc= 0.8200 time= 0.0752\n",
      "Epoch: 0309 train_loss= 0.3540 train_acc= 0.9714 val_loss= 0.7454 val_acc= 0.8200 time= 0.0755\n",
      "Epoch: 0310 train_loss= 0.3534 train_acc= 0.9857 val_loss= 0.7461 val_acc= 0.8133 time= 0.0721\n",
      "Epoch: 0311 train_loss= 0.3521 train_acc= 0.9857 val_loss= 0.7476 val_acc= 0.8067 time= 0.0656\n",
      "Epoch: 0312 train_loss= 0.3507 train_acc= 0.9857 val_loss= 0.7480 val_acc= 0.8067 time= 0.0685\n",
      "Epoch: 0313 train_loss= 0.3489 train_acc= 0.9786 val_loss= 0.7463 val_acc= 0.8067 time= 0.0683\n",
      "Epoch: 0314 train_loss= 0.3479 train_acc= 0.9786 val_loss= 0.7454 val_acc= 0.8067 time= 0.0783\n",
      "Epoch: 0315 train_loss= 0.3472 train_acc= 0.9786 val_loss= 0.7437 val_acc= 0.8067 time= 0.0769\n",
      "Epoch: 0316 train_loss= 0.3468 train_acc= 0.9714 val_loss= 0.7425 val_acc= 0.8167 time= 0.0814\n",
      "Epoch: 0317 train_loss= 0.3458 train_acc= 0.9714 val_loss= 0.7410 val_acc= 0.8167 time= 0.0737\n",
      "Epoch: 0318 train_loss= 0.3439 train_acc= 0.9714 val_loss= 0.7381 val_acc= 0.8167 time= 0.0726\n",
      "Epoch: 0319 train_loss= 0.3414 train_acc= 0.9714 val_loss= 0.7347 val_acc= 0.8200 time= 0.0749\n",
      "Epoch: 0320 train_loss= 0.3384 train_acc= 0.9714 val_loss= 0.7324 val_acc= 0.8233 time= 0.0715\n",
      "Epoch: 0321 train_loss= 0.3371 train_acc= 0.9786 val_loss= 0.7315 val_acc= 0.8233 time= 0.0677\n",
      "Epoch: 0322 train_loss= 0.3366 train_acc= 0.9786 val_loss= 0.7320 val_acc= 0.8267 time= 0.0740\n",
      "Epoch: 0323 train_loss= 0.3362 train_acc= 0.9786 val_loss= 0.7333 val_acc= 0.8300 time= 0.0776\n",
      "Epoch: 0324 train_loss= 0.3352 train_acc= 0.9786 val_loss= 0.7339 val_acc= 0.8300 time= 0.0757\n",
      "Epoch: 0325 train_loss= 0.3342 train_acc= 0.9786 val_loss= 0.7340 val_acc= 0.8300 time= 0.0735\n",
      "Epoch: 0326 train_loss= 0.3330 train_acc= 0.9786 val_loss= 0.7333 val_acc= 0.8300 time= 0.0757\n",
      "Epoch: 0327 train_loss= 0.3315 train_acc= 0.9786 val_loss= 0.7316 val_acc= 0.8300 time= 0.0766\n",
      "Epoch: 0328 train_loss= 0.3318 train_acc= 0.9714 val_loss= 0.7307 val_acc= 0.8300 time= 0.0635\n",
      "Epoch: 0329 train_loss= 0.3331 train_acc= 0.9714 val_loss= 0.7322 val_acc= 0.8300 time= 0.0657\n",
      "Epoch: 0330 train_loss= 0.3334 train_acc= 0.9714 val_loss= 0.7339 val_acc= 0.8200 time= 0.0681\n",
      "Epoch: 0331 train_loss= 0.3327 train_acc= 0.9786 val_loss= 0.7360 val_acc= 0.8267 time= 0.0634\n",
      "Epoch: 0332 train_loss= 0.3317 train_acc= 0.9857 val_loss= 0.7382 val_acc= 0.8233 time= 0.0667\n",
      "Epoch: 0333 train_loss= 0.3306 train_acc= 0.9857 val_loss= 0.7396 val_acc= 0.8233 time= 0.0622\n",
      "Epoch: 0334 train_loss= 0.3289 train_acc= 0.9857 val_loss= 0.7409 val_acc= 0.8233 time= 0.0619\n",
      "Epoch: 0335 train_loss= 0.3273 train_acc= 0.9786 val_loss= 0.7409 val_acc= 0.8200 time= 0.0653\n",
      "Epoch: 0336 train_loss= 0.3257 train_acc= 0.9786 val_loss= 0.7382 val_acc= 0.8200 time= 0.0647\n",
      "Epoch: 0337 train_loss= 0.3233 train_acc= 0.9786 val_loss= 0.7326 val_acc= 0.8233 time= 0.0653\n",
      "Epoch: 0338 train_loss= 0.3218 train_acc= 0.9786 val_loss= 0.7274 val_acc= 0.8233 time= 0.0646\n",
      "Epoch: 0339 train_loss= 0.3212 train_acc= 0.9786 val_loss= 0.7245 val_acc= 0.8267 time= 0.0645\n",
      "Epoch: 0340 train_loss= 0.3215 train_acc= 0.9714 val_loss= 0.7228 val_acc= 0.8167 time= 0.0628\n",
      "Epoch: 0341 train_loss= 0.3218 train_acc= 0.9714 val_loss= 0.7221 val_acc= 0.8167 time= 0.0651\n",
      "Epoch: 0342 train_loss= 0.3210 train_acc= 0.9714 val_loss= 0.7215 val_acc= 0.8167 time= 0.0631\n",
      "Epoch: 0343 train_loss= 0.3194 train_acc= 0.9714 val_loss= 0.7214 val_acc= 0.8167 time= 0.0650\n",
      "Epoch: 0344 train_loss= 0.3173 train_acc= 0.9714 val_loss= 0.7224 val_acc= 0.8233 time= 0.0619\n",
      "Epoch: 0345 train_loss= 0.3161 train_acc= 0.9786 val_loss= 0.7245 val_acc= 0.8200 time= 0.0621\n",
      "Epoch: 0346 train_loss= 0.3160 train_acc= 0.9786 val_loss= 0.7281 val_acc= 0.8200 time= 0.0621\n",
      "Epoch: 0347 train_loss= 0.3163 train_acc= 0.9786 val_loss= 0.7320 val_acc= 0.8233 time= 0.0631\n",
      "Epoch: 0348 train_loss= 0.3161 train_acc= 0.9786 val_loss= 0.7332 val_acc= 0.8200 time= 0.0614\n",
      "Epoch: 0349 train_loss= 0.3156 train_acc= 0.9786 val_loss= 0.7330 val_acc= 0.8200 time= 0.0619\n",
      "Epoch: 0350 train_loss= 0.3135 train_acc= 0.9786 val_loss= 0.7274 val_acc= 0.8333 time= 0.0664\n",
      "Epoch: 0351 train_loss= 0.3117 train_acc= 0.9786 val_loss= 0.7217 val_acc= 0.8333 time= 0.0628\n",
      "Epoch: 0352 train_loss= 0.3113 train_acc= 0.9786 val_loss= 0.7169 val_acc= 0.8233 time= 0.0629\n",
      "Epoch: 0353 train_loss= 0.3119 train_acc= 0.9714 val_loss= 0.7144 val_acc= 0.8233 time= 0.0674\n",
      "Epoch: 0354 train_loss= 0.3125 train_acc= 0.9714 val_loss= 0.7131 val_acc= 0.8167 time= 0.0615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0355 train_loss= 0.3112 train_acc= 0.9714 val_loss= 0.7121 val_acc= 0.8133 time= 0.0669\n",
      "Epoch: 0356 train_loss= 0.3090 train_acc= 0.9714 val_loss= 0.7118 val_acc= 0.8133 time= 0.0616\n",
      "Epoch: 0357 train_loss= 0.3079 train_acc= 0.9786 val_loss= 0.7134 val_acc= 0.8267 time= 0.0625\n",
      "Epoch: 0358 train_loss= 0.3085 train_acc= 0.9786 val_loss= 0.7184 val_acc= 0.8267 time= 0.0655\n",
      "Epoch: 0359 train_loss= 0.3112 train_acc= 0.9857 val_loss= 0.7263 val_acc= 0.8233 time= 0.0629\n",
      "Epoch: 0360 train_loss= 0.3139 train_acc= 0.9857 val_loss= 0.7324 val_acc= 0.8233 time= 0.0647\n",
      "Epoch: 0361 train_loss= 0.3139 train_acc= 0.9857 val_loss= 0.7318 val_acc= 0.8200 time= 0.0637\n",
      "Epoch: 0362 train_loss= 0.3117 train_acc= 0.9857 val_loss= 0.7264 val_acc= 0.8200 time= 0.0638\n",
      "Epoch: 0363 train_loss= 0.3092 train_acc= 0.9857 val_loss= 0.7202 val_acc= 0.8200 time= 0.0627\n",
      "Epoch: 0364 train_loss= 0.3076 train_acc= 0.9857 val_loss= 0.7137 val_acc= 0.8200 time= 0.0615\n",
      "Epoch: 0365 train_loss= 0.3086 train_acc= 0.9786 val_loss= 0.7105 val_acc= 0.8167 time= 0.0637\n",
      "Epoch: 0366 train_loss= 0.3109 train_acc= 0.9714 val_loss= 0.7106 val_acc= 0.8067 time= 0.0613\n",
      "Epoch: 0367 train_loss= 0.3098 train_acc= 0.9714 val_loss= 0.7104 val_acc= 0.8100 time= 0.0625\n",
      "Epoch: 0368 train_loss= 0.3071 train_acc= 0.9714 val_loss= 0.7091 val_acc= 0.8100 time= 0.0615\n",
      "Epoch: 0369 train_loss= 0.3037 train_acc= 0.9857 val_loss= 0.7091 val_acc= 0.8167 time= 0.0626\n",
      "Epoch: 0370 train_loss= 0.3008 train_acc= 0.9857 val_loss= 0.7109 val_acc= 0.8267 time= 0.0619\n",
      "Epoch: 0371 train_loss= 0.2998 train_acc= 0.9857 val_loss= 0.7150 val_acc= 0.8300 time= 0.0629\n",
      "Epoch: 0372 train_loss= 0.2998 train_acc= 0.9857 val_loss= 0.7196 val_acc= 0.8267 time= 0.0615\n",
      "Epoch: 0373 train_loss= 0.3000 train_acc= 0.9929 val_loss= 0.7232 val_acc= 0.8133 time= 0.0624\n",
      "Epoch: 0374 train_loss= 0.2995 train_acc= 0.9929 val_loss= 0.7228 val_acc= 0.8167 time= 0.0612\n",
      "Epoch: 0375 train_loss= 0.2988 train_acc= 0.9929 val_loss= 0.7208 val_acc= 0.8133 time= 0.0663\n",
      "Epoch: 0376 train_loss= 0.2980 train_acc= 0.9857 val_loss= 0.7168 val_acc= 0.8133 time= 0.0614\n",
      "Epoch: 0377 train_loss= 0.2978 train_acc= 0.9857 val_loss= 0.7132 val_acc= 0.8133 time= 0.0612\n",
      "Epoch: 0378 train_loss= 0.2976 train_acc= 0.9857 val_loss= 0.7101 val_acc= 0.8133 time= 0.0637\n",
      "Epoch: 0379 train_loss= 0.2973 train_acc= 0.9857 val_loss= 0.7075 val_acc= 0.8167 time= 0.0637\n",
      "Epoch: 0380 train_loss= 0.2975 train_acc= 0.9857 val_loss= 0.7057 val_acc= 0.8067 time= 0.0664\n",
      "Epoch: 0381 train_loss= 0.2970 train_acc= 0.9857 val_loss= 0.7051 val_acc= 0.8067 time= 0.0629\n",
      "Epoch: 0382 train_loss= 0.2963 train_acc= 0.9857 val_loss= 0.7055 val_acc= 0.8100 time= 0.0661\n",
      "Epoch: 0383 train_loss= 0.2956 train_acc= 0.9857 val_loss= 0.7068 val_acc= 0.8133 time= 0.0685\n",
      "Epoch: 0384 train_loss= 0.2949 train_acc= 0.9857 val_loss= 0.7084 val_acc= 0.8200 time= 0.0625\n",
      "Epoch: 0385 train_loss= 0.2930 train_acc= 0.9857 val_loss= 0.7070 val_acc= 0.8167 time= 0.0660\n",
      "Epoch: 0386 train_loss= 0.2904 train_acc= 0.9857 val_loss= 0.7038 val_acc= 0.8167 time= 0.0655\n",
      "Epoch: 0387 train_loss= 0.2877 train_acc= 0.9857 val_loss= 0.7006 val_acc= 0.8167 time= 0.0632\n",
      "Epoch: 0388 train_loss= 0.2864 train_acc= 0.9786 val_loss= 0.6978 val_acc= 0.8133 time= 0.0626\n",
      "Epoch: 0389 train_loss= 0.2861 train_acc= 0.9786 val_loss= 0.6973 val_acc= 0.8133 time= 0.0644\n",
      "Epoch: 0390 train_loss= 0.2869 train_acc= 0.9786 val_loss= 0.6983 val_acc= 0.8133 time= 0.0626\n",
      "Epoch: 0391 train_loss= 0.2878 train_acc= 0.9786 val_loss= 0.6989 val_acc= 0.8133 time= 0.0654\n",
      "Epoch: 0392 train_loss= 0.2879 train_acc= 0.9786 val_loss= 0.6991 val_acc= 0.8100 time= 0.0615\n",
      "Epoch: 0393 train_loss= 0.2857 train_acc= 0.9786 val_loss= 0.6994 val_acc= 0.8133 time= 0.0611\n",
      "Epoch: 0394 train_loss= 0.2837 train_acc= 0.9786 val_loss= 0.7001 val_acc= 0.8233 time= 0.0653\n",
      "Epoch: 0395 train_loss= 0.2824 train_acc= 0.9857 val_loss= 0.7008 val_acc= 0.8233 time= 0.0643\n",
      "Epoch: 0396 train_loss= 0.2821 train_acc= 0.9857 val_loss= 0.7021 val_acc= 0.8267 time= 0.0634\n",
      "Epoch: 0397 train_loss= 0.2830 train_acc= 0.9857 val_loss= 0.7033 val_acc= 0.8233 time= 0.0653\n",
      "Epoch: 0398 train_loss= 0.2837 train_acc= 0.9857 val_loss= 0.7055 val_acc= 0.8267 time= 0.0612\n",
      "Epoch: 0399 train_loss= 0.2835 train_acc= 0.9857 val_loss= 0.7067 val_acc= 0.8233 time= 0.0631\n",
      "Epoch: 0400 train_loss= 0.2816 train_acc= 0.9857 val_loss= 0.7035 val_acc= 0.8300 time= 0.0615\n",
      "Epoch 400: early stopping\n"
     ]
    }
   ],
   "source": [
    "DATASET = 'cora'\n",
    "FILTER = 'localpool'  # 'chebyshev'\n",
    "MAX_DEGREE = 2  # maximum polynomial degree\n",
    "SYM_NORM = True  # symmetric (True) vs. left-only (False) normalization\n",
    "NB_EPOCH = 20000\n",
    "PATIENCE = 10  # early stopping patience\n",
    "\n",
    "# Get data\n",
    "X, A, y = load_data(\n",
    "    dataset=DATASET)  # X: 特征（2708，2708）、 A: 邻接矩阵 （2708，2708）、 y: 标签 （2708，7）\n",
    "y_train, y_val, y_test, idx_train, idx_val, idx_test, train_mask = get_splits(\n",
    "    y)\n",
    "X /= X.sum(1).reshape(-1, 1)  # Normalize  X : (2708, 1433)\n",
    "\n",
    "if FILTER == 'localpool':\n",
    "    \"\"\" Local pooling filters (see 'renormalization trick' in Kipf & Welling, arXiv 2016) \"\"\"\n",
    "    print('Using local pooling filters...')\n",
    "    A_ = preprocess_adj(A, SYM_NORM)  # 加入 self-loop 的邻接矩阵\n",
    "    support = 1\n",
    "    graph = [X, A_] # 按照自定义的 GCN 层， 输入应该是包含两部分的 列表\n",
    "    G = [Input(shape=(None, None), batch_shape=(None, None), sparse=True)]\n",
    "\n",
    "elif FILTER == 'chebyshev':\n",
    "    \"\"\" Chebyshev polynomial basis filters (Defferard et al., NIPS 2016)  \"\"\"\n",
    "    print('Using Chebyshev polynomial basis filters...')\n",
    "    L = normalized_laplacian(A, SYM_NORM)  # 对拉普拉斯矩阵进行归一化处理\n",
    "    L_scaled = rescale_laplacian(L)  # L~ = 2 / Lambda * L - I\n",
    "    T_k = chebyshev_polynomial(L_scaled, MAX_DEGREE)  # 2介切比雪夫多项式\n",
    "    support = MAX_DEGREE + 1\n",
    "    graph = [X] + T_k  # 列表相加\n",
    "    G = [\n",
    "        Input(shape=(None, None), batch_shape=(None, None), sparse=True)\n",
    "        for _ in range(support)\n",
    "    ]\n",
    "\n",
    "else:\n",
    "    raise Exception('Invalid filter type.')\n",
    "\n",
    "### 函数式建模， 把 层当作 函数 使用\n",
    "X_in = Input(shape=(X.shape[1], ))  # 输入维度，不包括 batch_size， 即 输入的特征维度\n",
    "\n",
    "# Define model architecture\n",
    "# NOTE: We pass arguments for graph convolutional layers as a list of tensors.\n",
    "# This is somewhat hacky, more elegant options would require rewriting the Layer base class.\n",
    "H = Dropout(0.5)(X_in)\n",
    "H = GraphConvolution(16,\n",
    "                     support,\n",
    "                     activation='relu',\n",
    "                     kernel_regularizer=l2(5e-4))([H] + G)\n",
    "H = Dropout(0.5)(H)\n",
    "Y = GraphConvolution(y.shape[1], support, activation='softmax')([H] + G)\n",
    "\n",
    "# Compile model\n",
    "model = Model(inputs=[X_in] + G, outputs=Y)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01))\n",
    "\n",
    "# Helper variables for main training loop\n",
    "wait = 0\n",
    "preds = None\n",
    "best_val_loss = 99999\n",
    "\n",
    "# Fit\n",
    "for epoch in range(1, NB_EPOCH + 1):\n",
    "\n",
    "    # Log wall-clock time\n",
    "    t = time.time()\n",
    "\n",
    "    # Single training iteration (we mask nodes without labels for loss calculation)\n",
    "    model.fit(\n",
    "        graph,\n",
    "        y_train,\n",
    "        sample_weight=\n",
    "        train_mask,  # 向sample_weight 用于对损失函数进行加权 , 因此只有训练数据对损失函数有贡献\n",
    "        batch_size=A.shape[0],  # 每次的 batch_size  ,\n",
    "        epochs=1,\n",
    "        shuffle=False,\n",
    "        verbose=0)\n",
    "\n",
    "    # Predict on full dataset\n",
    "    preds = model.predict(graph, batch_size=A.shape[0])\n",
    "\n",
    "    # Train / validation scores\n",
    "    train_val_loss, train_val_acc = evaluate_preds(preds, [y_train, y_val],\n",
    "                                                   [idx_train, idx_val])\n",
    "    print(\"Epoch: {:04d}\".format(epoch),\n",
    "          \"train_loss= {:.4f}\".format(train_val_loss[0]),\n",
    "          \"train_acc= {:.4f}\".format(train_val_acc[0]),\n",
    "          \"val_loss= {:.4f}\".format(train_val_loss[1]),\n",
    "          \"val_acc= {:.4f}\".format(train_val_acc[1]),\n",
    "          \"time= {:.4f}\".format(time.time() - t))\n",
    "\n",
    "    # Early stopping\n",
    "    if train_val_loss[1] < best_val_loss:\n",
    "        best_val_loss = train_val_loss[1]\n",
    "        wait = 0\n",
    "    else:\n",
    "        if wait >= PATIENCE:\n",
    "            print('Epoch {}: early stopping'.format(epoch))\n",
    "            break\n",
    "        wait += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T02:47:49.748098Z",
     "start_time": "2020-04-27T02:47:49.740268Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.7351 accuracy= 0.8220\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate_preds(preds, [y_test], [idx_test])\n",
    "print(\"Test set results:\", \"loss= {:.4f}\".format(test_loss[0]),\n",
    "      \"accuracy= {:.4f}\".format(test_acc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T14:11:59.715775Z",
     "start_time": "2020-04-26T14:11:59.692402Z"
    },
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.2420109e-01,  1.7771399e-11, -9.4312154e-02, ...,\n",
       "         8.3028547e-02,  1.3846582e-01, -1.6981850e-11],\n",
       "       [ 6.8800889e-02, -4.1161324e-11, -6.7035429e-02, ...,\n",
       "         2.0541064e-02, -1.1935651e-01,  7.6480794e-11],\n",
       "       [ 8.9236625e-02, -1.4951953e-11,  1.9362820e-02, ...,\n",
       "         5.1603135e-02,  4.7853532e-01, -7.1407248e-11],\n",
       "       ...,\n",
       "       [-6.3166186e-02,  5.2331157e-11,  1.9311365e-01, ...,\n",
       "        -3.1927913e-02,  1.4044446e-01, -4.1707922e-12],\n",
       "       [-1.1241451e-01,  2.2368515e-11,  1.7177092e-01, ...,\n",
       "        -3.6130950e-02,  3.7736788e-01,  5.7867818e-11],\n",
       "       [ 1.8448421e-03, -1.4665644e-10,  5.2221458e-02, ...,\n",
       "        -3.6790878e-02, -3.4460209e-02, -2.3777178e-12]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary() # 模型结构\n",
    "model.layers[1].name \n",
    "weights, biases = model.layers[3].get_weights() # 模型权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "model.save(\"my_keras_model.h5\") # 保存模型\n",
    "model = keras.models.load_model(\"my_keras_model.h5\")\n",
    "\n",
    "save_weights() # 保存权重\n",
    "load_weights()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 464,
   "position": {
    "height": "40px",
    "left": "922px",
    "right": "20px",
    "top": "11px",
    "width": "502px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
