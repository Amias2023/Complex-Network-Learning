{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 基于  kipf_gcn_keras: https://github.com/tkipf/keras-gcn  + William_GraphSAGE_tf : https://github.com/williamleif/GraphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T14:57:21.821703Z",
     "start_time": "2020-05-29T14:57:20.046417Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import os\n",
    "import pickle as pkl\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.initializers import glorot_uniform, Zeros\n",
    "from tensorflow.python.keras.layers import Input, Dense, Dropout, Layer, LSTM\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.regularizers import l2\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.layers import Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T16:05:18.608786Z",
     "start_time": "2020-05-29T16:05:18.555752Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 所需函数\n",
    "def get_splits(y, ):\n",
    "    \n",
    "    idx_list = np.arange(len(y))\n",
    "    \n",
    "    # 训练集中每个类别均分布 各 20个。\n",
    "    idx_train = []\n",
    "    label_count = {}\n",
    "    for i, label in enumerate(y):\n",
    "        label = np.argmax(label)\n",
    "        if label_count.get(label, 0) < 20:\n",
    "            idx_train.append(i)\n",
    "            label_count[label] = label_count.get(label, 0) + 1\n",
    "    \n",
    "    #  train： 140；  val： 500；  test：1000\n",
    "    idx_val_test = list(set(idx_list) - set(idx_train))\n",
    "    idx_val = idx_val_test[0:500]\n",
    "    idx_test = idx_val_test[500:1500]\n",
    "    \n",
    "    y_train = np.zeros(y.shape, dtype=np.int32)\n",
    "    y_val = np.zeros(y.shape, dtype=np.int32)\n",
    "    y_test = np.zeros(y.shape, dtype=np.int32)\n",
    "    y_train[idx_train] = y[idx_train]\n",
    "    y_val[idx_val] = y[idx_val]\n",
    "    y_test[idx_test] = y[idx_test]\n",
    "    train_mask = sample_mask(idx_train, y.shape[0])\n",
    "    val_mask = sample_mask(idx_val, y.shape[0])\n",
    "    test_mask = sample_mask(idx_test, y.shape[0])\n",
    "\n",
    "    return y_train, y_val, y_test, train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "def get_splits_kipf(y, ):\n",
    "    \n",
    "    \n",
    "    idx_train = range(140) \n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "    \n",
    "    y_train = np.zeros(y.shape, dtype=np.int32)\n",
    "    y_val = np.zeros(y.shape, dtype=np.int32)\n",
    "    y_test = np.zeros(y.shape, dtype=np.int32)\n",
    "    y_train[idx_train] = y[idx_train]\n",
    "    y_val[idx_val] = y[idx_val]\n",
    "    y_test[idx_test] = y[idx_test]\n",
    "    train_mask = sample_mask(idx_train, y.shape[0])\n",
    "    val_mask = sample_mask(idx_val, y.shape[0])\n",
    "    test_mask = sample_mask(idx_test, y.shape[0])\n",
    "\n",
    "    return y_train, y_val, y_test, train_mask, val_mask, test_mask\n",
    "\n",
    "def get_splits_0(y):\n",
    "    \"\"\"\n",
    "    数据集划分：\n",
    "    \n",
    "    y : 标签数据 即 load_data() 函数 返回的 labels 数据 \n",
    "    数据格式  (2708 , 7 ) 2708 个数据，一共 7 个 class ， one-hot 维度 7 维 \n",
    "    \"\"\"\n",
    "\n",
    "    idx_train = range(140)  # ??? 少一部分数据啊\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    y_train = np.zeros(y.shape, dtype=np.int32) # 初始化 (2708,7) size 的全 0 矩阵。\n",
    "    y_val = np.zeros(y.shape, dtype=np.int32)\n",
    "    y_test = np.zeros(y.shape, dtype=np.int32)\n",
    "    y_train[idx_train] = y[idx_train] #  取 y 矩阵前 140 行， 其余行赋值为 0 \n",
    "    y_val[idx_val] = y[idx_val]\n",
    "    y_test[idx_test] = y[idx_test]\n",
    "\n",
    "    # 训练数据的样本掩码\n",
    "    train_mask = sample_mask(idx_train, y.shape[0]) # 前 140行为 true， 后面为 false 的向量\n",
    "    return y_train, y_val, y_test, idx_train, idx_val, idx_test, train_mask\n",
    "\n",
    "\n",
    "def load_data_0(path=\"data/cora/\", dataset=\"cora\"):\n",
    "    \"\"\"\n",
    "    Load citation network dataset \n",
    "    \n",
    "    (cora only for now)\n",
    "    \"\"\"\n",
    "\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    ### 读取样本id，特征和标签\n",
    "    idx_features_labels = np.genfromtxt(\n",
    "        \"{}{}.content\".format(path, dataset),\n",
    "        dtype=np.dtype(str))  # np.genfromtxt()生成 array\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1],\n",
    "                             dtype=np.float32)  # 提取样本的特征，并将其转换为csr矩阵\n",
    "    labels = encode_onehot(\n",
    "        idx_features_labels[:, -1])  # 提取样本的标签，并将其转换为one-hot编码形式\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)  # 样本的id数组\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}  # 创建一个字典储存数据id\n",
    "\n",
    "    ### 读取样本之间关系 ： 连边\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(\n",
    "                         edges_unordered.shape)  # 无序边  map 成为有序\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)  # 构建图的邻接矩阵\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(\n",
    "        adj.T > adj)  # 将非对称邻接矩阵转变为对称邻接矩阵\n",
    "\n",
    "    print('Dataset has {} nodes, {} edges, {} features.'.format(\n",
    "        adj.shape[0], edges.shape[0], features.shape[1]))\n",
    "\n",
    "    return features.todense(), adj, labels  # 返回特征的密集矩阵表示、邻接矩阵和标签的one-hot编码\n",
    "\n",
    "\n",
    "\n",
    "def load_data_v1(dataset=\"cora\",path=\"data/cora/\"):\n",
    "    \n",
    "    \n",
    "    # 提取数据\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                        dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    onehot_labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(onehot_labels.shape[0], onehot_labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    # adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "    adj = convert_symmetric(adj, )\n",
    "\n",
    "    print('Dataset has {} nodes, {} edges, {} features.'.format(\n",
    "        adj.shape[0], edges.shape[0], features.shape[1]))\n",
    "\n",
    "    y_train, y_val, y_test, train_mask, val_mask, test_mask = get_splits_kipf(\n",
    "        onehot_labels)  #  分割标签\n",
    "\n",
    "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "def parse_index_file(filename):\n",
    "    \"\"\"Parse index file.\"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "\n",
    "def load_data(dataset_str):\n",
    "    \"\"\"Load data.\"\"\"\n",
    "    FILE_PATH = os.path.abspath(__file__)\n",
    "    DIR_PATH = os.path.dirname(FILE_PATH)\n",
    "    DATA_PATH = os.path.join(DIR_PATH, 'data/')\n",
    "    DATA_PATH = \"../data/cora/\"\n",
    "\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(\"{}ind.{}.{}\".format(DATA_PATH, dataset_str, names[i]),\n",
    "                  'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "\n",
    "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "    test_idx_reorder = parse_index_file(\"{}ind.{}.test.index\".format(\n",
    "        DATA_PATH, dataset_str))\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "\n",
    "    if dataset_str == 'citeseer':\n",
    "        # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
    "        # Find isolated nodes, add them as zero-vecs into the right position\n",
    "        test_idx_range_full = range(min(test_idx_reorder),\n",
    "                                    max(test_idx_reorder) + 1)\n",
    "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
    "        tx_extended[test_idx_range - min(test_idx_range), :] = tx\n",
    "        tx = tx_extended\n",
    "        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
    "        ty_extended[test_idx_range - min(test_idx_range), :] = ty\n",
    "        ty = ty_extended\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil()\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "\n",
    "    labels = np.vstack((ally, ty))\n",
    "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
    "\n",
    "    idx_test = test_idx_range.tolist()\n",
    "    idx_train = range(len(y))\n",
    "    idx_val = range(len(y), len(y) + 500)\n",
    "\n",
    "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
    "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "\n",
    "    y_train = np.zeros(labels.shape)\n",
    "    y_val = np.zeros(labels.shape)\n",
    "    y_test = np.zeros(labels.shape)\n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_val[val_mask, :] = labels[val_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "\n",
    "    return sp.csr_matrix(\n",
    "        adj), features, y_train, y_val, y_test, train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "\n",
    "def convert_symmetric(X, sparse=True):\n",
    "    if sparse:\n",
    "        X += X.T - sp.diags(X.diagonal())\n",
    "    else:\n",
    "        X += X.T - np.diag(X.diagonal())\n",
    "    return X\n",
    "\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {\n",
    "        c: np.identity(len(classes))[i, :]\n",
    "        for i, c in enumerate(classes)\n",
    "    }\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "\n",
    "def normalize_adj(adj, symmetric=True):\n",
    "    if symmetric:\n",
    "        d = sp.diags(np.power(np.array(adj.sum(1)), -0.5).flatten(), 0)\n",
    "        a_norm = adj.dot(d).transpose().dot(d).tocsr()\n",
    "    else:\n",
    "        d = sp.diags(np.power(np.array(adj.sum(1)), -1).flatten(), 0)\n",
    "        a_norm = d.dot(adj).tocsr()\n",
    "    return a_norm\n",
    "\n",
    "\n",
    "def preprocess_adj(adj, symmetric=True):\n",
    "    adj = adj + sp.eye(adj.shape[0])\n",
    "    adj = normalize_adj(adj, symmetric)\n",
    "    return adj\n",
    "\n",
    "\n",
    "def plot_embeddings(embeddings, X, Y):\n",
    "\n",
    "    emb_list = []\n",
    "    for k in X:\n",
    "        emb_list.append(embeddings[k])\n",
    "    emb_list = np.array(emb_list)\n",
    "\n",
    "    model = TSNE(n_components=2)\n",
    "    node_pos = model.fit_transform(emb_list)\n",
    "\n",
    "    color_idx = {}\n",
    "    for i in range(len(X)):\n",
    "        color_idx.setdefault(Y[i][:], [])\n",
    "        color_idx[Y[i][:]].append(i)\n",
    "\n",
    "    for c, idx in color_idx.items():\n",
    "        plt.scatter(node_pos[idx, 0], node_pos[idx, 1], label=c)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "    rowsum = np.array(features.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    features = r_mat_inv.dot(features)\n",
    "    return features.todense()\n",
    "\n",
    "def categorical_crossentropy(preds, labels):\n",
    "    \"\"\"\n",
    "    损失函数计算： 所有有标签的数据，主要用于训练时进行梯度下降 ： \n",
    "    \n",
    "    L=Y*ln(Z)\n",
    "    \n",
    "    param preds: 模型对样本的输出数组\n",
    "    param labels: 样本的one-hot标签数组\n",
    "    return: 样本的平均交叉熵损失\n",
    "    \"\"\"\n",
    "    return np.mean(-np.log(np.extract(\n",
    "        labels, preds)))  # np.extract(condition, x)函数，根据某个条件从数组中抽取元素\n",
    "\n",
    "\n",
    "def accuracy(preds, labels):\n",
    "    \"\"\"\n",
    "    计算准确率\n",
    "    \"\"\"\n",
    "    # np.argmax(x,1) 每一行元素最大值索引  : 相当于将 one-hot 转成一个数值 [0,1,0,0,0] = 1\n",
    "    # np.equal(x1, x2) 比较 array 对应元素是否相等 返回 np.array([ True,  True, False])\n",
    "    # np.mean(np.array([ True,  True, False])) = 0.666\n",
    "    return np.mean(np.equal(np.argmax(labels, 1), np.argmax(preds, 1)))\n",
    "\n",
    "\n",
    "def evaluate_preds(preds, labels, indices):\n",
    "    \"\"\"\n",
    "     评估样本划分的损失函数和准确率\n",
    "     \n",
    "    :param preds:对于样本的预测值\n",
    "    :param labels:样本的标签one-hot向量\n",
    "    :param indices:样本的索引集合\n",
    "    :return:交叉熵损失函数列表、准确率列表\n",
    "    \"\"\"\n",
    "    split_loss = list()\n",
    "    split_acc = list()\n",
    "\n",
    "    for y_split, idx_split in zip(labels, indices):\n",
    "        # 计算每一个样本划分的交叉熵损失函数\n",
    "        split_loss.append(\n",
    "            categorical_crossentropy(preds[idx_split], y_split[idx_split]))\n",
    "        # 计算每一个样本划分的准确率\n",
    "        split_acc.append(accuracy(preds[idx_split], y_split[idx_split]))\n",
    "\n",
    "    return split_loss, split_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T15:53:05.781252Z",
     "start_time": "2020-05-29T15:53:05.750773Z"
    }
   },
   "outputs": [],
   "source": [
    "# GraphSAGE 模型\n",
    "class MeanAggregator(Layer):\n",
    "\n",
    "    def __init__(self, units, input_dim, neigh_max, concat=True, dropout_rate=0.0, activation=tf.nn.relu, l2_reg=0,\n",
    "                 use_bias=False,\n",
    "                 seed=1024, **kwargs):\n",
    "        super(MeanAggregator, self).__init__()\n",
    "        self.units = units\n",
    "        self.neigh_max = neigh_max\n",
    "        self.concat = concat\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.l2_reg = l2_reg\n",
    "        self.use_bias = use_bias\n",
    "        self.activation = activation\n",
    "        self.seed = seed\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "    def build(self, input_shapes):\n",
    "\n",
    "        self.neigh_weights = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                             initializer=glorot_uniform(\n",
    "                                                 seed=self.seed),\n",
    "                                             regularizer=l2(self.l2_reg),\n",
    "                                             name=\"neigh_weights\")\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.units), initializer=Zeros(),\n",
    "                                        name='bias_weight')\n",
    "\n",
    "        self.dropout = Dropout(self.dropout_rate)\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        features, node, neighbours = inputs\n",
    "\n",
    "        node_feat = tf.nn.embedding_lookup(features, node)\n",
    "        neigh_feat = tf.nn.embedding_lookup(features, neighbours)\n",
    "\n",
    "        node_feat = self.dropout(node_feat, training=training)\n",
    "        neigh_feat = self.dropout(neigh_feat, training=training)\n",
    "\n",
    "        concat_feat = tf.concat([neigh_feat, node_feat], axis=1)\n",
    "        concat_mean = tf.reduce_mean(concat_feat, axis=1, keep_dims=False)\n",
    "\n",
    "        output = tf.matmul(concat_mean, self.neigh_weights)\n",
    "        if self.use_bias:\n",
    "            output += self.bias\n",
    "        if self.activation:\n",
    "            output = self.activation(output)\n",
    "\n",
    "        # output = tf.nn.l2_normalize(output,dim=-1)\n",
    "        output._uses_learning_phase = True\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'units': self.units,\n",
    "                  'concat': self.concat,\n",
    "                  'seed': self.seed\n",
    "                  }\n",
    "\n",
    "        base_config = super(MeanAggregator, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class PoolingAggregator(Layer):\n",
    "\n",
    "    def __init__(self, units, input_dim, neigh_max, aggregator='meanpooling', concat=True,\n",
    "                 dropout_rate=0.0,\n",
    "                 activation=tf.nn.relu, l2_reg=0, use_bias=False,\n",
    "                 seed=1024, ):\n",
    "        super(PoolingAggregator, self).__init__()\n",
    "        self.output_dim = units\n",
    "        self.input_dim = input_dim\n",
    "        self.concat = concat\n",
    "        self.pooling = aggregator\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.l2_reg = l2_reg\n",
    "        self.use_bias = use_bias\n",
    "        self.activation = activation\n",
    "        self.neigh_max = neigh_max\n",
    "        self.seed = seed\n",
    "\n",
    "        # if neigh_input_dim is None:\n",
    "\n",
    "    def build(self, input_shapes):\n",
    "\n",
    "        self.dense_layers = [Dense(\n",
    "            self.input_dim, activation='relu', use_bias=True, kernel_regularizer=l2(self.l2_reg))]\n",
    "\n",
    "        self.neigh_weights = self.add_weight(\n",
    "            shape=(self.input_dim * 2, self.output_dim),\n",
    "            initializer=glorot_uniform(\n",
    "                seed=self.seed),\n",
    "            regularizer=l2(self.l2_reg),\n",
    "\n",
    "            name=\"neigh_weights\")\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.output_dim,),\n",
    "                                        initializer=Zeros(),\n",
    "                                        name='bias_weight')\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "\n",
    "        features, node, neighbours = inputs\n",
    "\n",
    "        node_feat = tf.nn.embedding_lookup(features, node)\n",
    "        neigh_feat = tf.nn.embedding_lookup(features, neighbours)\n",
    "\n",
    "        dims = tf.shape(neigh_feat)\n",
    "        batch_size = dims[0]\n",
    "        num_neighbors = dims[1]\n",
    "        h_reshaped = tf.reshape(\n",
    "            neigh_feat, (batch_size * num_neighbors, self.input_dim))\n",
    "\n",
    "        for l in self.dense_layers:\n",
    "            h_reshaped = l(h_reshaped)\n",
    "        neigh_feat = tf.reshape(\n",
    "            h_reshaped, (batch_size, num_neighbors, int(h_reshaped.shape[-1])))\n",
    "\n",
    "        if self.pooling == \"meanpooling\":\n",
    "            neigh_feat = tf.reduce_mean(neigh_feat, axis=1, keep_dims=False)\n",
    "        else:\n",
    "            neigh_feat = tf.reduce_max(neigh_feat, axis=1)\n",
    "\n",
    "        output = tf.concat(\n",
    "            [tf.squeeze(node_feat, axis=1), neigh_feat], axis=-1)\n",
    "\n",
    "        output = tf.matmul(output, self.neigh_weights)\n",
    "        if self.use_bias:\n",
    "            output += self.bias\n",
    "        if self.activation:\n",
    "            output = self.activation(output)\n",
    "\n",
    "        # output = tf.nn.l2_normalize(output, dim=-1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'output_dim': self.output_dim,\n",
    "                  'concat': self.concat\n",
    "                  }\n",
    "\n",
    "        base_config = super(PoolingAggregator, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "def GraphSAGE(feature_dim, neighbor_num, n_hidden, n_classes, use_bias=True, activation=tf.nn.relu,\n",
    "              aggregator_type='mean', dropout_rate=0.0, l2_reg=0):\n",
    "    \n",
    "    \"\"\"\n",
    "    feature_dim : 输入特征维度\n",
    "    neighbor_num : \n",
    "    n_hidden : \n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    features = Input(shape=(feature_dim,))\n",
    "    node_input = Input(shape=(1,), dtype=tf.int32)\n",
    "    neighbor_input = [Input(shape=(l,), dtype=tf.int32) for l in neighbor_num]\n",
    "\n",
    "    if aggregator_type == 'mean':\n",
    "        aggregator = MeanAggregator\n",
    "    else:\n",
    "        aggregator = PoolingAggregator\n",
    "\n",
    "    h = features\n",
    "    for i in range(0, len(neighbor_num)):\n",
    "        if i > 0:\n",
    "            feature_dim = n_hidden\n",
    "        if i == len(neighbor_num) - 1:\n",
    "            activation = tf.nn.softmax\n",
    "            n_hidden = n_classes\n",
    "        h = aggregator(units=n_hidden, input_dim=feature_dim, activation=activation, l2_reg=l2_reg, use_bias=use_bias,\n",
    "                       dropout_rate=dropout_rate, neigh_max=neighbor_num[i], aggregator=aggregator_type)(\n",
    "            [h, node_input, neighbor_input[i]])  #\n",
    "\n",
    "    output = h\n",
    "    input_list = [features, node_input] + neighbor_input\n",
    "    model = Model(input_list, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T15:53:08.034910Z",
     "start_time": "2020-05-29T15:53:08.027416Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample_neighs(G, nodes, sample_num=None, self_loop=False, shuffle=True):  # 抽样邻居节点\n",
    "    _sample = np.random.choice\n",
    "    neighs = [list(G[int(node)]) for node in nodes]  # nodes里每个节点的邻居\n",
    "    if sample_num:\n",
    "        if self_loop:\n",
    "            sample_num -= 1\n",
    "\n",
    "        samp_neighs = [\n",
    "            list(_sample(neigh, sample_num, replace=False)) if len(neigh) >= sample_num else list(\n",
    "                _sample(neigh, sample_num, replace=True)) for neigh in neighs]  # 采样邻居 : 如果大于采样数，就无放回采样\n",
    "        if self_loop:\n",
    "            samp_neighs = [\n",
    "                samp_neigh + list([nodes[i]]) for i, samp_neigh in enumerate(samp_neighs)]  # gcn邻居要加上自己\n",
    "\n",
    "        if shuffle:\n",
    "            samp_neighs = [list(np.random.permutation(x)) for x in samp_neighs] #  对采样得到的邻居随机排序\n",
    "    else:\n",
    "        samp_neighs = neighs\n",
    "    return np.asarray(samp_neighs), np.asarray(list(map(len, samp_neighs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T16:07:14.560078Z",
     "start_time": "2020-05-29T16:07:09.588057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n",
      "Dataset has 2708 nodes, 5429 edges, 1433 features.\n"
     ]
    }
   ],
   "source": [
    "DATASET = 'cora'\n",
    "features, A, y = load_data_0(\n",
    "    dataset=DATASET)  # X: 特征（2708，2708）、 A: 邻接矩阵 （2708，2708）、 y: 标签 （2708，7）\n",
    "y_train, y_val, y_test, idx_train, idx_val, idx_test, train_mask = get_splits_0(\n",
    "    y)\n",
    "\n",
    "features /= features.sum(axis=1, ).reshape(-1, 1)\n",
    "G = nx.from_scipy_sparse_matrix(A, create_using=nx.DiGraph())\n",
    "\n",
    "A = preprocess_adj(A)\n",
    "\n",
    "indexs = np.arange(A.shape[0])\n",
    "neigh_number = [10, 25]\n",
    "neigh_maxlen = []\n",
    "\n",
    "model_input = [features,\n",
    "               np.asarray(indexs, dtype=np.int32)]  # np.asarray 将列表转换为一个 array\n",
    "\n",
    "for num in neigh_number:\n",
    "    sample_neigh, sample_neigh_len = sample_neighs(G,\n",
    "                                                   indexs,\n",
    "                                                   num,\n",
    "                                                   self_loop=False)\n",
    "    model_input.extend([sample_neigh])\n",
    "    neigh_maxlen.append(max(sample_neigh_len))\n",
    "    \n",
    "\n",
    "model = GraphSAGE(feature_dim=features.shape[1],\n",
    "                  neighbor_num=neigh_maxlen,\n",
    "                  n_hidden=16,\n",
    "                  n_classes=y_train.shape[1],\n",
    "                  use_bias=True,\n",
    "                  activation=tf.nn.relu,\n",
    "                  aggregator_type='mean',\n",
    "                  dropout_rate=0.5,\n",
    "                  l2_reg=2.5e-4)\n",
    "model.compile(\n",
    "    Adam(0.01),\n",
    "    'categorical_crossentropy',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T16:11:37.411189Z",
     "start_time": "2020-05-29T16:09:41.415738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 1.9041 train_acc= 0.2214 val_loss= 1.9110 val_acc= 0.1467\n",
      "Epoch: 0002 train_loss= 1.8921 train_acc= 0.3929 val_loss= 1.9010 val_acc= 0.3000\n",
      "Epoch: 0003 train_loss= 1.8801 train_acc= 0.5286 val_loss= 1.8907 val_acc= 0.4867\n",
      "Epoch: 0004 train_loss= 1.8676 train_acc= 0.4786 val_loss= 1.8799 val_acc= 0.4333\n",
      "Epoch: 0005 train_loss= 1.8547 train_acc= 0.4214 val_loss= 1.8688 val_acc= 0.3700\n",
      "Epoch: 0006 train_loss= 1.8416 train_acc= 0.4071 val_loss= 1.8578 val_acc= 0.3700\n",
      "Epoch: 0007 train_loss= 1.8290 train_acc= 0.4000 val_loss= 1.8472 val_acc= 0.3633\n",
      "Epoch: 0008 train_loss= 1.8162 train_acc= 0.3929 val_loss= 1.8365 val_acc= 0.3633\n",
      "Epoch: 0009 train_loss= 1.8034 train_acc= 0.3929 val_loss= 1.8256 val_acc= 0.3633\n",
      "Epoch: 0010 train_loss= 1.7907 train_acc= 0.3929 val_loss= 1.8146 val_acc= 0.3633\n",
      "Epoch: 0011 train_loss= 1.7782 train_acc= 0.3929 val_loss= 1.8038 val_acc= 0.3633\n",
      "Epoch: 0012 train_loss= 1.7662 train_acc= 0.3929 val_loss= 1.7933 val_acc= 0.3633\n",
      "Epoch: 0013 train_loss= 1.7544 train_acc= 0.3857 val_loss= 1.7829 val_acc= 0.3633\n",
      "Epoch: 0014 train_loss= 1.7429 train_acc= 0.3857 val_loss= 1.7729 val_acc= 0.3633\n",
      "Epoch: 0015 train_loss= 1.7317 train_acc= 0.3857 val_loss= 1.7634 val_acc= 0.3633\n",
      "Epoch: 0016 train_loss= 1.7210 train_acc= 0.3857 val_loss= 1.7545 val_acc= 0.3633\n",
      "Epoch: 0017 train_loss= 1.7105 train_acc= 0.3857 val_loss= 1.7459 val_acc= 0.3633\n",
      "Epoch: 0018 train_loss= 1.7005 train_acc= 0.3929 val_loss= 1.7379 val_acc= 0.3633\n",
      "Epoch: 0019 train_loss= 1.6906 train_acc= 0.3929 val_loss= 1.7302 val_acc= 0.3633\n",
      "Epoch: 0020 train_loss= 1.6809 train_acc= 0.4000 val_loss= 1.7227 val_acc= 0.3667\n",
      "Epoch: 0021 train_loss= 1.6711 train_acc= 0.4000 val_loss= 1.7153 val_acc= 0.3667\n",
      "Epoch: 0022 train_loss= 1.6612 train_acc= 0.4071 val_loss= 1.7078 val_acc= 0.3667\n",
      "Epoch: 0023 train_loss= 1.6512 train_acc= 0.4071 val_loss= 1.7005 val_acc= 0.3667\n",
      "Epoch: 0024 train_loss= 1.6409 train_acc= 0.4071 val_loss= 1.6930 val_acc= 0.3700\n",
      "Epoch: 0025 train_loss= 1.6302 train_acc= 0.4214 val_loss= 1.6855 val_acc= 0.3733\n",
      "Epoch: 0026 train_loss= 1.6191 train_acc= 0.4214 val_loss= 1.6779 val_acc= 0.3767\n",
      "Epoch: 0027 train_loss= 1.6077 train_acc= 0.4214 val_loss= 1.6700 val_acc= 0.3800\n",
      "Epoch: 0028 train_loss= 1.5960 train_acc= 0.4214 val_loss= 1.6620 val_acc= 0.3833\n",
      "Epoch: 0029 train_loss= 1.5839 train_acc= 0.4214 val_loss= 1.6538 val_acc= 0.3867\n",
      "Epoch: 0030 train_loss= 1.5714 train_acc= 0.4286 val_loss= 1.6454 val_acc= 0.3933\n",
      "Epoch: 0031 train_loss= 1.5587 train_acc= 0.4286 val_loss= 1.6367 val_acc= 0.3933\n",
      "Epoch: 0032 train_loss= 1.5458 train_acc= 0.4357 val_loss= 1.6280 val_acc= 0.3933\n",
      "Epoch: 0033 train_loss= 1.5325 train_acc= 0.4500 val_loss= 1.6189 val_acc= 0.4000\n",
      "Epoch: 0034 train_loss= 1.5190 train_acc= 0.4571 val_loss= 1.6096 val_acc= 0.4067\n",
      "Epoch: 0035 train_loss= 1.5053 train_acc= 0.4571 val_loss= 1.6002 val_acc= 0.4133\n",
      "Epoch: 0036 train_loss= 1.4914 train_acc= 0.4643 val_loss= 1.5907 val_acc= 0.4133\n",
      "Epoch: 0037 train_loss= 1.4774 train_acc= 0.4714 val_loss= 1.5811 val_acc= 0.4133\n",
      "Epoch: 0038 train_loss= 1.4633 train_acc= 0.4786 val_loss= 1.5712 val_acc= 0.4300\n",
      "Epoch: 0039 train_loss= 1.4490 train_acc= 0.4786 val_loss= 1.5608 val_acc= 0.4300\n",
      "Epoch: 0040 train_loss= 1.4346 train_acc= 0.4786 val_loss= 1.5502 val_acc= 0.4333\n",
      "Epoch: 0041 train_loss= 1.4201 train_acc= 0.4786 val_loss= 1.5393 val_acc= 0.4333\n",
      "Epoch: 0042 train_loss= 1.4056 train_acc= 0.4929 val_loss= 1.5283 val_acc= 0.4367\n",
      "Epoch: 0043 train_loss= 1.3910 train_acc= 0.5143 val_loss= 1.5172 val_acc= 0.4400\n",
      "Epoch: 0044 train_loss= 1.3763 train_acc= 0.5143 val_loss= 1.5060 val_acc= 0.4500\n",
      "Epoch: 0045 train_loss= 1.3615 train_acc= 0.5286 val_loss= 1.4949 val_acc= 0.4633\n",
      "Epoch: 0046 train_loss= 1.3467 train_acc= 0.5357 val_loss= 1.4840 val_acc= 0.4800\n",
      "Epoch: 0047 train_loss= 1.3319 train_acc= 0.5643 val_loss= 1.4732 val_acc= 0.4933\n",
      "Epoch: 0048 train_loss= 1.3170 train_acc= 0.6000 val_loss= 1.4625 val_acc= 0.5067\n",
      "Epoch: 0049 train_loss= 1.3021 train_acc= 0.6214 val_loss= 1.4518 val_acc= 0.5233\n",
      "Epoch: 0050 train_loss= 1.2872 train_acc= 0.6357 val_loss= 1.4411 val_acc= 0.5367\n",
      "Epoch: 0051 train_loss= 1.2724 train_acc= 0.6429 val_loss= 1.4302 val_acc= 0.5700\n",
      "Epoch: 0052 train_loss= 1.2578 train_acc= 0.6571 val_loss= 1.4193 val_acc= 0.5833\n",
      "Epoch: 0053 train_loss= 1.2431 train_acc= 0.6571 val_loss= 1.4083 val_acc= 0.6033\n",
      "Epoch: 0054 train_loss= 1.2286 train_acc= 0.6929 val_loss= 1.3972 val_acc= 0.6167\n",
      "Epoch: 0055 train_loss= 1.2141 train_acc= 0.7000 val_loss= 1.3860 val_acc= 0.6300\n",
      "Epoch: 0056 train_loss= 1.2000 train_acc= 0.7000 val_loss= 1.3747 val_acc= 0.6367\n",
      "Epoch: 0057 train_loss= 1.1859 train_acc= 0.7214 val_loss= 1.3638 val_acc= 0.6400\n",
      "Epoch: 0058 train_loss= 1.1721 train_acc= 0.7500 val_loss= 1.3529 val_acc= 0.6467\n",
      "Epoch: 0059 train_loss= 1.1583 train_acc= 0.7571 val_loss= 1.3424 val_acc= 0.6500\n",
      "Epoch: 0060 train_loss= 1.1446 train_acc= 0.7643 val_loss= 1.3321 val_acc= 0.6567\n",
      "Epoch: 0061 train_loss= 1.1311 train_acc= 0.7643 val_loss= 1.3217 val_acc= 0.6700\n",
      "Epoch: 0062 train_loss= 1.1178 train_acc= 0.7643 val_loss= 1.3118 val_acc= 0.6800\n",
      "Epoch: 0063 train_loss= 1.1047 train_acc= 0.7643 val_loss= 1.3020 val_acc= 0.6900\n",
      "Epoch: 0064 train_loss= 1.0916 train_acc= 0.7643 val_loss= 1.2918 val_acc= 0.6933\n",
      "Epoch: 0065 train_loss= 1.0789 train_acc= 0.7786 val_loss= 1.2817 val_acc= 0.7000\n",
      "Epoch: 0066 train_loss= 1.0663 train_acc= 0.7786 val_loss= 1.2715 val_acc= 0.7067\n",
      "Epoch: 0067 train_loss= 1.0539 train_acc= 0.7786 val_loss= 1.2616 val_acc= 0.7067\n",
      "Epoch: 0068 train_loss= 1.0419 train_acc= 0.7857 val_loss= 1.2519 val_acc= 0.7067\n",
      "Epoch: 0069 train_loss= 1.0300 train_acc= 0.7929 val_loss= 1.2427 val_acc= 0.7100\n",
      "Epoch: 0070 train_loss= 1.0183 train_acc= 0.7857 val_loss= 1.2335 val_acc= 0.7100\n",
      "Epoch: 0071 train_loss= 1.0069 train_acc= 0.7929 val_loss= 1.2241 val_acc= 0.7100\n",
      "Epoch: 0072 train_loss= 0.9957 train_acc= 0.8071 val_loss= 1.2140 val_acc= 0.7133\n",
      "Epoch: 0073 train_loss= 0.9847 train_acc= 0.8071 val_loss= 1.2047 val_acc= 0.7100\n",
      "Epoch: 0074 train_loss= 0.9737 train_acc= 0.8000 val_loss= 1.1964 val_acc= 0.7167\n",
      "Epoch: 0075 train_loss= 0.9628 train_acc= 0.8071 val_loss= 1.1884 val_acc= 0.7300\n",
      "Epoch: 0076 train_loss= 0.9522 train_acc= 0.8071 val_loss= 1.1804 val_acc= 0.7333\n",
      "Epoch: 0077 train_loss= 0.9420 train_acc= 0.8071 val_loss= 1.1727 val_acc= 0.7367\n",
      "Epoch: 0078 train_loss= 0.9319 train_acc= 0.8071 val_loss= 1.1646 val_acc= 0.7367\n",
      "Epoch: 0079 train_loss= 0.9220 train_acc= 0.8143 val_loss= 1.1554 val_acc= 0.7367\n",
      "Epoch: 0080 train_loss= 0.9121 train_acc= 0.8286 val_loss= 1.1459 val_acc= 0.7367\n",
      "Epoch: 0081 train_loss= 0.9026 train_acc= 0.8286 val_loss= 1.1366 val_acc= 0.7367\n",
      "Epoch: 0082 train_loss= 0.8931 train_acc= 0.8357 val_loss= 1.1285 val_acc= 0.7367\n",
      "Epoch: 0083 train_loss= 0.8836 train_acc= 0.8357 val_loss= 1.1213 val_acc= 0.7400\n",
      "Epoch: 0084 train_loss= 0.8741 train_acc= 0.8357 val_loss= 1.1150 val_acc= 0.7533\n",
      "Epoch: 0085 train_loss= 0.8649 train_acc= 0.8357 val_loss= 1.1090 val_acc= 0.7533\n",
      "Epoch: 0086 train_loss= 0.8559 train_acc= 0.8429 val_loss= 1.1036 val_acc= 0.7533\n",
      "Epoch: 0087 train_loss= 0.8472 train_acc= 0.8500 val_loss= 1.0980 val_acc= 0.7567\n",
      "Epoch: 0088 train_loss= 0.8386 train_acc= 0.8571 val_loss= 1.0912 val_acc= 0.7600\n",
      "Epoch: 0089 train_loss= 0.8302 train_acc= 0.8643 val_loss= 1.0843 val_acc= 0.7600\n",
      "Epoch: 0090 train_loss= 0.8218 train_acc= 0.8643 val_loss= 1.0766 val_acc= 0.7633\n",
      "Epoch: 0091 train_loss= 0.8138 train_acc= 0.8643 val_loss= 1.0692 val_acc= 0.7700\n",
      "Epoch: 0092 train_loss= 0.8058 train_acc= 0.8643 val_loss= 1.0620 val_acc= 0.7700\n",
      "Epoch: 0093 train_loss= 0.7980 train_acc= 0.8643 val_loss= 1.0552 val_acc= 0.7700\n",
      "Epoch: 0094 train_loss= 0.7904 train_acc= 0.8714 val_loss= 1.0494 val_acc= 0.7700\n",
      "Epoch: 0095 train_loss= 0.7829 train_acc= 0.8929 val_loss= 1.0437 val_acc= 0.7600\n",
      "Epoch: 0096 train_loss= 0.7758 train_acc= 0.8929 val_loss= 1.0392 val_acc= 0.7733\n",
      "Epoch: 0097 train_loss= 0.7687 train_acc= 0.8929 val_loss= 1.0349 val_acc= 0.7700\n",
      "Epoch: 0098 train_loss= 0.7617 train_acc= 0.8929 val_loss= 1.0295 val_acc= 0.7667\n",
      "Epoch: 0099 train_loss= 0.7549 train_acc= 0.8929 val_loss= 1.0240 val_acc= 0.7767\n",
      "Epoch: 0100 train_loss= 0.7482 train_acc= 0.8857 val_loss= 1.0182 val_acc= 0.7767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0101 train_loss= 0.7417 train_acc= 0.8857 val_loss= 1.0118 val_acc= 0.7767\n",
      "Epoch: 0102 train_loss= 0.7353 train_acc= 0.8857 val_loss= 1.0067 val_acc= 0.7767\n",
      "Epoch: 0103 train_loss= 0.7289 train_acc= 0.8857 val_loss= 1.0017 val_acc= 0.7800\n",
      "Epoch: 0104 train_loss= 0.7228 train_acc= 0.8929 val_loss= 0.9964 val_acc= 0.7800\n",
      "Epoch: 0105 train_loss= 0.7167 train_acc= 0.8929 val_loss= 0.9922 val_acc= 0.7767\n",
      "Epoch: 0106 train_loss= 0.7106 train_acc= 0.9071 val_loss= 0.9890 val_acc= 0.7733\n",
      "Epoch: 0107 train_loss= 0.7048 train_acc= 0.9071 val_loss= 0.9861 val_acc= 0.7733\n",
      "Epoch: 0108 train_loss= 0.6990 train_acc= 0.9071 val_loss= 0.9827 val_acc= 0.7733\n",
      "Epoch: 0109 train_loss= 0.6935 train_acc= 0.9000 val_loss= 0.9783 val_acc= 0.7733\n",
      "Epoch: 0110 train_loss= 0.6880 train_acc= 0.9000 val_loss= 0.9726 val_acc= 0.7767\n",
      "Epoch: 0111 train_loss= 0.6827 train_acc= 0.9000 val_loss= 0.9661 val_acc= 0.7833\n",
      "Epoch: 0112 train_loss= 0.6776 train_acc= 0.9000 val_loss= 0.9603 val_acc= 0.7800\n",
      "Epoch: 0113 train_loss= 0.6725 train_acc= 0.9000 val_loss= 0.9562 val_acc= 0.7900\n",
      "Epoch: 0114 train_loss= 0.6673 train_acc= 0.9000 val_loss= 0.9531 val_acc= 0.7833\n",
      "Epoch: 0115 train_loss= 0.6623 train_acc= 0.9143 val_loss= 0.9510 val_acc= 0.7833\n",
      "Epoch: 0116 train_loss= 0.6575 train_acc= 0.9143 val_loss= 0.9490 val_acc= 0.7900\n",
      "Epoch: 0117 train_loss= 0.6526 train_acc= 0.9143 val_loss= 0.9460 val_acc= 0.7833\n",
      "Epoch: 0118 train_loss= 0.6479 train_acc= 0.9143 val_loss= 0.9436 val_acc= 0.7833\n",
      "Epoch: 0119 train_loss= 0.6433 train_acc= 0.9071 val_loss= 0.9416 val_acc= 0.7833\n",
      "Epoch: 0120 train_loss= 0.6390 train_acc= 0.9071 val_loss= 0.9400 val_acc= 0.7767\n",
      "Epoch: 0121 train_loss= 0.6344 train_acc= 0.9071 val_loss= 0.9365 val_acc= 0.7767\n",
      "Epoch: 0122 train_loss= 0.6297 train_acc= 0.9071 val_loss= 0.9307 val_acc= 0.7800\n",
      "Epoch: 0123 train_loss= 0.6255 train_acc= 0.9071 val_loss= 0.9250 val_acc= 0.7833\n",
      "Epoch: 0124 train_loss= 0.6219 train_acc= 0.9071 val_loss= 0.9198 val_acc= 0.7867\n",
      "Epoch: 0125 train_loss= 0.6183 train_acc= 0.9071 val_loss= 0.9154 val_acc= 0.7833\n",
      "Epoch: 0126 train_loss= 0.6143 train_acc= 0.9143 val_loss= 0.9120 val_acc= 0.7833\n",
      "Epoch: 0127 train_loss= 0.6097 train_acc= 0.9143 val_loss= 0.9100 val_acc= 0.7800\n",
      "Epoch: 0128 train_loss= 0.6053 train_acc= 0.9071 val_loss= 0.9087 val_acc= 0.7800\n",
      "Epoch: 0129 train_loss= 0.6013 train_acc= 0.9071 val_loss= 0.9079 val_acc= 0.7800\n",
      "Epoch: 0130 train_loss= 0.5975 train_acc= 0.9071 val_loss= 0.9074 val_acc= 0.7800\n",
      "Epoch: 0131 train_loss= 0.5939 train_acc= 0.9071 val_loss= 0.9059 val_acc= 0.7767\n",
      "Epoch: 0132 train_loss= 0.5900 train_acc= 0.9071 val_loss= 0.9028 val_acc= 0.7800\n",
      "Epoch: 0133 train_loss= 0.5861 train_acc= 0.9071 val_loss= 0.8981 val_acc= 0.7800\n",
      "Epoch: 0134 train_loss= 0.5826 train_acc= 0.9071 val_loss= 0.8928 val_acc= 0.7800\n",
      "Epoch: 0135 train_loss= 0.5796 train_acc= 0.9071 val_loss= 0.8887 val_acc= 0.7833\n",
      "Epoch: 0136 train_loss= 0.5766 train_acc= 0.9071 val_loss= 0.8858 val_acc= 0.7833\n",
      "Epoch: 0137 train_loss= 0.5731 train_acc= 0.9143 val_loss= 0.8839 val_acc= 0.7833\n",
      "Epoch: 0138 train_loss= 0.5696 train_acc= 0.9143 val_loss= 0.8827 val_acc= 0.7800\n",
      "Epoch: 0139 train_loss= 0.5662 train_acc= 0.9214 val_loss= 0.8830 val_acc= 0.7800\n",
      "Epoch: 0140 train_loss= 0.5635 train_acc= 0.9214 val_loss= 0.8839 val_acc= 0.7800\n",
      "Epoch: 0141 train_loss= 0.5612 train_acc= 0.9214 val_loss= 0.8843 val_acc= 0.7767\n",
      "Epoch: 0142 train_loss= 0.5583 train_acc= 0.9214 val_loss= 0.8824 val_acc= 0.7800\n",
      "Epoch: 0143 train_loss= 0.5553 train_acc= 0.9214 val_loss= 0.8782 val_acc= 0.7767\n",
      "Epoch: 0144 train_loss= 0.5527 train_acc= 0.9071 val_loss= 0.8725 val_acc= 0.7800\n",
      "Epoch: 0145 train_loss= 0.5505 train_acc= 0.9143 val_loss= 0.8679 val_acc= 0.7800\n",
      "Epoch: 0146 train_loss= 0.5470 train_acc= 0.9214 val_loss= 0.8656 val_acc= 0.7833\n",
      "Epoch: 0147 train_loss= 0.5434 train_acc= 0.9286 val_loss= 0.8645 val_acc= 0.7800\n",
      "Epoch: 0148 train_loss= 0.5408 train_acc= 0.9214 val_loss= 0.8666 val_acc= 0.7733\n",
      "Epoch: 0149 train_loss= 0.5386 train_acc= 0.9214 val_loss= 0.8680 val_acc= 0.7733\n",
      "Epoch: 0150 train_loss= 0.5360 train_acc= 0.9214 val_loss= 0.8673 val_acc= 0.7700\n",
      "Epoch: 0151 train_loss= 0.5333 train_acc= 0.9143 val_loss= 0.8651 val_acc= 0.7733\n",
      "Epoch: 0152 train_loss= 0.5305 train_acc= 0.9143 val_loss= 0.8614 val_acc= 0.7767\n",
      "Epoch: 0153 train_loss= 0.5277 train_acc= 0.9143 val_loss= 0.8565 val_acc= 0.7767\n",
      "Epoch: 0154 train_loss= 0.5247 train_acc= 0.9143 val_loss= 0.8526 val_acc= 0.7800\n",
      "Epoch: 0155 train_loss= 0.5218 train_acc= 0.9214 val_loss= 0.8493 val_acc= 0.7767\n",
      "Epoch: 0156 train_loss= 0.5193 train_acc= 0.9214 val_loss= 0.8485 val_acc= 0.7767\n",
      "Epoch: 0157 train_loss= 0.5176 train_acc= 0.9214 val_loss= 0.8495 val_acc= 0.7767\n",
      "Epoch: 0158 train_loss= 0.5158 train_acc= 0.9214 val_loss= 0.8493 val_acc= 0.7767\n",
      "Epoch: 0159 train_loss= 0.5133 train_acc= 0.9214 val_loss= 0.8472 val_acc= 0.7767\n",
      "Epoch: 0160 train_loss= 0.5105 train_acc= 0.9286 val_loss= 0.8430 val_acc= 0.7767\n",
      "Epoch: 0161 train_loss= 0.5082 train_acc= 0.9286 val_loss= 0.8389 val_acc= 0.7800\n",
      "Epoch: 0162 train_loss= 0.5069 train_acc= 0.9286 val_loss= 0.8353 val_acc= 0.7800\n",
      "Epoch: 0163 train_loss= 0.5057 train_acc= 0.9214 val_loss= 0.8327 val_acc= 0.7800\n",
      "Epoch: 0164 train_loss= 0.5028 train_acc= 0.9214 val_loss= 0.8314 val_acc= 0.7800\n",
      "Epoch: 0165 train_loss= 0.4995 train_acc= 0.9214 val_loss= 0.8314 val_acc= 0.7767\n",
      "Epoch: 0166 train_loss= 0.4970 train_acc= 0.9214 val_loss= 0.8339 val_acc= 0.7767\n",
      "Epoch: 0167 train_loss= 0.4956 train_acc= 0.9286 val_loss= 0.8365 val_acc= 0.7767\n",
      "Epoch: 0168 train_loss= 0.4940 train_acc= 0.9286 val_loss= 0.8370 val_acc= 0.7767\n",
      "Epoch: 0169 train_loss= 0.4915 train_acc= 0.9286 val_loss= 0.8343 val_acc= 0.7767\n",
      "Epoch: 0170 train_loss= 0.4887 train_acc= 0.9286 val_loss= 0.8308 val_acc= 0.7733\n",
      "Epoch: 0171 train_loss= 0.4860 train_acc= 0.9357 val_loss= 0.8265 val_acc= 0.7767\n",
      "Epoch: 0172 train_loss= 0.4843 train_acc= 0.9357 val_loss= 0.8228 val_acc= 0.7800\n",
      "Epoch: 0173 train_loss= 0.4829 train_acc= 0.9357 val_loss= 0.8205 val_acc= 0.7833\n",
      "Epoch: 0174 train_loss= 0.4810 train_acc= 0.9357 val_loss= 0.8189 val_acc= 0.7800\n",
      "Epoch: 0175 train_loss= 0.4788 train_acc= 0.9429 val_loss= 0.8173 val_acc= 0.7833\n",
      "Epoch: 0176 train_loss= 0.4767 train_acc= 0.9429 val_loss= 0.8154 val_acc= 0.7833\n",
      "Epoch: 0177 train_loss= 0.4750 train_acc= 0.9429 val_loss= 0.8149 val_acc= 0.7767\n",
      "Epoch: 0178 train_loss= 0.4734 train_acc= 0.9429 val_loss= 0.8159 val_acc= 0.7767\n",
      "Epoch: 0179 train_loss= 0.4719 train_acc= 0.9429 val_loss= 0.8181 val_acc= 0.7733\n",
      "Epoch: 0180 train_loss= 0.4696 train_acc= 0.9429 val_loss= 0.8182 val_acc= 0.7733\n",
      "Epoch: 0181 train_loss= 0.4674 train_acc= 0.9357 val_loss= 0.8183 val_acc= 0.7767\n",
      "Epoch: 0182 train_loss= 0.4656 train_acc= 0.9357 val_loss= 0.8170 val_acc= 0.7800\n",
      "Epoch: 0183 train_loss= 0.4649 train_acc= 0.9286 val_loss= 0.8165 val_acc= 0.7800\n",
      "Epoch: 0184 train_loss= 0.4637 train_acc= 0.9357 val_loss= 0.8136 val_acc= 0.7833\n",
      "Epoch: 0185 train_loss= 0.4618 train_acc= 0.9357 val_loss= 0.8098 val_acc= 0.7833\n",
      "Epoch: 0186 train_loss= 0.4591 train_acc= 0.9500 val_loss= 0.8067 val_acc= 0.7867\n",
      "Epoch: 0187 train_loss= 0.4561 train_acc= 0.9500 val_loss= 0.8052 val_acc= 0.7833\n",
      "Epoch: 0188 train_loss= 0.4540 train_acc= 0.9500 val_loss= 0.8057 val_acc= 0.7800\n",
      "Epoch: 0189 train_loss= 0.4528 train_acc= 0.9500 val_loss= 0.8055 val_acc= 0.7733\n",
      "Epoch: 0190 train_loss= 0.4514 train_acc= 0.9500 val_loss= 0.8045 val_acc= 0.7767\n",
      "Epoch: 0191 train_loss= 0.4492 train_acc= 0.9500 val_loss= 0.7998 val_acc= 0.7733\n",
      "Epoch: 0192 train_loss= 0.4471 train_acc= 0.9500 val_loss= 0.7954 val_acc= 0.7833\n",
      "Epoch: 0193 train_loss= 0.4456 train_acc= 0.9571 val_loss= 0.7923 val_acc= 0.7867\n",
      "Epoch: 0194 train_loss= 0.4439 train_acc= 0.9500 val_loss= 0.7912 val_acc= 0.7867\n",
      "Epoch: 0195 train_loss= 0.4412 train_acc= 0.9571 val_loss= 0.7924 val_acc= 0.7900\n",
      "Epoch: 0196 train_loss= 0.4391 train_acc= 0.9571 val_loss= 0.7956 val_acc= 0.7867\n",
      "Epoch: 0197 train_loss= 0.4377 train_acc= 0.9500 val_loss= 0.7996 val_acc= 0.7833\n",
      "Epoch: 0198 train_loss= 0.4366 train_acc= 0.9500 val_loss= 0.8025 val_acc= 0.7800\n",
      "Epoch: 0199 train_loss= 0.4354 train_acc= 0.9500 val_loss= 0.8042 val_acc= 0.7800\n",
      "Epoch: 0200 train_loss= 0.4331 train_acc= 0.9500 val_loss= 0.8010 val_acc= 0.7833\n",
      "Test set results: loss= 0.8305 accuracy= 0.7900\n"
     ]
    }
   ],
   "source": [
    "NB_EPOCH = 200\n",
    "#val_data = (model_input, y_val, val_mask)\n",
    "\n",
    "\n",
    "for epoch in range(1, NB_EPOCH + 1):\n",
    "    model.fit(model_input,\n",
    "          y_train,\n",
    "          sample_weight=train_mask,\n",
    "          batch_size=A.shape[0],\n",
    "          epochs=1,\n",
    "          shuffle=False,\n",
    "          verbose=0)\n",
    "    # Predict on full dataset\n",
    "    \n",
    "    preds = model.predict(model_input, batch_size=A.shape[0])\n",
    "\n",
    "    # Train / validation scores\n",
    "    train_val_loss, train_val_acc = evaluate_preds(preds, [y_train, y_val],\n",
    "                                                   [idx_train, idx_val])\n",
    "    print(\"Epoch: {:04d}\".format(epoch),\n",
    "          \"train_loss= {:.4f}\".format(train_val_loss[0]),\n",
    "          \"train_acc= {:.4f}\".format(train_val_acc[0]),\n",
    "          \"val_loss= {:.4f}\".format(train_val_loss[1]),\n",
    "          \"val_acc= {:.4f}\".format(train_val_acc[1]),\n",
    "          )\n",
    "    \n",
    "test_loss, test_acc = evaluate_preds(preds, [y_test], [idx_test])\n",
    "print(\"Test set results:\", \"loss= {:.4f}\".format(test_loss[0]),\n",
    "      \"accuracy= {:.4f}\".format(test_acc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
