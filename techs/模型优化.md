# 过拟合的原因

1.建模样本抽取错误，包括（但不限于）样本数量太少，抽样方法错误，抽样时没有足够正确考虑业务场景或业务特点，等等导致抽出的样本数据不能有效足够代表业务逻辑或业务场景。



2.样本里的噪音数据干扰过大，大到模型过分记住了噪音特征，反而忽略了真实的输入输出间的关系。



3.建模时的“逻辑假设”到了模型应用时已经不能成立了。任何预测模型都是在假设的基础上才可以搭建和应用的，常用的假设包括：假设历史数据可以推测未来，假设业务环节没有发生显著变化，假设建模数据与后来的应用数据是相似的，等等。如果上述假设违反了业务场景的话，根据这些假设搭建的模型当然是无法有效应用的。 



4.参数太多、模型复杂度高 



5. 决策树模型。如果我们对于决策树的生长没有合理的限制和修剪的话，决策树的自由生长有可能每片叶子里只包含单纯的事件数据(event)或非事件数据（no event），可以想象，这种决策树当然可以完美匹配（拟合）训练数据，但是一旦应用到新的业务真实数据时，效果是一塌糊涂。



6. 神经网络模型



* 由于对样本数据,可能存在隐单元的表示不唯一,即产生的分类的决策面不唯一.随着学习的进行, BP算法使权值可能收敛过于复杂的决策面,并至极致。



* 权值学习迭代次数足够多(Overtraining),拟合了训练数据中的噪声和训练样例中没有代表性的特征。




# 2.解决

1.权值衰减. 主要应用在神经网络模型中

> 它在每次迭代过程中以某个小因子降低每个权值,这等效于修改E的定义,加入一个与网络权值的总量相应的惩罚项,此方法的动机是保持权值较小,避免weight decay,从而使学习过程向着复杂决策面的反方向偏。 



2. 适当的stopping criterion



> 在二次误差函数的情况下，关于早停止和权值衰减类似结果的原因说明。椭圆给出了常数误差函数的轮廓线，Wml表示误差函数的最小值。如果权向量的起始点为原点，按照局部负梯度的方向移动，那么它会沿着曲线给出的路径移动。通过对训练过程早停止，我们找到了一个权值向量w。定性地说，它类似于使用检点的权值衰减正则化项，然后最小化正则化误差函数的方法得到的权值。 


3.验证数据

一个最成功的方法是在训练数据外再为算法提供一套验证数据,应该使用在验证集合上产生最小误差的迭代次数,不是总能明显地确定验证集合何时达到最小误差。

Typically 30% of training patterns; Validation set error is checked each epoch; Stop training if validation error goes up 



4.交叉验证

> 交叉验证方法在可获得额外的数据提供验证集合时工作得很好,但是小训练集合的过度拟合问题更为严重。




# 1. keras 中 early stopping


```python


# early stoppping
from keras.callbacks import EarlyStopping
early_stopping = EarlyStopping(monitor='val_loss', patience=50, verbose=2)
# 训练
history = model.fit(train_X, train_y, epochs=300, batch_size=20, validation_data=(test_X, test_y), verbose=2, shuffle=False, callbacks=[early_stopping])

```
* monitor: 需要监视的量，val_loss，val_acc
* patience: 当early stop被激活(如发现loss相比上一个epoch训练没有下降)，则经过patience个epoch后停止训练
* verbose: 信息展示模式
* mode: 'auto','min','max'之一，在min模式训练，如果检测值停止下降则终止训练。在max模式下，当检测值不再上升的时候则停止训练。


## 2. 获取最佳模型


```python


best_weights_filepath = './best_weights.hdf5'
earlyStopping=kcallbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='auto')
saveBestModel = kcallbacks.ModelCheckpoint(best_weights_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')
 
# train model
history = model.fit(x_tr, y_tr, batch_size=batch_size, nb_epoch=n_epochs,
              verbose=1, validation_data=(x_va, y_va), callbacks=[earlyStopping, saveBestModel])
 
#reload best weights
model.load_weights(best_weights_filepath)

```

## 3. 交叉验证



-----------

# 模型优化

# 1. 
* Improve Performance With Data.
* Improve Performance With Algorithms.
* Improve Performance With Algorithm Tuning.
* Improve Performance With Ensembles.


## 1. Data

1. Invent More Data
* This is also related to adding noise, what we used to call adding jitter. It can act like a regularization method to curb overfitting the training dataset.

> In other words, if we have two cases with similar inputs, the desired outputs will usually be similar. That means we can take any training case and generate new training cases by adding small amounts of jitter to the inputs. As long as the amount of jitter is sufficiently small, we can assume that the desired output will not change enough to be of any consequence, so we can just use the same target value. The more training cases, the merrier, so this looks like a convenient way to improve training. But too much jitter will obviously produce garbage, while too little jitter will have little effect (Koistinen and Holmström 1992).







# 参考

* https://blog.csdn.net/ukakasu/article/details/80089866
* https://mp.weixin.qq.com/s/2vwmq3ybxv06qbFoQviyHQ

* 获取 最佳模型 - https://blog.csdn.net/ukakasu/article/details/84133270
