# 1. 文本信息

## 1， 用户端-- User Profile

* 注册的姓名，个性签名
* 发表的动态，评论，日记
* 聊天记录

## 2. 物品端--- item Profile

* 物品的标题，描述
* 物品的本身内容（新闻）
* 其他的基本属性的文本

--------

# 2. 构建用户画像

* 文本结构化， 保留关键信息
> 准确性， 粒度 ， 覆盖面

*  根据用户行为数据，把物品的结构化结果传递给用户

## 1. 结构化文本

* 关键字提取： TF-IDF 和 TextRank
* 实体识别：人物、位置和地点、著作、影视剧、历史事件和热点事件...

* 内容分类：将文本按照分类体系分类，用分类来表达较粗粒度的结构信息

* 文本 ：在无人制定分类体系的前提下，无监督地将文本划分成多个类，类编号也是用户画像的常见构成

* 主题模型： 从大量的文本中嘘唏主题向量，预测新的文本的主题概率分布，

* 嵌入： Embedding： 挖掘字面意思之下的语义信息

--------

# 3.  文本结构化算法

## 1. TF-IDF （Term Frequency   INverse Document Frequency）

> * 一篇文字中 反复出现的词会更重要
> * 在所有文本中都出现的词 不重要

* TF : 词频-- 出现的次数
* IDF : 统计每一个词出现在了多少个文本中，记为n（文档频率） 总的文档数目N----$$IDF = log（N/n+1）$$
  * 出现的文档数目少， IDF大
  * + 1 防止新词的0 出现无穷大
  * 对于新词，可以赋值为0 ，或者默认赋值为所有词的平均文档频率

* 计算 TR ✖️ IDF = 词的权重
  * 计算所有词权重的平均值， 去在权重平均值之上的词作为关键词
  
 
## 2. TextRank

算法思想：

* 1. 文本中，设定窗口宽度； k个词，统计窗口内的词和词的共现关系-- 看成无向图
* 2. 所有词初始化的重要性都是 1；
* 3. 每个节点把自己的权重平均分配给“和自己有连接“的其他节点
* 4. 每个节点将所有其他节点分给自己的权重求和，作为自己的新权重；
* 5. 如此反复迭代第 3、4 两步，直到所有的节点权重收敛为止。

那些有共现关系的会互相支持对方成为关键词。




## 3. 内容分类

* 门户网站具有自己的频道体系，内容分类体系，

* 短文本分类的方法是 SVM 常用工具-- Facebook的开源FastText


## 4. 实体识别 - Named- Entity Recognition
> 序列标注问题： 给定字符序列，对每一个字符分类

* 1. 词性标注：对每一个分好的词，分类为定义的词性集合的之一

* 2.实体识别：对每一个分好的词，识别为定义的命名实体集合之一
* 3. 分词问题：对每一个字符分类为“词开始”“词中间”“词结束”三...

常用的算法 ： 隐形马尔可夫模型HMM 条件随机场 CRF

常用工具： sapCy > NLTK


## 5. 聚类分析

主题模型：LDA的主题模型得到更好的软聚类的效果，一条文本属于多个类簇。

* LDA 模型需要设定主题个数，如果你有时间，那么这个 K 可通过实验进行对比挑选：每次计算K个主题俩俩之间的平均相似度，选择一个较低的K值？？
* 计算资源足够，主题数目可以多一些 


开源的LDA训练工具有 Gensim PLDA 


## 6. 词嵌入 ---得到一个 稠密的向量

Word2Vec ： 浅层神经网络学习得到每个词的向量表达
































